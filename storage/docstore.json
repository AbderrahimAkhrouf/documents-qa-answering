{"docstore/metadata": {"d4628515-1f2f-47ea-86f6-25d0d76c1035": {"doc_hash": "4b26ca0c1eaa665a7cdbce6f1abf937c4d1b76e5f3b8e4f12c3296f9e203e617"}, "cb49f97a-3aa0-4212-8005-fb5c70cb8ff8": {"doc_hash": "4d5c5cff06f8310a32694671acfcd6b01e2af25dc8cd76ecf9520d232271ffa0"}, "2cc76dff-87a2-46f4-9964-fe98b162f4a2": {"doc_hash": "aefc2f0443ffbf50e7bb4750f10921b1e776b0166b6ceaf3460f0be097ff3219"}, "28734f21-e5d5-418b-8a33-0318bc753a4e": {"doc_hash": "80e3654153b0630bd04ccea0976685269b790cfddaef17c6ff32c9b0be73d837"}, "23c8cb0f-647d-4596-9c23-e6fc2562944c": {"doc_hash": "e9c6f0f0034a892e33ee7851f5f79a851a194c538c01d08bc6a51f528744ea1f"}, "4f54020d-2f43-4631-8cdd-9941aefd8ca3": {"doc_hash": "ef8641cda58e0a26c6fb879a154fb387a2a4e243a89b8717e03ee76f781eb3e2"}, "cf3c4a92-db82-4877-9e2c-f2908ca6bd1e": {"doc_hash": "e84275c39cb998ec040d688ee07c94fd32ca0e6609be3325bc931dff47a96aa5"}, "73107650-0c1d-467a-8f9e-2a85dff78c1e": {"doc_hash": "184377a1960e821827c3aa01121705e63bf79d6c1b7ec43cd95693363a4533d7"}, "ba6488a6-f975-4588-a75f-d0b082dcf28b": {"doc_hash": "259abbbca64538fcdcb788eb4c9ed9fe4f52dd112935139b640f423f3d758797"}, "57681db3-c712-4ebe-941e-901878af9b94": {"doc_hash": "32fd042bc412263953d7480c1496447db554c819b04b9202e846d8272e4b2b19"}, "0f4de1f6-d75d-481b-8233-4538aa6dd01a": {"doc_hash": "17444a0ea4d604a2f4078500adf80ddfb802b0bea25594a61f99667d80e6036c"}, "19139b82-d1b5-43d4-a7bd-e35297d8e82e": {"doc_hash": "0fa342dc9cbe831d68ec86228e2794be910ca74ec24bbfe460e39457b9ac6102"}, "257e9425-9f18-4f4f-89bd-9aace4633dbe": {"doc_hash": "b5cfccf9a9ad3f6da35d1cb4181c167203653c289d41cf76c81fc289c31df67a"}, "edff8844-dec3-413c-acb5-d95c49d37b7e": {"doc_hash": "40a96fc1b3d6ce8f12d43a6985c21b6e0feaa99b04bf83da082c8a7f32dbb095"}, "fd9fdfe0-2305-4b69-97ec-f5eda22bc01c": {"doc_hash": "e07d213da7a30bd4dc602df5840652901a8eae0607150638ee2eb5bc48665dfb"}, "0a402834-f7ed-4f26-8200-9b91cdf11b4c": {"doc_hash": "65f9073f2453e65782bbe9eb2e88c6371d5f2efa011ee0a3e787ca59556db89c"}, "a051f339-922c-4b89-b4fc-b6940a45dc5a": {"doc_hash": "b847093ebc374ac4b98a54f6ddb75083f3b78a1a511d50d47fd192950a55ecd1"}, "7bc9bee8-cb63-4c8a-8b58-15e0baf51f92": {"doc_hash": "15756d60724040ac1be7384080859e539f530a5ab38c462eacf1296e8bac95c3"}, "406a4296-6669-468a-bac1-42083f0c6181": {"doc_hash": "e0cfef08d3bc2ceec9c4c4991c76f01561e1d0d9a308814b7a285b73d6d67b4d"}, "65c138df-617c-4874-8abf-4d6feef1fe9e": {"doc_hash": "b78cc72acbc6e5b85497d39b3a50c9f1d54c63067257c3327ef2148737dce006"}, "a08a854c-ce44-4fe0-adbe-4c8c4fd402e0": {"doc_hash": "9c20d38f211497bbb4d7110e958e88ac66503c564128a5aad058b8ddaefa9930"}, "0435b871-4048-4e5a-aa21-82cb34b77642": {"doc_hash": "10014486c9baeeee64b08545d573c35611589b59cbf4a0dc86c0293a8e040bcd"}, "a97d10ad-fd39-4d39-bec4-f1c888d80c37": {"doc_hash": "4bd5e7287e63f79941810f2464d4634a36bc23aa227a7140a55570d934e8eee3"}, "05696886-7400-41a8-9087-a90319edb626": {"doc_hash": "4279281685d2be0e382f7378dc9bf8a4e8c2fcfa4ee4b5d8e39caf41951600ab"}, "52a93de9-980f-425f-a7ed-1c2e53757d68": {"doc_hash": "fe24ad80dd665e523a020c99730330d3764cb2d059f08858eb148d99db502169"}, "3044d36b-22ab-4bf1-9baa-3218119a8e05": {"doc_hash": "addd79111a14fffa81e6190ab24df13a7008952eeff856012d716733ff13fd99"}, "10f495a8-e5a3-4331-b4c3-979249d8e5df": {"doc_hash": "6403d4920176df5ac86c41c8a37918e7d1eb7199308389b4e99319ec502383e1"}, "59f34a73-617d-4d45-90d0-2dfd09f9a620": {"doc_hash": "8e7e6575521748d985f2c6c8e7cc3b23c8d71d3caa73186760f664470b3cf1e9"}, "31969453-c8d0-4760-a7b4-a32666005be6": {"doc_hash": "6e5230e7a35961d87665496159f766c8adcc35b36535b3df857f0fceb435b69c"}, "b20e5506-61f6-4801-ad12-da7da6debc74": {"doc_hash": "aee16b3fbc041923d44a01a3f857f0a161162eb6e220b0277efb836ae2cbf2cf"}, "7c61d074-41bb-4a7c-9faf-f35d94f86ca3": {"doc_hash": "fe15a8433c9c7acda8f4ac9d1cadac4321d3d5352403f6ee4fb8c5cd277e3548"}, "27edb7b6-3c71-4d30-8b51-f392be1b0b1c": {"doc_hash": "64c700bea1b4d210892c83e9427a899d432787effd7b19dc4d72d336e994e8fa"}, "6518de0d-dda4-4d06-b56a-ad870a44f66b": {"doc_hash": "d432fe866ecfdb4a9409c341035177b793f1635f11160e9d08ae45dfb7e76731"}, "4df198f5-f606-4714-a3c2-cffd8c295899": {"doc_hash": "32fb84c78494863ea07d56e44137dae8c22c4e8da6aa0df2fd12ca7fd35c3d06"}, "6633cb98-b968-447b-969d-9ff214531f79": {"doc_hash": "3db1835b3e7b72675eba1627e04bb536792f0ecd37e2c816d6fb96fd4433593e"}, "f64ef16c-5d58-40d7-b16e-13a5550bd232": {"doc_hash": "b917bd34bc3afe083ceea80ad4f17351f20fd1af523539c5e89c384f8e377669"}, "dedb899c-393a-4e00-88ee-3bd1e5d2dfdb": {"doc_hash": "c8a8c8ecae1b4f99b06372e146169f9e5e673f1208724a223c70ef56ce743295"}, "040f261a-e914-4818-9948-18bbb65aca82": {"doc_hash": "c8bf2a712584bb83a370925425d63564a99c1d03d01aa6e5b1a8e0f524a303d3"}, "ad590b0f-21fe-41a9-a41b-fedf22806134": {"doc_hash": "db4e28e7e982c62c43bece656dfcc47cc97c41a7db36d1a70b3aa08f1bba7416"}, "1ec8c8b1-2c08-4d68-9769-aafcb27956e7": {"doc_hash": "c88819cf6ece2bd3fdd77dd64c13bbdf1186e0abc69f6fb8b16b537bcf9d9f51"}, "598dad45-b79a-4f0f-b791-626974833e17": {"doc_hash": "5f40ca128d69dbe47f34d3156dde8b15f4c0108db5156baa107bd7f950bb8800"}, "e5f4a401-1e88-462f-8b8b-405dfd8ac7e4": {"doc_hash": "1fd94d43c9a6603bae26b34fc0a87bdc54bf619c0a3cdf56a4b84840175b5a39"}, "6742c4da-7e0a-4a21-9fe4-e182dd0e221c": {"doc_hash": "a2666ca0798f5c9a7d525cd988baf989fe6ba6197cb7428b572e12b7f77c332c"}, "9e9f0a56-7e29-4909-88bd-73e467a5eaeb": {"doc_hash": "537efb6661d06d58276b55cd61947d7acd1b1c4089bae7198e6e3241c2584e4d"}, "a7456b3b-d65d-4c0a-a434-f699011c3d80": {"doc_hash": "aee2a92c67ee3cbb1e3c79cb23644d671bff93252f41dce83d0965a276ef54a2"}, "84e8d558-a595-4bf4-9587-f7ede9273991": {"doc_hash": "4367e7b7f299fc13ec4688810fc3313aa79f0665c12da47203e526c43d807647"}, "fd074a23-19f2-4efd-bd4a-ff87a76d76d0": {"doc_hash": "312ae5b8707c296847fbb4135d430dab40e2b0a429580997a12a31a78dff7f3a"}, "11f1db50-1d2a-41be-8e60-894c2a28ce8b": {"doc_hash": "0630f3faaccd461c07695a182ea9e82314f8f500cbe9899e995c4357a8fabaab"}, "ba107626-2d72-4316-b3a1-fa3799a192fe": {"doc_hash": "856d3c1b2659a460c3d0c1eba3c146f16a1bb585464d530bafddb84f24c04809"}, "0f00fb90-1605-4f75-9509-e5dda2c7db19": {"doc_hash": "3a1cb78de522ffb503a1f88c121f93b5dee7bbb1bb1d64f110c0c0efce448668"}, "154ed6bc-79e8-4bc9-8426-2480a3c0238c": {"doc_hash": "fa896d8a12e1943930c965879201f5e57648325d502abbdf6496790ba76be64d"}, "0e9a8331-517d-4453-97f5-037b8d82d3d7": {"doc_hash": "94ad38df10a01b87f94123b22f5f0ff958f612e76b3fe5aeae357f1ee20cf667"}, "0e1eb5a1-f9db-4fb3-bb17-613d176c030b": {"doc_hash": "f49b20544135b08a7da459581c6bedfd71c96d600aefc60dc0d70d89798013aa"}, "44ee76b9-bf15-4cc8-95fd-aa2265dbf3ce": {"doc_hash": "2e18f2b632a681f880ae0a76bb490ab7cada3ffc7e6c649904a2af91e3049450"}, "0e371c83-9b2f-4a26-89e0-c6697ab820d8": {"doc_hash": "ebab343c445163f12ef3bb159f51b0d2977d82a1c6a6ddcf20ada77f25364835"}, "203af889-47e6-413d-b785-fcfee2a17832": {"doc_hash": "f7133e919e4af4cd108b2736856890bef924871ec16e1b51631bdd205caa6e0a"}, "074d81c9-eb70-40c1-9099-2d5eaa51fc93": {"doc_hash": "eeff765b9ff755b84034d05db37da59eef591f45f2f216f999bce9f5700288bc"}, "963338af-f8c1-4291-a9b0-4686a6eb7f94": {"doc_hash": "575ba4cefcf0b71a6ef2bfbd076c55977fdff82d67e50fddbe0b70d7eebfa589"}, "ab286212-c22d-499d-8d31-60d0a2af8264": {"doc_hash": "ad9996d3ecd4fd4de5e7b580df5fb680bc5114fef0725c439958482c6a8fbac4"}, "0ceb5ddd-d825-40cb-b26d-cdbe59a738f6": {"doc_hash": "820024c671770c3efa2a98755b921abb22fdf7a88922ac424916837c5283b2e1"}, "d4e6f2f1-1603-4c5c-96fd-71b098f988db": {"doc_hash": "88722359a816f8540ab1a4f78f21c9576465c73ea01e8bd64b0bc16ab4cd5f3b"}, "076a8af5-e36e-4070-9ae0-3d014e547de1": {"doc_hash": "9c03fed300ffe9555d06afef33e9387cb1e35a79e333134146058504056bde2e"}, "e13b5f59-ed8a-42a8-8569-db7f6f108ff0": {"doc_hash": "527fc5e5f6dcd31578e17ea8914510009ea9eb314eef3decf0a297c25f7ade1b"}, "bf41e409-ceaa-4311-8982-adc573e48688": {"doc_hash": "41905b623b8889ee46f0545097fe766997d8c0058df4db48ac8e491a40e00857"}, "1b1dcd45-cc0b-40da-a634-3825d04c51b7": {"doc_hash": "40a359674b6140fa88c077a3bddef0dcae3ba0c256ef7953e7c8556f1ea94b2a"}, "1e067fee-1de3-4a11-bb31-b19b4c9e379f": {"doc_hash": "d3d428b355178dc51ed48d07151ffdb9091c6fb8079e13e629aa7e9330ae9860"}, "1b371e23-4721-4cb3-a324-15e5c2cdcbdd": {"doc_hash": "7d18622bf72894e80bb85a3919d82af0f3592bd90f4cb133471ca09fc61cb2eb"}, "44e46b2f-9826-4bf3-bc87-7dac192228b9": {"doc_hash": "8a667b60d73b58f9fc9f85653d637e30203f46e97f459498eb70dab72bd58422"}, "03e1dd28-c2b5-4961-bff2-1de7c3789fd3": {"doc_hash": "52677b484a0ab4293dba6b4be2808992a0cc5ac5cfe365411315d79290921c73"}, "2d2bce7f-3073-4a67-9e1b-15cfb44cc4a8": {"doc_hash": "6c96ae8d1b654cd484ed277ec78c814e941c78781a19f0909063e666756c74ee"}, "318e8d10-9e10-4124-b963-da894ef2e3eb": {"doc_hash": "c34d0134221c51016d0422b4d0e7654c4b1c3d21d78c1b4671ba682271c69c9f"}, "d14f1254-258a-4f0c-9d42-50293e276bb5": {"doc_hash": "ff60c568641fb71c2cf9323356649e57268f061a15a8b756c6d04aafc2eeb1c7"}, "6bee79f4-ff09-42e7-a221-c1db43a284b2": {"doc_hash": "e3cd765478ac205e26922277607f0e46dd88da812c06d3598aa9cee61fbd7d9e"}, "97ae1dac-86ff-4854-ad6d-4674257dfce4": {"doc_hash": "37df03426700dc56f1bf26f81dc4ee421304853861f409d607eae775af6e85cd"}, "be32a1b6-df02-4633-b9a8-9f3a57d9a340": {"doc_hash": "929512418db177f2b5adcf4f9b00090bc05e0ee5b131a5bd45d362f2ef24fe1e"}, "eaae58fd-baa7-4d8f-a627-4531e2c1399b": {"doc_hash": "964b9a6c5f61660dcce57053c784e8eb879149c150241fa68e6086c2edec8083"}, "9ea34853-1573-4000-88c8-873919d0f1fe": {"doc_hash": "424365d43930189cf422ee1ade8a491ebcf4e442c0ec55c24b0b4b95bf4b3f4f"}, "5f97977f-a8ef-4908-90fa-7bb879980d03": {"doc_hash": "68100f25e7b0f82f7dd8da826dde561f4e936a3203a034bd7c63bb76eb6fe0ef"}, "e917ed70-7707-4875-b958-654c0af5fb63": {"doc_hash": "5e4595539cda087ff29287abbbf33cc028d4a8676c58ec40c5c06340c9de6fcb"}, "9c943d83-50a6-4df9-946a-400040557fef": {"doc_hash": "6a5fe59d4fcfcf28c61b1cbf6e3fc6e147ba5d597ef1267ec717590bc6ca831b"}, "5b0d908d-ca02-4ebf-bf71-35f841b2eb04": {"doc_hash": "bba27ac8fbb1b07f867ca0e542eaceb123cd3e17d1df900862c915d401aa4252"}, "d06ef884-b1a9-4831-b274-d9bca4368733": {"doc_hash": "5decd18e8c31784d036398ec728bd1da64c8c264abf11f770a1b88a46d6d1e32"}, "ef3b3112-6b8d-4f3a-952a-bfa2d7916467": {"doc_hash": "c84898c58d6c46c76a205dc11e993e922053f59710a2c0bc06c264932915d29c"}, "6164f625-9d41-4bb1-9d65-fbba101f6963": {"doc_hash": "ac4ec04eef479021f479f7fab4cb86b6df2acb9d9fee899efbfa218fe7a2e818"}, "a596c78e-f592-4709-8c55-a2a56de14fc7": {"doc_hash": "e8136c24b7eede8d377b3875a274590f3bf2403d1a64b3d89e2be03e8f29dc98"}, "e6a67f75-fb4d-4909-898b-bb5f0750403a": {"doc_hash": "ed3ef5d4f99484269e3e5d8027f40992090084a6b2f5fbf8c812ddaef4c55203"}, "8314f153-2970-455c-b99c-e00aff13d161": {"doc_hash": "ec643b4b199f6243e049209025376bcf09ddeaacddc01b1f80f9f845610860bc"}, "c0fcdda1-7ef3-472e-8398-ed41d923cfaf": {"doc_hash": "8e6fecc3745542327aec9ebe09b778347d1dde81bd29e9a6248766f39b3cf7b9"}, "1587cc8c-90a7-4be2-973a-c0af1d2e402d": {"doc_hash": "0b07b120490797262997cf49cd707d588b3c237aacbddeea1b6064efe5e79975"}, "1f2e0772-14a7-493a-99ce-397a9b2acc56": {"doc_hash": "6fb4fd214f60b758c4fdd56d21f9d8c2a705ef3157dc8ed10f62371666bf76eb"}, "1d1e7d17-3d74-42d4-b1ad-de61da5659c6": {"doc_hash": "4b26ca0c1eaa665a7cdbce6f1abf937c4d1b76e5f3b8e4f12c3296f9e203e617", "ref_doc_id": "d4628515-1f2f-47ea-86f6-25d0d76c1035"}, "924bd91d-ed72-4c4e-9100-6a2c3ccc96b8": {"doc_hash": "f829665f3053715bbd76fe99bec451c1dae53d35a747139be39815de6d8c165b", "ref_doc_id": "cb49f97a-3aa0-4212-8005-fb5c70cb8ff8"}, "5a528f2d-0d66-479a-81c9-046f97289c43": {"doc_hash": "d334659ff76b02ec2c124f4399631be65569aa9f2886a7327b487e482f05dd14", "ref_doc_id": "cb49f97a-3aa0-4212-8005-fb5c70cb8ff8"}, "3a5db9c7-ca84-4422-be6b-cf4ad6731a48": {"doc_hash": "b104620450cc648e99f1093db9a3ede41eafe43970d035ae3a0f5b28ba352c10", "ref_doc_id": "2cc76dff-87a2-46f4-9964-fe98b162f4a2"}, "19f72316-d940-4728-bfd3-6840b75b97d8": {"doc_hash": "9f95825d8f9cb862994536e3e4b41ece0db7cf7ca73364703bdeaef820bc2ad9", "ref_doc_id": "2cc76dff-87a2-46f4-9964-fe98b162f4a2"}, "941c1ae7-ec72-49a0-aff4-0deaf59a32cf": {"doc_hash": "62bbee8dc8d00b39e58b1d99d6bd893a52de74f0a616eaae9a918abab639b625", "ref_doc_id": "28734f21-e5d5-418b-8a33-0318bc753a4e"}, "af992b63-bba3-4858-aadb-8e1194b8d90c": {"doc_hash": "f37649ad35639581c33b124c0d586dd78f7ed8b3271f36f1c0cea145ce7688d2", "ref_doc_id": "28734f21-e5d5-418b-8a33-0318bc753a4e"}, "4a9355c0-ee4d-40fa-92b0-4f0283c44351": {"doc_hash": "178a35fca7e561cedb856bdac581355e6e754eb89a2e928259624c9adc43db39", "ref_doc_id": "23c8cb0f-647d-4596-9c23-e6fc2562944c"}, "f527ad07-34d6-417e-ad5f-8429c0040d14": {"doc_hash": "24ec50a219b6a2216a30373b6e26b41c0b6bed2efe380f306cbb30588194bbd1", "ref_doc_id": "23c8cb0f-647d-4596-9c23-e6fc2562944c"}, "5f458e21-b384-4b55-8b1c-5cd833cda64d": {"doc_hash": "ce62ea83ac9665bdaf32c8e9521cdf4607e8a9830bce22348d3981f2d0be17ce", "ref_doc_id": "4f54020d-2f43-4631-8cdd-9941aefd8ca3"}, "5aaf3b75-993b-40f0-8b2f-8fbca305f169": {"doc_hash": "8a9f19675426ae45f0008033e9bb6dbd699a492698a25d05e8af9b9c1ee29f4f", "ref_doc_id": "4f54020d-2f43-4631-8cdd-9941aefd8ca3"}, "e274fee9-e0d4-4ae4-856e-bfc65684262f": {"doc_hash": "98b80c41cd89d82e1749e26ed82933f25a522cf114118ced66641ba0f227abf1", "ref_doc_id": "cf3c4a92-db82-4877-9e2c-f2908ca6bd1e"}, "793bbc40-715f-4ee9-b11a-f8c20ea5a916": {"doc_hash": "c298350695eaa7b824d0fc2459812252682ce3583cbbcdbc110f0316d2744e96", "ref_doc_id": "cf3c4a92-db82-4877-9e2c-f2908ca6bd1e"}, "055e1273-c70c-4622-9e78-93718cac370d": {"doc_hash": "984ea0402834a87eb6a43837348a6f5e3710f11c0116cde81cf1f14345da9a38", "ref_doc_id": "73107650-0c1d-467a-8f9e-2a85dff78c1e"}, "b5b34ce3-0cb1-4672-97cf-11b544065441": {"doc_hash": "fa6a1f20409bfbefc649d1b5a65b37dc1a6408aecafec79c69b5597c45a65b9c", "ref_doc_id": "73107650-0c1d-467a-8f9e-2a85dff78c1e"}, "35298a8b-7f91-4f4a-95bf-92983732010c": {"doc_hash": "b8da2661dd80d56abf9a32ee8aae81d1b40885b9baf85dc50ec809a8b3803a8c", "ref_doc_id": "ba6488a6-f975-4588-a75f-d0b082dcf28b"}, "1040b50c-52a9-4920-beb4-dcd70eb2e18b": {"doc_hash": "e4906ba22e7fe24cc751578f067b46bf79410658d0c221e3a8ad51b724a45d88", "ref_doc_id": "ba6488a6-f975-4588-a75f-d0b082dcf28b"}, "3764afa8-ff16-4f59-8a98-d5e8e030056a": {"doc_hash": "91bf8882f7bacfc10c3204b84ef00a24a675712726c0dd9a62624a128d12c97f", "ref_doc_id": "57681db3-c712-4ebe-941e-901878af9b94"}, "4e5b2854-e70a-488d-a29c-2a9d112b08c9": {"doc_hash": "2f00078fc5896c33dc42e37b52ecb27666d1b43627acfb21267839f84fc3af01", "ref_doc_id": "57681db3-c712-4ebe-941e-901878af9b94"}, "5c7b844d-4d02-44ef-9a17-8d40bc43a24d": {"doc_hash": "17444a0ea4d604a2f4078500adf80ddfb802b0bea25594a61f99667d80e6036c", "ref_doc_id": "0f4de1f6-d75d-481b-8233-4538aa6dd01a"}, "aae345c1-f574-4e6d-906e-554557a3bea3": {"doc_hash": "0fa342dc9cbe831d68ec86228e2794be910ca74ec24bbfe460e39457b9ac6102", "ref_doc_id": "19139b82-d1b5-43d4-a7bd-e35297d8e82e"}, "4bd16d19-fff7-4bfa-ab29-de5e2d6eea51": {"doc_hash": "5a1f045cb169cb9d064445a0facf94c73c7a5b6267b7e857db8d03edf9407403", "ref_doc_id": "257e9425-9f18-4f4f-89bd-9aace4633dbe"}, "61591b94-fb42-4ab4-8bf6-4fc07098a3e3": {"doc_hash": "5fca9b6e95629371a74399a40a049f813bdf5120ac6f6f837913b2bb4400d92d", "ref_doc_id": "257e9425-9f18-4f4f-89bd-9aace4633dbe"}, "7f6322eb-a831-4b34-a5d9-e7eec7b3f9c7": {"doc_hash": "49953cc6e4345f33f27e30220d6378414cc1708f589aceb0ed7cf191bfb8ebc3", "ref_doc_id": "edff8844-dec3-413c-acb5-d95c49d37b7e"}, "64b0a2e2-8f35-42eb-8a38-ecd4aa48ef1d": {"doc_hash": "714129f3d59b3951305cb2273eaceefa8644872e699dab5d99d1eb3d5d82153e", "ref_doc_id": "edff8844-dec3-413c-acb5-d95c49d37b7e"}, "d41f9130-62e5-4140-b7eb-3690aaa6f732": {"doc_hash": "09e55f9cdcbb05630c786d5b0884cc267c6da9ef158dbe637759c5f38f8d20ba", "ref_doc_id": "fd9fdfe0-2305-4b69-97ec-f5eda22bc01c"}, "ea998542-7a20-41be-a64b-ca912e79c9d5": {"doc_hash": "8e39b5bbaa7b9fcb5ce841249f80bb333243911b35b4eb73ddca9123d2cf6f52", "ref_doc_id": "fd9fdfe0-2305-4b69-97ec-f5eda22bc01c"}, "74ddba6b-99ca-44c9-8449-11cc73cda500": {"doc_hash": "32f451664dd2196b4cad82d877bbe7471a88e616e79f662929c509c1fceaac0e", "ref_doc_id": "0a402834-f7ed-4f26-8200-9b91cdf11b4c"}, "d5428378-ba1f-4a18-a8a2-299298de8143": {"doc_hash": "b98866b7321221721a52e2871a569654a3ad22e75eb65782fcf0406a4ddbc805", "ref_doc_id": "0a402834-f7ed-4f26-8200-9b91cdf11b4c"}, "8eb9e29d-7e82-4a3e-8059-2b979e80ac68": {"doc_hash": "c44ba4af5f2d3ac9c1d7b8ec09d32489ac59ed2ebeadbde412e197e153769984", "ref_doc_id": "a051f339-922c-4b89-b4fc-b6940a45dc5a"}, "857ade76-a6a8-4a28-bd11-6ed6c78e123f": {"doc_hash": "8f6a03585f186e471e7b096c300a8c56c347dc4efb42e4d6b3d4fbe050bf0626", "ref_doc_id": "a051f339-922c-4b89-b4fc-b6940a45dc5a"}, "be4f57e5-5637-473d-a7ae-ac79fe464385": {"doc_hash": "3a27143f4200d4f0e9290cfbed2aff1cd74898fef899d74ad60e48ed5283ab23", "ref_doc_id": "7bc9bee8-cb63-4c8a-8b58-15e0baf51f92"}, "85cca884-cba4-43e1-afde-d2713a0c1180": {"doc_hash": "4518ba82cf50067997ab05aeaeac2faceea8457574e2cbb5f9c2a5bb45c9d18e", "ref_doc_id": "7bc9bee8-cb63-4c8a-8b58-15e0baf51f92"}, "041f3096-e99f-4022-bdc7-2c70a15b014f": {"doc_hash": "5bfe9aba57b7d150946832af9a145c5dbee11da824e8f8fb92d13d95f295d0f9", "ref_doc_id": "406a4296-6669-468a-bac1-42083f0c6181"}, "63f19542-2331-4d54-b081-881dd6d50775": {"doc_hash": "ab5c33e543ae93090f68f398cb2ddc6b135e287eb9e7fb2f45a28a6250b099ce", "ref_doc_id": "406a4296-6669-468a-bac1-42083f0c6181"}, "dd1a448b-490c-40cb-b9fc-f8fca2d6629c": {"doc_hash": "d9d175e49247292f137b2dc07e905c1a8f59644e7c4225ef23b858680e9487d7", "ref_doc_id": "65c138df-617c-4874-8abf-4d6feef1fe9e"}, "2c16c064-0a0f-48a1-a538-25289bd81aed": {"doc_hash": "2ee5663cbb89735a7a42db502393016dffb37e03e52bac12b4ebb9be2b6787fe", "ref_doc_id": "65c138df-617c-4874-8abf-4d6feef1fe9e"}, "13fb6ce1-69ae-4efc-b403-c84b67c09020": {"doc_hash": "7daacb8f72585d16fe506fab5abf1e2dcd2ed28a855125ffa0d84b29352266d7", "ref_doc_id": "a08a854c-ce44-4fe0-adbe-4c8c4fd402e0"}, "6e4a52a6-6ca3-45d7-8222-fa3a534edd16": {"doc_hash": "4101f7aef6a9fb95f2fae1e5aa722d3a3868dcb0147274612b58c72292c5811a", "ref_doc_id": "a08a854c-ce44-4fe0-adbe-4c8c4fd402e0"}, "925368db-e7ca-4466-af71-1e77a5b65668": {"doc_hash": "3b854477ab79723ba80154f66463120754b76b219e1da1840af4720c2844add8", "ref_doc_id": "0435b871-4048-4e5a-aa21-82cb34b77642"}, "57595640-e086-42ce-a3e5-4f12fcb124a3": {"doc_hash": "fc9fe24a9530d91adde6e81f5dc869f9132c617d13f22c8d567f2f6b85a1d626", "ref_doc_id": "0435b871-4048-4e5a-aa21-82cb34b77642"}, "46fe14bb-fad4-4f6f-8c4d-2531488cc735": {"doc_hash": "54e707e36290845aa6cfa5fe6c780b734f97ced70177f93509b5d3979e9fbd05", "ref_doc_id": "a97d10ad-fd39-4d39-bec4-f1c888d80c37"}, "04a952db-d803-4a90-8852-a299026eedfc": {"doc_hash": "a5206a800c0e53c6860c74f622b3cf2d85fa834233adba28f9fd9eb18296529b", "ref_doc_id": "a97d10ad-fd39-4d39-bec4-f1c888d80c37"}, "3473b3cd-4dfe-4f70-a391-44577870b020": {"doc_hash": "53cf6471f76ad6671a73e08684e5a6bb35ac5003fdc6d55cd9bf34337ab58d77", "ref_doc_id": "05696886-7400-41a8-9087-a90319edb626"}, "39d7b088-bda1-4b5e-810d-133b42eeacff": {"doc_hash": "e1e3600f6544d5d9c39a8cab9b8451dd758cf3517398ea9ec2fed89d8339bb28", "ref_doc_id": "05696886-7400-41a8-9087-a90319edb626"}, "59d3092a-68df-4820-83e0-80278ab6acb1": {"doc_hash": "fe24ad80dd665e523a020c99730330d3764cb2d059f08858eb148d99db502169", "ref_doc_id": "52a93de9-980f-425f-a7ed-1c2e53757d68"}, "a2af8075-0c76-4b88-967e-c7b58e914ea8": {"doc_hash": "addd79111a14fffa81e6190ab24df13a7008952eeff856012d716733ff13fd99", "ref_doc_id": "3044d36b-22ab-4bf1-9baa-3218119a8e05"}, "43602b72-2b7a-40d4-aae1-2bb3109bdbfe": {"doc_hash": "0510e2cabcc392a7acaa9507d877c29908510651c7f1a6aa503006e1a2292035", "ref_doc_id": "10f495a8-e5a3-4331-b4c3-979249d8e5df"}, "958108fd-9415-41b9-bba1-cfead2a09248": {"doc_hash": "16844d0a21e0ff17f928b3ca20efda4dbe5338c6dcde0e737cf1fba15a7026a3", "ref_doc_id": "10f495a8-e5a3-4331-b4c3-979249d8e5df"}, "435d8253-c2ed-4503-8c24-b453b9b93b64": {"doc_hash": "a487464b43ed92d7dff2ecf096bdb67e7852fc32c6c48ad69043e78c741a6324", "ref_doc_id": "59f34a73-617d-4d45-90d0-2dfd09f9a620"}, "cf26d3da-a8f2-4340-82da-b07e523bed31": {"doc_hash": "b51dda1cc3a8627f6b51d024022c39709e6443ba627e5180f43e64e7a5a4bb2a", "ref_doc_id": "59f34a73-617d-4d45-90d0-2dfd09f9a620"}, "c9e1e885-5520-47fe-8669-54c6025a7277": {"doc_hash": "d2310e15f44c81de10cd0e9dc717023853c3102d8f5cb2bb4375777830266d62", "ref_doc_id": "31969453-c8d0-4760-a7b4-a32666005be6"}, "7e7b6e0f-c4d1-4f65-99fd-2766d644e910": {"doc_hash": "2d91f84d061b578ad8853628ab5523d0828de7f6c5b54379c20a4686e76ae14a", "ref_doc_id": "31969453-c8d0-4760-a7b4-a32666005be6"}, "dc5aeeb3-beff-4583-862b-a1501e6c7846": {"doc_hash": "27b221628b4e7bf288ea45adf90e9a27e7a045f27bd174fd8f33a4d5cfddf92a", "ref_doc_id": "b20e5506-61f6-4801-ad12-da7da6debc74"}, "64e50b3e-18cf-49d8-8f54-b7046d63fae2": {"doc_hash": "b83fdd5eeabd3cba94ce503fd32b52047ab3c057b7625d77ec7d433afa50650f", "ref_doc_id": "b20e5506-61f6-4801-ad12-da7da6debc74"}, "c3e8d662-0db6-4b5f-908e-d038e72c6ad7": {"doc_hash": "c35ab92da6e70c33c477e56a9b61217ba751597e034c5945855326c9084efc22", "ref_doc_id": "7c61d074-41bb-4a7c-9faf-f35d94f86ca3"}, "3af45a22-dfbd-44c2-b71b-bc58ea17228a": {"doc_hash": "ccd2457ec1c957654527d235d1623d58e0c2d104e49b676b89ff98f59f150bb8", "ref_doc_id": "7c61d074-41bb-4a7c-9faf-f35d94f86ca3"}, "2b6806b5-e7b8-4f53-bbb8-04ce601964d2": {"doc_hash": "7c1fceffe09bab98df4fcaa7ae276d2d64ddc5d4cf8efa91ab528a3033d96d86", "ref_doc_id": "27edb7b6-3c71-4d30-8b51-f392be1b0b1c"}, "ed6b8ae4-7744-4674-870f-6086bfa05304": {"doc_hash": "6be6a0499f074c462150722ec2a3d993c1e9fa92d69dd3424563aafd63e95f4c", "ref_doc_id": "27edb7b6-3c71-4d30-8b51-f392be1b0b1c"}, "fc892e84-0da4-4245-840a-852069ad8181": {"doc_hash": "d432fe866ecfdb4a9409c341035177b793f1635f11160e9d08ae45dfb7e76731", "ref_doc_id": "6518de0d-dda4-4d06-b56a-ad870a44f66b"}, "62529978-b833-457c-831b-8094e4a10a93": {"doc_hash": "80a3bfdeab4069d22b3605e868bac1dc942617cfaf6e7c1474b1ae8da927c09e", "ref_doc_id": "4df198f5-f606-4714-a3c2-cffd8c295899"}, "32b02e64-4749-4af8-abde-96b4267d05f4": {"doc_hash": "9852dcad0030a83b053b08dc3fd8593e21f1cbc1f3626dcfacfc9d3f78b65494", "ref_doc_id": "4df198f5-f606-4714-a3c2-cffd8c295899"}, "ec2e768e-4b09-43d4-bc97-6419d2ebd26f": {"doc_hash": "743bd9bb51ee1422231c1a2455975cdb4d3fe1ecd84c77b00b845763f44e47c1", "ref_doc_id": "6633cb98-b968-447b-969d-9ff214531f79"}, "8a7d85af-eafb-4b73-a496-754080e1c711": {"doc_hash": "b6d51ab0500d40ffdf6f602637ef92ff75b91e9f0222eee05bb667d10840e792", "ref_doc_id": "6633cb98-b968-447b-969d-9ff214531f79"}, "c0526a46-e196-46d1-ae25-206a3a58006f": {"doc_hash": "bc93333064f768e467aa4db45834dfae59d1f0607bea4015ac2aacc9b8675b1f", "ref_doc_id": "f64ef16c-5d58-40d7-b16e-13a5550bd232"}, "102d1be8-4d2d-4804-aa05-aaaec189c3a8": {"doc_hash": "9b623a1d844aa989c4bce3c524fcaf29bb81b7b7e86072ed9f53751e6873969d", "ref_doc_id": "f64ef16c-5d58-40d7-b16e-13a5550bd232"}, "7bd5681e-547b-4ffe-98b9-c5f840045017": {"doc_hash": "21f03d2c76b5f9c545edeccfb9d4633c6763249b9af91a672449c3cb93a6ef32", "ref_doc_id": "dedb899c-393a-4e00-88ee-3bd1e5d2dfdb"}, "1a3fb2ef-0a42-4c8b-8212-5882c8e59ff3": {"doc_hash": "011c520fd17886122a5b59ca762173699887f85c08d0646205cd53d605f6afe6", "ref_doc_id": "dedb899c-393a-4e00-88ee-3bd1e5d2dfdb"}, "f260e5a4-ca53-4575-8201-9ae078411479": {"doc_hash": "1db626dc8932eeeb5e70b6aae3c9b0db40141657562a7aa231e9f272f6ecb10c", "ref_doc_id": "040f261a-e914-4818-9948-18bbb65aca82"}, "9bf950e2-ccbd-403e-90fd-d858d97a4e2d": {"doc_hash": "404102fbf864aec910c10a397930da2072efbc63d194501c63436392f61212ef", "ref_doc_id": "040f261a-e914-4818-9948-18bbb65aca82"}, "fb0df862-8a0c-4926-9d86-45e245183d49": {"doc_hash": "d9f607c264f76a1ae5f3950ee1bdf3c05990302e35a8ebd534aa16ba89b3f3e5", "ref_doc_id": "ad590b0f-21fe-41a9-a41b-fedf22806134"}, "5420fbac-7c32-4b10-a748-aa149a857223": {"doc_hash": "32143edc9f46ab85643b88d3fb0497e6a39f69d81c2130e29a887b0188ff581b", "ref_doc_id": "ad590b0f-21fe-41a9-a41b-fedf22806134"}, "e975a0d2-165f-4f97-8855-53d42cd0638f": {"doc_hash": "c88819cf6ece2bd3fdd77dd64c13bbdf1186e0abc69f6fb8b16b537bcf9d9f51", "ref_doc_id": "1ec8c8b1-2c08-4d68-9769-aafcb27956e7"}, "57953232-be82-44ce-97d3-910966bdd746": {"doc_hash": "5f40ca128d69dbe47f34d3156dde8b15f4c0108db5156baa107bd7f950bb8800", "ref_doc_id": "598dad45-b79a-4f0f-b791-626974833e17"}, "c0d270c5-dbca-42ea-9e99-c24e5b966712": {"doc_hash": "cc94b13d953d2c2ad6c9ae063f9271fe04a39e61735783d41cb05d3ddeb9770f", "ref_doc_id": "e5f4a401-1e88-462f-8b8b-405dfd8ac7e4"}, "a8f3c25a-b9f4-4c20-aba3-1e29d643033c": {"doc_hash": "76f71b34409583d1f86716b28ae382f8e40131da06904cff29782de21b1b3ba1", "ref_doc_id": "e5f4a401-1e88-462f-8b8b-405dfd8ac7e4"}, "28744c52-322a-4899-a6e0-fedd5b134458": {"doc_hash": "37a2c5057a2d9b4ca6ba7c740fe389c55f6d74d23240c0e02fe3cd3592ec65cb", "ref_doc_id": "6742c4da-7e0a-4a21-9fe4-e182dd0e221c"}, "2e0985ac-c3f2-42e1-a1fe-aade395804e8": {"doc_hash": "4ae285d902f9a15a42623756265b34dc968ee3638aa1680064c57d980a17ada4", "ref_doc_id": "6742c4da-7e0a-4a21-9fe4-e182dd0e221c"}, "f8e35205-bd27-4b38-a90b-1c72ba35fe8f": {"doc_hash": "0adf93c852820412cec71cee8fe3a0d50aa1e61f4a439eb62e020542f9425a2a", "ref_doc_id": "9e9f0a56-7e29-4909-88bd-73e467a5eaeb"}, "1c972d34-d3b8-4ec5-9acb-5d1fc239bf9b": {"doc_hash": "b958947e62d869ba0dcbb8d7923c6e76618dd5287851bd74ef64b992b52a4d82", "ref_doc_id": "9e9f0a56-7e29-4909-88bd-73e467a5eaeb"}, "d1a2ad8d-d439-4461-af79-9b425b652b1c": {"doc_hash": "aee2a92c67ee3cbb1e3c79cb23644d671bff93252f41dce83d0965a276ef54a2", "ref_doc_id": "a7456b3b-d65d-4c0a-a434-f699011c3d80"}, "43b1053b-0a4a-46ae-a0ee-9fc41955ad9e": {"doc_hash": "ed797efdb4fd56e7eadeb98335596da74e3aee2725301b244f942666ac61cd05", "ref_doc_id": "84e8d558-a595-4bf4-9587-f7ede9273991"}, "42ec8213-57f8-4ef5-b272-27e16f7caaa7": {"doc_hash": "e7606dee7ac83b53756e8113a3d74d6eeea36ee9da2386e08f2695d37fb3d172", "ref_doc_id": "84e8d558-a595-4bf4-9587-f7ede9273991"}, "cdd72c20-924f-444d-8f21-a8fe217f38c5": {"doc_hash": "312ae5b8707c296847fbb4135d430dab40e2b0a429580997a12a31a78dff7f3a", "ref_doc_id": "fd074a23-19f2-4efd-bd4a-ff87a76d76d0"}, "6bf0c098-2f15-4fa6-8ba2-2765e2d4f404": {"doc_hash": "0630f3faaccd461c07695a182ea9e82314f8f500cbe9899e995c4357a8fabaab", "ref_doc_id": "11f1db50-1d2a-41be-8e60-894c2a28ce8b"}, "3171923f-0c02-4444-a14b-c2814f035ea3": {"doc_hash": "856d3c1b2659a460c3d0c1eba3c146f16a1bb585464d530bafddb84f24c04809", "ref_doc_id": "ba107626-2d72-4316-b3a1-fa3799a192fe"}, "a65709ff-3b33-4f8e-9cdc-44bb8a0be449": {"doc_hash": "d2d244f4d0eda72d96c6d5efc6d589a91b9802e13a0dfb73d6eb280b7f0380ea", "ref_doc_id": "0f00fb90-1605-4f75-9509-e5dda2c7db19"}, "a541e458-9efa-4027-9f76-d4123e9752d6": {"doc_hash": "04a3e1f2ee14e57640dfcba6ab09eccce9398290f8f261c96539233aa94abf6b", "ref_doc_id": "0f00fb90-1605-4f75-9509-e5dda2c7db19"}, "bc492931-4fe0-4d5c-87d4-a9ab193f12be": {"doc_hash": "fa896d8a12e1943930c965879201f5e57648325d502abbdf6496790ba76be64d", "ref_doc_id": "154ed6bc-79e8-4bc9-8426-2480a3c0238c"}, "1cd43094-43e0-4ba5-817f-d4145d585e6b": {"doc_hash": "94ad38df10a01b87f94123b22f5f0ff958f612e76b3fe5aeae357f1ee20cf667", "ref_doc_id": "0e9a8331-517d-4453-97f5-037b8d82d3d7"}, "b3c9600a-369d-456c-b8b8-679f00fecd55": {"doc_hash": "f49b20544135b08a7da459581c6bedfd71c96d600aefc60dc0d70d89798013aa", "ref_doc_id": "0e1eb5a1-f9db-4fb3-bb17-613d176c030b"}, "3fbb6995-dc85-435f-8fe5-2f790713c197": {"doc_hash": "2e18f2b632a681f880ae0a76bb490ab7cada3ffc7e6c649904a2af91e3049450", "ref_doc_id": "44ee76b9-bf15-4cc8-95fd-aa2265dbf3ce"}, "0cf5db57-0271-4cf3-b67d-7f6eff1d9cf2": {"doc_hash": "ebab343c445163f12ef3bb159f51b0d2977d82a1c6a6ddcf20ada77f25364835", "ref_doc_id": "0e371c83-9b2f-4a26-89e0-c6697ab820d8"}, "2d8bb20d-b87b-4ad1-9b35-af47cff055dd": {"doc_hash": "f7133e919e4af4cd108b2736856890bef924871ec16e1b51631bdd205caa6e0a", "ref_doc_id": "203af889-47e6-413d-b785-fcfee2a17832"}, "e30d4ba2-7e62-4193-ab40-0ca19977cae5": {"doc_hash": "0cebd1f9335aef0ed3d14a8a3965d2bce794547d862160d4b2275f5b060aaf07", "ref_doc_id": "074d81c9-eb70-40c1-9099-2d5eaa51fc93"}, "b8ead17b-639b-4db2-b46e-d74e26b71cb3": {"doc_hash": "1a89a93b1374b06cc50721f11a9cc0c9412cde18bb4abea32748b68d4ebda790", "ref_doc_id": "074d81c9-eb70-40c1-9099-2d5eaa51fc93"}, "273bc17e-2ea4-49da-af39-40cfc92f1752": {"doc_hash": "02bcee9c2c341459ec80f9787cec19c5dc2d86bd892d48b436e4cab85a6d2a33", "ref_doc_id": "963338af-f8c1-4291-a9b0-4686a6eb7f94"}, "8081d5a9-f1a3-453a-bb77-1a7a6343f332": {"doc_hash": "4de27ff195ba831dd10bcf94c6dac3216ef9a58f5c880410321819afc12cecde", "ref_doc_id": "963338af-f8c1-4291-a9b0-4686a6eb7f94"}, "4dac1fdc-d778-4658-b30c-4f48a7b3eb4c": {"doc_hash": "ad9996d3ecd4fd4de5e7b580df5fb680bc5114fef0725c439958482c6a8fbac4", "ref_doc_id": "ab286212-c22d-499d-8d31-60d0a2af8264"}, "09ad95d7-15a5-4ea4-b423-fbdecc0a8585": {"doc_hash": "820024c671770c3efa2a98755b921abb22fdf7a88922ac424916837c5283b2e1", "ref_doc_id": "0ceb5ddd-d825-40cb-b26d-cdbe59a738f6"}, "9497ee67-f2e1-406e-a923-8b6e55fc8775": {"doc_hash": "2456de6b25e1ec6be0fdb6c61ea7b6ea3ad76aed4ae37869a066993b3d03ad0c", "ref_doc_id": "d4e6f2f1-1603-4c5c-96fd-71b098f988db"}, "c628c85a-5f1c-495b-b97e-22608f93d7a1": {"doc_hash": "69e8776e4a4f9eb09918a3c2019da63ada92f123a577b7fbb93a48fd5b3c4a7d", "ref_doc_id": "d4e6f2f1-1603-4c5c-96fd-71b098f988db"}, "e32034af-9926-49f8-8023-e4a452ec43c6": {"doc_hash": "7daee48782547bc567d09c786d4e0cfab4c4592f974eb1d4aabe108a78a4ece9", "ref_doc_id": "076a8af5-e36e-4070-9ae0-3d014e547de1"}, "297dbeac-874a-4aa1-a9de-1ed25528bea1": {"doc_hash": "e60f08f9483056075c0d2c3d652c97c8c909fbbf4e04f964b6975e633fa17682", "ref_doc_id": "076a8af5-e36e-4070-9ae0-3d014e547de1"}, "8aaae5eb-f328-4746-b399-07994d2b0acb": {"doc_hash": "a2994ef71c78814b935c054a37b6b0eaab118063e5166a753e96975906bc8eb9", "ref_doc_id": "e13b5f59-ed8a-42a8-8569-db7f6f108ff0"}, "13be01f6-0b0c-4a82-bdad-1b2a0a99128c": {"doc_hash": "7ebd38598f4d172aec72ff3e5f81704b40b9852a729ea8d0ce4fede09076864c", "ref_doc_id": "e13b5f59-ed8a-42a8-8569-db7f6f108ff0"}, "6c1a6d24-8acc-4c3f-94c9-4e876126c15c": {"doc_hash": "e202b912773371d66102df7f1c9b35ca5eb8a1e37ee294809bebd801e4bb5754", "ref_doc_id": "bf41e409-ceaa-4311-8982-adc573e48688"}, "0a363440-8cc2-458e-a73e-310ec1288106": {"doc_hash": "be3d844b46c57e22e2a5e7ddfdaf7e70db55c455f0f4012a8b7cc752ab4aaf05", "ref_doc_id": "bf41e409-ceaa-4311-8982-adc573e48688"}, "9bbe9300-82ab-48f9-924e-25c28da0c22c": {"doc_hash": "16b40d6ae8535d52bafb9332506b529f52aed7b1324bb8ce77de76464a0e2ae8", "ref_doc_id": "1b1dcd45-cc0b-40da-a634-3825d04c51b7"}, "a58397a2-e7ac-42c7-bd5a-06a0c0a7dc12": {"doc_hash": "ce346e200cf0efdb622eb84c5732f93867b14092495e3266a9b0709b964aa0dc", "ref_doc_id": "1b1dcd45-cc0b-40da-a634-3825d04c51b7"}, "46074122-c9e3-4232-a7d4-2e98354962ee": {"doc_hash": "4f10a42f785395ee40f0ec9fc06f75d5a3f518558c0fe0f923784e4f84248966", "ref_doc_id": "1e067fee-1de3-4a11-bb31-b19b4c9e379f"}, "61a50473-f583-42dd-91be-5973c3121201": {"doc_hash": "47be80372113ac829a8ecf47422c38ac02742966cf5a3f8a3b9c3cf6bdb87f4c", "ref_doc_id": "1e067fee-1de3-4a11-bb31-b19b4c9e379f"}, "e1448d1a-02c9-4f78-897b-a75f5e49518d": {"doc_hash": "161889d9f69f7b4b649a6f324b304da5caae01f37eeefe72f96afc789a0b4111", "ref_doc_id": "1b371e23-4721-4cb3-a324-15e5c2cdcbdd"}, "27fc9f8d-9779-4761-aefc-081a9cf1604e": {"doc_hash": "7d15deec3bec2178446e4b933f144f970a21a7927ba07f70d30c347750836839", "ref_doc_id": "1b371e23-4721-4cb3-a324-15e5c2cdcbdd"}, "fc5fa2b7-8871-41b4-ac91-b27d57ca55df": {"doc_hash": "f6ccc621d40088792056012b1e3d05464fd65fbdb56cdca839082bae9bc88625", "ref_doc_id": "44e46b2f-9826-4bf3-bc87-7dac192228b9"}, "bddd3a50-6b0f-465e-92a0-95c8fa716e8b": {"doc_hash": "e911abccebe9900f802dc35366ffa3351554105e7c14c47399bad2e24cf2518f", "ref_doc_id": "44e46b2f-9826-4bf3-bc87-7dac192228b9"}, "802a0e3c-1c99-4f2f-bb01-1cd426dcc64d": {"doc_hash": "7e1d211af2d12c8512c1a99ecfd4fd4b60a615630a801fbfa59dcc9d1a875aad", "ref_doc_id": "03e1dd28-c2b5-4961-bff2-1de7c3789fd3"}, "56bde1d1-72a9-42bd-b1cb-e76711c9c745": {"doc_hash": "0385018c96a16a7245af0c2eca21874e0e8da0ba47231ad1ec6f30aa1d4de22d", "ref_doc_id": "03e1dd28-c2b5-4961-bff2-1de7c3789fd3"}, "b78e3b9d-35dd-49b8-a721-e3b6d14f0300": {"doc_hash": "a0ffc7ff936f4678ea3a48f76650aa47b2c1f5ee7fadf81d37a4636c166f4741", "ref_doc_id": "2d2bce7f-3073-4a67-9e1b-15cfb44cc4a8"}, "5063748f-bc7b-4a58-ae66-8a06e22d00a9": {"doc_hash": "8710fc93f4eea2f4c3887aa7b6e4aa6d9b6b6bf20f461b67b97235567b78a95d", "ref_doc_id": "2d2bce7f-3073-4a67-9e1b-15cfb44cc4a8"}, "f535e700-1b9a-4b90-a385-9a1ae62c3539": {"doc_hash": "fad3ac9795352ea06785cf23da2a47b381a8ee70ee63ee360c1bfa4aff3bf9f5", "ref_doc_id": "318e8d10-9e10-4124-b963-da894ef2e3eb"}, "427fea22-4fef-4742-af36-2be94792a0ea": {"doc_hash": "24fe423612546b081e2d88aac0bb2abf9a92f6be82e3d4799b105e47f10d903c", "ref_doc_id": "318e8d10-9e10-4124-b963-da894ef2e3eb"}, "e09d9d99-060d-41e7-adf9-27caf18d68ce": {"doc_hash": "ed377127b4f76f62d26faf1b817ca904ca1a51bbd6b3a2982b123b77d41abc98", "ref_doc_id": "d14f1254-258a-4f0c-9d42-50293e276bb5"}, "41291bbe-f253-424d-a1af-db318e580e5e": {"doc_hash": "8bf1e87b85dbcb5c3bbcccb339b93405fb09c765b0ae443f80ac15767b97c151", "ref_doc_id": "d14f1254-258a-4f0c-9d42-50293e276bb5"}, "7bc0591f-1dd1-4c1e-8673-cf0f4e9b7895": {"doc_hash": "a6d01c9a8ec8b329a2240ac55e728abd37ffe5b5157f7219ef109fa8d50e587b", "ref_doc_id": "6bee79f4-ff09-42e7-a221-c1db43a284b2"}, "b61d7613-b518-4049-9a3f-9bac9b73eb74": {"doc_hash": "d9bd28401a70188f37ca21e5d3b18d361e50abd51d358d899992acac3f9f9ac0", "ref_doc_id": "6bee79f4-ff09-42e7-a221-c1db43a284b2"}, "9b8337c3-efd1-4b76-932f-5ac249542895": {"doc_hash": "37df03426700dc56f1bf26f81dc4ee421304853861f409d607eae775af6e85cd", "ref_doc_id": "97ae1dac-86ff-4854-ad6d-4674257dfce4"}, "6b21f0fa-43f1-42b4-bba0-97d8142512ca": {"doc_hash": "929512418db177f2b5adcf4f9b00090bc05e0ee5b131a5bd45d362f2ef24fe1e", "ref_doc_id": "be32a1b6-df02-4633-b9a8-9f3a57d9a340"}, "e217ec36-ad39-4c40-99ed-54e10b954678": {"doc_hash": "964b9a6c5f61660dcce57053c784e8eb879149c150241fa68e6086c2edec8083", "ref_doc_id": "eaae58fd-baa7-4d8f-a627-4531e2c1399b"}, "ab350db6-6467-4be9-a297-ea6068301555": {"doc_hash": "424365d43930189cf422ee1ade8a491ebcf4e442c0ec55c24b0b4b95bf4b3f4f", "ref_doc_id": "9ea34853-1573-4000-88c8-873919d0f1fe"}, "59101f6d-aabb-4146-b573-f2e8ea767ba6": {"doc_hash": "68100f25e7b0f82f7dd8da826dde561f4e936a3203a034bd7c63bb76eb6fe0ef", "ref_doc_id": "5f97977f-a8ef-4908-90fa-7bb879980d03"}, "bf876f47-0b35-4a07-abe9-f73b64a1c677": {"doc_hash": "5e4595539cda087ff29287abbbf33cc028d4a8676c58ec40c5c06340c9de6fcb", "ref_doc_id": "e917ed70-7707-4875-b958-654c0af5fb63"}, "ff30797b-b5f4-4e6f-94f9-d328b37fd56a": {"doc_hash": "6a5fe59d4fcfcf28c61b1cbf6e3fc6e147ba5d597ef1267ec717590bc6ca831b", "ref_doc_id": "9c943d83-50a6-4df9-946a-400040557fef"}, "7b5f28c7-ecf9-409e-a0aa-230a0778d775": {"doc_hash": "bba27ac8fbb1b07f867ca0e542eaceb123cd3e17d1df900862c915d401aa4252", "ref_doc_id": "5b0d908d-ca02-4ebf-bf71-35f841b2eb04"}, "fa868cf2-6638-4e34-91f9-e2ed9301d35f": {"doc_hash": "5decd18e8c31784d036398ec728bd1da64c8c264abf11f770a1b88a46d6d1e32", "ref_doc_id": "d06ef884-b1a9-4831-b274-d9bca4368733"}, "7da151ff-3a8b-4874-8b1d-6676594fc3bd": {"doc_hash": "c84898c58d6c46c76a205dc11e993e922053f59710a2c0bc06c264932915d29c", "ref_doc_id": "ef3b3112-6b8d-4f3a-952a-bfa2d7916467"}, "08d26031-30a3-4c80-a4f4-6f3a57ae9f0a": {"doc_hash": "ac4ec04eef479021f479f7fab4cb86b6df2acb9d9fee899efbfa218fe7a2e818", "ref_doc_id": "6164f625-9d41-4bb1-9d65-fbba101f6963"}, "a684f2f9-1545-4572-b062-384505162233": {"doc_hash": "e8136c24b7eede8d377b3875a274590f3bf2403d1a64b3d89e2be03e8f29dc98", "ref_doc_id": "a596c78e-f592-4709-8c55-a2a56de14fc7"}, "3cb5a81a-17d9-4bdb-829e-6d44741be470": {"doc_hash": "e38d4ad6b6b1d18b809142bf8a491e94ba9a33b977706a942ea34cc01f938fe9", "ref_doc_id": "e6a67f75-fb4d-4909-898b-bb5f0750403a"}, "c7d715ea-b6ac-4aa5-877d-20861507801d": {"doc_hash": "846291a34a81b38aaf7a2edb423482fa93a17106e69d8283b01536e03d00e01d", "ref_doc_id": "e6a67f75-fb4d-4909-898b-bb5f0750403a"}, "7bf7b80c-a412-4ebf-b46c-51dde442924a": {"doc_hash": "ec643b4b199f6243e049209025376bcf09ddeaacddc01b1f80f9f845610860bc", "ref_doc_id": "8314f153-2970-455c-b99c-e00aff13d161"}, "79a042d5-b02a-42f2-894b-41efa2d8451d": {"doc_hash": "8e6fecc3745542327aec9ebe09b778347d1dde81bd29e9a6248766f39b3cf7b9", "ref_doc_id": "c0fcdda1-7ef3-472e-8398-ed41d923cfaf"}, "bdd40762-53a2-4381-8d42-010e8df959e0": {"doc_hash": "0b07b120490797262997cf49cd707d588b3c237aacbddeea1b6064efe5e79975", "ref_doc_id": "1587cc8c-90a7-4be2-973a-c0af1d2e402d"}, "9e4f1386-0807-4394-a6fd-8c531db7a2f8": {"doc_hash": "6fb4fd214f60b758c4fdd56d21f9d8c2a705ef3157dc8ed10f62371666bf76eb", "ref_doc_id": "1f2e0772-14a7-493a-99ce-397a9b2acc56"}}, "docstore/data": {"1d1e7d17-3d74-42d4-b1ad-de61da5659c6": {"__data__": {"id_": "1d1e7d17-3d74-42d4-b1ad-de61da5659c6", "embedding": null, "metadata": {"page_label": "1", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4628515-1f2f-47ea-86f6-25d0d76c1035", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4b26ca0c1eaa665a7cdbce6f1abf937c4d1b76e5f3b8e4f12c3296f9e203e617"}}, "hash": "4b26ca0c1eaa665a7cdbce6f1abf937c4d1b76e5f3b8e4f12c3296f9e203e617", "text": "arXiv:1508.04025v5  [cs.CL]  20 Sep 2015EffectiveApproaches to Attention-basedNeural Machine Tr anslation\nMinh-Thang Luong HieuPham Christopher D.Manning\nComputerScience Department,Stanford University,Stanfo rd, CA 94305\n{lmthang,hyhieu,manning }@stanford.edu\nAbstract\nAn attentional mechanism has lately been\nused to improve neural machine transla-\ntion (NMT) by selectively focusing on\nparts of the source sentence during trans-\nlation. However, there has been little\nwork exploring useful architectures for\nattention-based NMT. This paper exam-\ninestwosimpleandeffectiveclassesofat-\ntentional mechanism: a globalapproach\nwhich always attends to all source words\nand alocalone that only looks at a subset\nofsourcewordsatatime. Wedemonstrate\ntheeffectivenessofbothapproachesonthe\nWMT translation tasks between English\nand German inboth directions. With local\nattention, we achieve a signi\ufb01cant gain of\n5.0BLEUpoints overnon-attentional sys-\ntems that already incorporate known tech-\nniques such as dropout. Our ensemble\nmodel using different attention architec-\ntures yields a new state-of-the-art result in\nthe WMT\u201915 English to German transla-\ntion task with 25.9 BLEU points, an im-\nprovement of 1.0 BLEU points over the\nexisting best system backed by NMT and\nann-gram reranker.1\n1 Introduction\nNeural Machine Translation (NMT) achieved\nstate-of-the-art performances in large-scale trans-\nlation tasks such as from English to French\n(Luong et al., 2015) and English to German\n(Jean et al., 2015). NMT is appealing since it re-\nquires minimal domain knowledge and is concep-\ntually simple. The model by Luong et al. (2015)\nreadsthroughallthesourcewordsuntiltheend-of-\nsentence symbol <eos>isreached. It then starts\n1All our code and models are publicly available at\nhttp://nlp.stanford.edu/projects/nmt .B C D <eos> X Y ZXY Z <eos>\nA\nFigure 1: Neural machine translation \u2013 a stack-\ning recurrent architecture for translating a source\nsequence A B C D into a target sequence X Y\nZ.Here,<eos>marks the end of asentence.\nemittingonetarget wordatatime,asillustrated in\nFigure1. NMTisoftenalargeneuralnetworkthat\nistrainedinanend-to-endfashionandhastheabil-\nitytogeneralizewelltoverylongwordsequences.\nThis means the model does not have to explicitly\nstore gigantic phrase tables and language models\nas in the case of standard MT; hence, NMT has\na small memory footprint. Lastly, implementing\nNMT decoders is easy unlike the highly intricate\ndecoders in standard MT(Koehn et al., 2003).\nIn parallel, the concept of \u201cattention\u201d has\ngained popularity recently in training neural net-\nworks, allowing models to learn alignments be-\ntween different modalities, e.g., between image\nobjects and agent actions in the dynamic con-\ntrol problem (Mnih et al., 2014), between speech\nframes and text in the speech recognition task\n(?), or between visual features of a picture and\nits text description in the image caption gener-\nation task (Xuet al.,2015). In the context of\nNMT,Bahdanau et al. (2015) has successfully ap-\nplied such attentional mechanism to jointly trans-\nlate and align words. To the best of our knowl-\nedge, there has not been any other workexploring\ntheuse of attention-based architectures for NMT.\nIn this work, we design, with simplicity and ef-", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "924bd91d-ed72-4c4e-9100-6a2c3ccc96b8": {"__data__": {"id_": "924bd91d-ed72-4c4e-9100-6a2c3ccc96b8", "embedding": null, "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb49f97a-3aa0-4212-8005-fb5c70cb8ff8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4d5c5cff06f8310a32694671acfcd6b01e2af25dc8cd76ecf9520d232271ffa0"}, "3": {"node_id": "5a528f2d-0d66-479a-81c9-046f97289c43", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d334659ff76b02ec2c124f4399631be65569aa9f2886a7327b487e482f05dd14"}}, "hash": "f829665f3053715bbd76fe99bec451c1dae53d35a747139be39815de6d8c165b", "text": "fectiveness in mind, two novel types of attention-\nbased models: a globalapproach in which all\nsourcewordsareattendedanda localonewhereby\nonly a subset of source words are considered at a\ntime. The former approach resembles the model\nof (Bahdanau et al., 2015) but is simpler architec-\nturally. The latter can be viewed as an interesting\nblend between the hardandsoftattention models\nproposed in (Xuet al., 2015): it is computation-\nally less expensive than the global model or the\nsoft attention; atthesametime, unlike thehardat-\ntention, the local attention is differentiable almost\neverywhere, making it easier to implement and\ntrain.2Besides, we also examine various align-\nment functions for our attention-based models.\nExperimentally, we demonstrate that both of\nour approaches are effective in the WMT trans-\nlation tasks between English and German in both\ndirections. Our attentional models yield a boost\nof up to 5.0 BLEU over non-attentional systems\nwhich already incorporate known techniques such\nas dropout. For English to German translation,\nwe achieve new state-of-the-art (SOTA) results\nfor both WMT\u201914 and WMT\u201915, outperforming\nprevious SOTA systems, backed by NMT mod-\nels andn-gram LM rerankers, by more than 1.0\nBLEU. We conduct extensive analysis to evaluate\nourmodelsintermsoflearning, theability tohan-\ndlelongsentences, choices ofattentional architec-\ntures, alignment quality, and translation outputs.\n2 Neural MachineTranslation\nA neural machine translation system is a neural\nnetworkthat directly modelstheconditional prob-\nabilityp(y|x)of translating a source sentence,\nx1,...,x n, to a target sentence, y1,...,ym.3A\nbasic form of NMT consists of two components:\n(a) anencoderwhich computes a representation s\nfor each source sentence and (b) a decoderwhich\ngenerates one target word at a time and hence de-\ncomposes the conditional probability as:\nlogp(y|x) =/summationdisplaym\nj=1logp(yj|y<j,s)(1)\nA natural choice to model such a de-\ncomposition in the decoder is to use a\n2There is a recent work by Gregor etal. (2015), which is\nvery similar to our local attention and applied to the image\ngeneration task. However, as we detail later, our model is\nmuch simpler andcan achieve good performance for NMT.\n3All sentences are assumed to terminate with a special\n\u201cend-of-sentence\u201d token <eos>.recurrent neural network (RNN) architec-\nture, which most of the recent NMT work\nsuch as (Kalchbrenner and Blunsom, 2013;\nSutskever et al., 2014; Choet al.,2014;\nBahdanau et al., 2015; Luong et al.,2015;\nJean et al., 2015) have in common. They, how-\never, differ in terms of which RNN architectures\nare used for the decoder and how the encoder\ncomputes the source sentence representation s.\nKalchbrenner and Blunsom (2013) used an\nRNN with the standard hidden unit for the\ndecoder and a convolutional neural network for\nencoding the source sentence representation. On\nthe other hand, both Sutskever et al. (2014) and\nLuong et al. (2015) stacked multiple layers of an\nRNN with a Long Short-Term Memory (LSTM)\nhidden unit for both the encoder and the decoder.\nChoet al. (2014), Bahdanau et al. (2015), and\nJean et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5a528f2d-0d66-479a-81c9-046f97289c43": {"__data__": {"id_": "5a528f2d-0d66-479a-81c9-046f97289c43", "embedding": null, "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb49f97a-3aa0-4212-8005-fb5c70cb8ff8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4d5c5cff06f8310a32694671acfcd6b01e2af25dc8cd76ecf9520d232271ffa0"}, "2": {"node_id": "924bd91d-ed72-4c4e-9100-6a2c3ccc96b8", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "f829665f3053715bbd76fe99bec451c1dae53d35a747139be39815de6d8c165b"}}, "hash": "d334659ff76b02ec2c124f4399631be65569aa9f2886a7327b487e482f05dd14", "text": "(2014), Bahdanau et al. (2015), and\nJean et al. (2015)alladoptedadifferent versionof\nthe RNN with an LSTM-inspired hidden unit, the\ngatedrecurrentunit(GRU),forbothcomponents.4\nIn more detail, one can parameterize the proba-\nbility of decoding each word yjas:\np(yj|y<j,s) = softmax( g(hj))(2)\nwithgbeing the transformation function that out-\nputs a vocabulary-sized vector.5Here,hjis the\nRNNhidden unit, abstractly computed as:\nhj=f(hj\u22121,s), (3)\nwherefcomputes the current hidden state\ngiven the previous hidden state and can be\neither a vanilla RNN unit, a GRU, or an LSTM\nunit. In (Kalchbrenner and Blunsom, 2013;\nSutskever et al., 2014; Choet al.,2014;\nLuong et al., 2015), the source representa-\ntionsis only used once to initialize the\ndecoder hidden state. On the other hand, in\n(Bahdanau et al.,2015; Jean et al., 2015) and\nthis work, s, in fact, implies a set of source\nhidden states which are consulted throughout the\nentire course of the translation process. Such an\napproach isreferred toasanattention mechanism,\nwhich wewill discuss next.\nIn this work, following (Sutskever et al.,2014;\nLuong et al., 2015), we use the stacking LSTM\narchitecture for our NMT systems, as illustrated\n4TheyallusedasingleRNNlayerexceptforthelattertwo\nworks whichutilizeda bidirectional RNNfor the encoder.\n5Onecanprovide gwithotherinputssuchasthecurrently\npredictedword yjas in(Bahdanau et al.,2015).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3a5db9c7-ca84-4422-be6b-cf4ad6731a48": {"__data__": {"id_": "3a5db9c7-ca84-4422-be6b-cf4ad6731a48", "embedding": null, "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cc76dff-87a2-46f4-9964-fe98b162f4a2", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "aefc2f0443ffbf50e7bb4750f10921b1e776b0166b6ceaf3460f0be097ff3219"}, "3": {"node_id": "19f72316-d940-4728-bfd3-6840b75b97d8", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9f95825d8f9cb862994536e3e4b41ece0db7cf7ca73364703bdeaef820bc2ad9"}}, "hash": "b104620450cc648e99f1093db9a3ede41eafe43970d035ae3a0f5b28ba352c10", "text": "in Figure 1. We use the LSTM unit de\ufb01ned in\n(Zaremba et al., 2015). Our training objective is\nformulated asfollows:\nJt=/summationdisplay\n(x,y)\u2208D\u2212logp(y|x)(4)\nwithDbeing our parallel training corpus.\n3 Attention-based Models\nOur various attention-based models are classifed\nintotwobroad categories, globalandlocal. These\nclasses differ in terms of whether the \u201cattention\u201d\nis placed on all source positions or on only a few\nsource positions. We illustrate these two model\ntypes inFigure 2and 3 respectively.\nCommontothesetwotypesofmodelsisthefact\nthatateachtimestep tinthedecoding phase,both\napproaches \ufb01rst take as input the hidden state ht\nat the top layer of a stacking LSTM. The goal is\nthentoderiveacontext vector ctthat captures rel-\nevant source-side information to help predict the\ncurrent target word yt. While these models differ\nin how the context vector ctis derived, they share\nthe samesubsequent steps.\nSpeci\ufb01cally,giventhetargethiddenstate htand\nthe source-side context vector ct, we employ a\nsimple concatenation layer to combine the infor-\nmationfrombothvectorstoproduceanattentional\nhidden state asfollows:\n\u02dcht= tanh(Wc[ct;ht]) (5)\nTheattentional vector \u02dchtisthenfedthroughthe\nsoftmax layer to produce the predictive distribu-\ntion formulated as:\np(yt|y<t,x) = softmax( Ws\u02dcht)(6)\nWe now detail how each model type computes\nthe source-side context vector ct.\n3.1 Global Attention\nThe idea of a global attentional model is to con-\nsider all thehidden states of the encoder when de-\nriving the context vector ct. In this model type,\na variable-length alignment vector at, whose size\nequalsthenumberoftimestepsonthesourceside,\nis derived by comparing the current target hidden\nstatehtwith each source hidden state \u00afhs:\nat(s) = align( ht,\u00afhs) (7)\n=exp/parenleftbig\nscore(ht,\u00afhs)/parenrightbig\n/summationtext\ns\u2032exp/parenleftbig\nscore(ht,\u00afhs\u2032)/parenrightbigyt\n\u02dcht\nct\nat\nht\u00afhsGlobal align weightsAttention Layer\nContext vector\nFigure2: Globalattentionalmodel \u2013ateachtime\nstept, the model infers a variable-length align-\nment weight vector atbased on the current target\nstatehtand all source states \u00afhs. A global context\nvectorctis then computed as the weighted aver-\nage, according to at, over all the source states.\nHere,scoreisreferredasa content-based function\nfor which weconsider three different alternatives:\nscore(ht,\u00afhs)=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3h\u22a4\nt\u00afhs dot\nh\u22a4\ntWa\u00afhs general\nv\u22a4\natanh/parenleftbig\nWa[ht;\u00afhs]/parenrightbig\nconcat\nBesides,inourearlyattemptstobuildattention-\nbased models, we use a location-based function\nin which the alignment scores are computed from\nsolely thetarget hidden state htasfollows:\nat= softmax( Waht) location (8)\nGiventhealignment vectorasweights,thecontext\nvectorctiscomputedastheweightedaverageover\nall the source hidden states.6\nComparisonto(Bahdanau et al., 2015) \u2013While\nour global attention approach is similar in spirit\nto the model proposed by Bahdanau et al. (2015),\nthereareseveralkeydifferenceswhichre\ufb02ecthow\nwe have both simpli\ufb01ed and generalized from\nthe original model. First, we simply use hid-\nden states at the top LSTM layers in both the\nencoder and decoder as illustrated in Figure 2.\nBahdanau et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "19f72316-d940-4728-bfd3-6840b75b97d8": {"__data__": {"id_": "19f72316-d940-4728-bfd3-6840b75b97d8", "embedding": null, "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cc76dff-87a2-46f4-9964-fe98b162f4a2", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "aefc2f0443ffbf50e7bb4750f10921b1e776b0166b6ceaf3460f0be097ff3219"}, "2": {"node_id": "3a5db9c7-ca84-4422-be6b-cf4ad6731a48", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b104620450cc648e99f1093db9a3ede41eafe43970d035ae3a0f5b28ba352c10"}}, "hash": "9f95825d8f9cb862994536e3e4b41ece0db7cf7ca73364703bdeaef820bc2ad9", "text": "Bahdanau et al. (2015), on the other hand, use\nthe concatenation of the forward and backward\nsource hidden states in the bi-directional encoder\n6Eq. (8) implies that all alignment vectors atare of the\nsame length. For short sentences, we only use the top part of\natand forlong sentences, weignore words near the end.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "941c1ae7-ec72-49a0-aff4-0deaf59a32cf": {"__data__": {"id_": "941c1ae7-ec72-49a0-aff4-0deaf59a32cf", "embedding": null, "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28734f21-e5d5-418b-8a33-0318bc753a4e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "80e3654153b0630bd04ccea0976685269b790cfddaef17c6ff32c9b0be73d837"}, "3": {"node_id": "af992b63-bba3-4858-aadb-8e1194b8d90c", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "f37649ad35639581c33b124c0d586dd78f7ed8b3271f36f1c0cea145ce7688d2"}}, "hash": "62bbee8dc8d00b39e58b1d99d6bd893a52de74f0a616eaae9a918abab639b625", "text": "yt\n\u02dcht\nct\nat\nhtpt\n\u00afhsAttention Layer\nContext vector\nLocal weightsAligned position\nFigure3: Local attention model \u2013themodel \ufb01rst\npredictsasinglealignedposition ptforthecurrent\ntargetword. Awindowcenteredaroundthesource\npositionptis then used to compute a context vec-\ntorct, a weighted average of the source hidden\nstates in the window. The weights atare inferred\nfrom the current target state htand those source\nstates\u00afhsinthe window.\nand target hidden states in their non-stacking uni-\ndirectionaldecoder. Second,ourcomputationpath\nis simpler; we go from ht\u2192at\u2192ct\u2192\u02dcht\nthen make a prediction as detailed in Eq. (5),\nEq. (6), and Figure 2. On the other hand, at\nany time t, Bahdanau et al. (2015) build from the\nprevious hidden state ht\u22121\u2192at\u2192ct\u2192\nht, which, in turn, goes through a deep-output\nand a maxout layer before making predictions.7\nLastly, Bahdanau et al. (2015) only experimented\nwith one alignment function, the concatproduct;\nwhereas we show later that the other alternatives\nare better.\n3.2 Local Attention\nThe global attention has a drawback that it has to\nattend to all words on the source side for each tar-\nget word, which is expensive and can potentially\nrender itimpractical totranslate longer sequences,\ne.g., paragraphs or documents. To address this\nde\ufb01ciency, we propose a localattentional mech-\nanism thatchooses tofocusonlyonasmall subset\nof the source positions per target word.\nThis model takes inspiration from the tradeoff\nbetween the softandhardattentional models pro-\nposed by Xuet al. (2015) to tackle the image cap-\ntion generation task. In their work, soft attention\n7We willrefer tothis difference againinSection3.3.refers to the global attention approach in which\nweights are placed \u201csoftly\u201d over all patches in the\nsource image. The hard attention, on the other\nhand,selects onepatchoftheimagetoattendtoat\natime. Whilelessexpensive atinference time,the\nhard attention model is non-differentiable and re-\nquires more complicated techniques such as vari-\nance reduction or reinforcement learning totrain.\nOur local attention mechanism selectively fo-\ncuses on a small window of context and is differ-\nentiable. Thisapproachhasanadvantageofavoid-\ningthe expensive computation incurred in thesoft\nattention and at the same time, is easier to train\nthan the hard attention approach. In concrete de-\ntails, the model \ufb01rst generates an aligned position\nptfor each target word at time t. Thecontext vec-\ntorctis then derived as a weighted average over\nthe set of source hidden states within the window\n[pt\u2212D,pt+D];Disempiricallyselected.8Unlike\nthe global approach, the local alignment vector at\nis now \ufb01xed-dimensional, i.e., \u2208R2D+1. Wecon-\nsider twovariants of the model asbelow.\nMonotonic alignment ( local-m)\u2013wesimplyset\npt=tassuming that source and target sequences\nareroughlymonotonically aligned. Thealignment\nvectoratisde\ufb01ned according toEq. (7).9\nPredictive alignment ( local-p) \u2013 instead of as-\nsumingmonotonic alignments, ourmodelpredicts\nanaligned position as follows:\npt=S\u00b7sigmoid(v\u22a4\nptanh(Wpht)),(9)\nWpandvpare the model parameters which will\nbelearnedtopredictpositions. Sisthesourcesen-\ntence length. As a result of sigmoid,pt\u2208[0,S].\nTo favor alignment points near pt, we place a\nGaussian distribution centered around pt.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "af992b63-bba3-4858-aadb-8e1194b8d90c": {"__data__": {"id_": "af992b63-bba3-4858-aadb-8e1194b8d90c", "embedding": null, "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28734f21-e5d5-418b-8a33-0318bc753a4e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "80e3654153b0630bd04ccea0976685269b790cfddaef17c6ff32c9b0be73d837"}, "2": {"node_id": "941c1ae7-ec72-49a0-aff4-0deaf59a32cf", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "62bbee8dc8d00b39e58b1d99d6bd893a52de74f0a616eaae9a918abab639b625"}}, "hash": "f37649ad35639581c33b124c0d586dd78f7ed8b3271f36f1c0cea145ce7688d2", "text": "To favor alignment points near pt, we place a\nGaussian distribution centered around pt. Specif-\nically, our alignment weights are now de\ufb01ned as:\nat(s) = align( ht,\u00afhs)exp/parenleftbigg\n\u2212(s\u2212pt)2\n2\u03c32/parenrightbigg\n(10)\nWe use the same alignfunction as in Eq. (7) and\nthe standard deviation is empirically set as \u03c3=D\n2.\nNote that ptis arealnummber; whereas sis an\nintegerwithin the window centered at pt.10\n8If the window crosses the sentence boundaries, we sim-\nplyignoretheoutsidepartandconsiderwordsinthewindow.\n9local-mis the same as the global model except that the\nvectoratis \ufb01xed-lengthand shorter.\n10local-pissimilartothelocal-mmodelexceptthatwedy-\nnamically compute ptand use a truncated Gaussian distribu-\ntion to modify the original alignment weights align(ht,\u00afhs)\nas shown in Eq. (10). By utilizing ptto derive at, we can\ncompute backprop gradients for Wpandvp. This model is\ndifferentiable almost everywhere.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4a9355c0-ee4d-40fa-92b0-4f0283c44351": {"__data__": {"id_": "4a9355c0-ee4d-40fa-92b0-4f0283c44351", "embedding": null, "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23c8cb0f-647d-4596-9c23-e6fc2562944c", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e9c6f0f0034a892e33ee7851f5f79a851a194c538c01d08bc6a51f528744ea1f"}, "3": {"node_id": "f527ad07-34d6-417e-ad5f-8429c0040d14", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "24ec50a219b6a2216a30373b6e26b41c0b6bed2efe380f306cbb30588194bbd1"}}, "hash": "178a35fca7e561cedb856bdac581355e6e754eb89a2e928259624c9adc43db39", "text": "\u02dcht\nAttention Layer\nB C D <eos> X Y ZXY Z <eos>\nA\nFigure 4: Input-feeding approach \u2013 Attentional\nvectors\u02dchtarefedasinputstothenexttimestepsto\ninform the model about past alignment decisions.\nComparisonto(Gregor et al., 2015) \u2013havepro-\nposed aselective attention mechanism, very simi-\nlar to our local attention, for the image generation\ntask. Their approach allows themodel toselect an\nimage patch of varying location and zoom. We,\ninstead, use the same \u201czoom\u201d for all target posi-\ntions,whichgreatlysimpli\ufb01estheformulationand\nstill achieves good performance.\n3.3 Input-feedingApproach\nIn our proposed global and local approaches,\nthe attentional decisions are made independently,\nwhich is suboptimal. Whereas, in standard MT,\nacoverage set is often maintained during the\ntranslation process to keep track of which source\nwords have been translated. Likewise, in atten-\ntional NMTs,alignment decisions shouldbemade\njointly taking into account past alignment infor-\nmation. To address that, we propose an input-\nfeedingapproach in which attentional vectors \u02dcht\nareconcatenated withinputs at thenext timesteps\nas illustrated in Figure 4.11The effects of hav-\ning such connections are two-fold: (a) we hope\nto make the model fully aware of previous align-\nment choices and (b) we create a very deep net-\nwork spanning both horizontally and vertically.\nComparison to other work \u2013\nBahdanau et al. (2015) use context vectors,\nsimilar to our ct, in building subsequent hidden\nstates, which can also achieve the \u201ccoverage\u201d\neffect. However, there has not been any analysis\nof whether such connections are useful as done\n11Ifnis the number of LSTM cells, the input size of the\n\ufb01rstLSTMlayer is 2n; those of subsequent layers are n.in this work. Also, our approach is more general;\nas illustrated in Figure 4, it can be applied to\ngeneral stacking recurrent architectures, including\nnon-attentional models.\nXuet al. (2015) propose a doubly attentional\napproach with an additional constraint added to\nthetraining objective tomakesurethemodel pays\nequal attention to all parts of the image during the\ncaption generation process. Such a constraint can\nalso be useful to capture the coverage set effect\nin NMT that we mentioned earlier. However, we\nchose to use the input-feeding approach since it\nprovides \ufb02exibility for the model todecide on any\nattentional constraints it deems suitable.\n4 Experiments\nWe evaluate the effectiveness of our models\non the WMT translation tasks between En-\nglish and German in both directions. new-\nstest2013 (3000 sentences) is used as a develop-\nment set to select our hyperparameters. Transla-\ntion performances are reported in case-sensitive\nBLEU (Papineni et al., 2002) on newstest2014\n(2737 sentences) and newstest2015 (2169 sen-\ntences). Following (Luong et al., 2015), wereport\ntranslation quality using two types of BLEU: (a)\ntokenized12BLEUto be comparable with existing\nNMT work and (b) NIST13BLEU to be compara-\nblewith WMTresults.\n4.1 Training Details\nAll our models are trained on the WMT\u201914 train-\ningdataconsisting of4.5Msentencespairs(116M\nEnglish words, 110M German words). Similar\nto (Jean et al., 2015), we limit our vocabularies to\nbe the top 50K most frequent words for both lan-\nguages. Words not in these shortlisted vocabular-\niesare converted into a universal token <unk>.\nWhen training our NMT systems, following\n(Bahdanau et al.,2015; Jean et al., 2015), we \ufb01l-\nter out sentence pairs whose lengths exceed\n50 words and shuf\ufb02e mini-batches as we pro-\nceed.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f527ad07-34d6-417e-ad5f-8429c0040d14": {"__data__": {"id_": "f527ad07-34d6-417e-ad5f-8429c0040d14", "embedding": null, "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23c8cb0f-647d-4596-9c23-e6fc2562944c", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e9c6f0f0034a892e33ee7851f5f79a851a194c538c01d08bc6a51f528744ea1f"}, "2": {"node_id": "4a9355c0-ee4d-40fa-92b0-4f0283c44351", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "178a35fca7e561cedb856bdac581355e6e754eb89a2e928259624c9adc43db39"}}, "hash": "24ec50a219b6a2216a30373b6e26b41c0b6bed2efe380f306cbb30588194bbd1", "text": "Our stacking LSTM models have 4 lay-\ners, each with 1000 cells, and 1000-dimensional\nembeddings. We follow (Sutskever et al.,2014;\nLuong et al., 2015) in training NMT with similar\nsettings: (a) our parameters are uniformly initial-\nized in[\u22120.1,0.1], (b) we train for 10 epochs us-\n12All texts are tokenized with tokenizer.perl and\nBLEUscores are computed with multi-bleu.perl .\n13Withthe mteval-v13a script as per WMTguideline.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f458e21-b384-4b55-8b1c-5cd833cda64d": {"__data__": {"id_": "5f458e21-b384-4b55-8b1c-5cd833cda64d", "embedding": null, "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f54020d-2f43-4631-8cdd-9941aefd8ca3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ef8641cda58e0a26c6fb879a154fb387a2a4e243a89b8717e03ee76f781eb3e2"}, "3": {"node_id": "5aaf3b75-993b-40f0-8b2f-8fbca305f169", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8a9f19675426ae45f0008033e9bb6dbd699a492698a25d05e8af9b9c1ee29f4f"}}, "hash": "ce62ea83ac9665bdaf32c8e9521cdf4607e8a9830bce22348d3981f2d0be17ce", "text": "System Ppl BLEU\nWinning WMT\u201914system \u2013 phrase-based +large LM (Bucket al., 2014) 20.7\nExisting NMTsystems\nRNNsearch (Jean et al., 2015) 16.5\nRNNsearch +unk replace (Jean etal., 2015) 19.0\nRNNsearch +unk replace +large vocab + ensemble 8models (Jean etal., 2015) 21.6\nOurNMTsystems\nBase 10.6 11.3\nBase +reverse 9.9 12.6 ( +1.3)\nBase +reverse +dropout 8.1 14.0 ( +1.4)\nBase +reverse +dropout +global attention ( location) 7.3 16.8 ( +2.8)\nBase +reverse +dropout +global attention ( location) +feed input 6.4 18.1 ( +1.3)\nBase +reverse +dropout +local-p attention ( general) +feed input5.919.0 (+0.9)\nBase +reverse +dropout +local-p attention ( general) +feed input +unk replace 20.9 ( +1.9)\nEnsemble 8models +unk replace 23.0 (+2.1)\nTable1:WMT'14 English-German results \u2013shown arethe perplexities (ppl) and the tokenized BLEU\nscores of various systems on newstest2014. We highlight the bestsystem in bold and give progressive\nimprovementsinitalicbetweenconsecutivesystems. local-preferestothelocalattentionwithpredictive\nalignments. Weindicate for each attention model the alignm ent score function used inpararentheses.\ning plain SGD, (c) a simple learning rate sched-\nule is employed \u2013 we start with a learning rate of\n1; after 5 epochs, we begin to halve the learning\nrate every epoch, (d) our mini-batch size is 128,\nand (e) the normalized gradient is rescaled when-\never its norm exceeds 5. Additionally, we also\nusedropoutwithprobability 0.2forourLSTMsas\nsuggested by (Zaremba et al., 2015). For dropout\nmodels, we train for 12 epochs and start halving\nthe learning rate after 8 epochs. For local atten-\ntion models, we empirically set the window size\nD= 10.\nOur code is implemented in MATLAB. When\nrunning on a single GPU device Tesla K40, we\nachieve a speed of 1K targetwords per second.\nIt takes 7\u201310 days tocompletely train amodel.\n4.2 English-German Results\nWe compare our NMT systems in the English-\nGerman task with various other systems. These\ninclude the winning system in WMT\u201914\n(Buck et al., 2014), a phrase-based system\nwhose language models were trained on a huge\nmonolingual text, the Common Crawl corpus.\nFor end-to-end NMT systems, to the best of\nour knowledge, (Jean et al., 2015) is the only\nwork experimenting with this language pair and\ncurrently the SOTA system. We only present\nresults for some of our attention models and will\nlater analyze therest in Section 5.\nAs shown in Table 1, we achieve pro-gressive improvements when (a) reversing the\nsource sentence, + 1.3BLEU, as proposed in\n(Sutskever et al., 2014) and (b) using dropout,\n+1.4BLEU. On top of that, (c) the global atten-\ntion approach gives a signi\ufb01cant boost of + 2.8\nBLEU, making our model slightly better than the\nbase attentional system of Bahdanau et al. (2015)\n(rowRNNSearch ). When (d) using the input-\nfeedingapproach, we seize another notable gain\nof +1.3BLEU and outperform their system. The\nlocal attention model with predictive alignments\n(rowlocal-p) proves to be even better, giving\nus a further improvement of + 0.9BLEU on top\nof the global attention model. It is interest-\ning to observe the trend previously reported in\n(Luong et al., 2015)thatperplexitystronglycorre-\nlates with translation quality.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5aaf3b75-993b-40f0-8b2f-8fbca305f169": {"__data__": {"id_": "5aaf3b75-993b-40f0-8b2f-8fbca305f169", "embedding": null, "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f54020d-2f43-4631-8cdd-9941aefd8ca3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ef8641cda58e0a26c6fb879a154fb387a2a4e243a89b8717e03ee76f781eb3e2"}, "2": {"node_id": "5f458e21-b384-4b55-8b1c-5cd833cda64d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ce62ea83ac9665bdaf32c8e9521cdf4607e8a9830bce22348d3981f2d0be17ce"}}, "hash": "8a9f19675426ae45f0008033e9bb6dbd699a492698a25d05e8af9b9c1ee29f4f", "text": "In total, we achieve\na signi\ufb01cant gain of 5.0 BLEU points over the\nnon-attentional baseline, which already includes\nknown techniques such as source reversing and\ndropout.\nThe unknown replacement technique proposed\nin(Luong et al., 2015;Jean et al.,2015)yieldsan-\nother nice gain of + 1.9BLEU,demonstrating that\nour attentional models do learn useful alignments\nfor unknown works. Finally, by ensembling 8\ndifferent models of various settings, e.g., using\ndifferent attention approaches, with and without\ndropout etc., wewere able to achieve a new SOTA\nresult of 23.0BLEU, outperforming the existing", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e274fee9-e0d4-4ae4-856e-bfc65684262f": {"__data__": {"id_": "e274fee9-e0d4-4ae4-856e-bfc65684262f", "embedding": null, "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3c4a92-db82-4877-9e2c-f2908ca6bd1e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e84275c39cb998ec040d688ee07c94fd32ca0e6609be3325bc931dff47a96aa5"}, "3": {"node_id": "793bbc40-715f-4ee9-b11a-f8c20ea5a916", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c298350695eaa7b824d0fc2459812252682ce3583cbbcdbc110f0316d2744e96"}}, "hash": "98b80c41cd89d82e1749e26ed82933f25a522cf114118ced66641ba0f227abf1", "text": "best system (Jean et al., 2015) by + 1.4BLEU.\nSystem BLEU\nTop\u2013NMT+5-gram rerank (Montreal) 24.9\nOur ensemble 8models + unk replace 25.9\nTable 2: WMT'15 English-German results \u2013\nNISTBLEU scores of the winning entry in\nWMT\u201915and our best one on newstest2015.\nLatest results in WMT\u201915 \u2013despite the fact that\nourmodelsweretrainedonWMT\u201914withslightly\nlessdata,wetestthemonnewstest2015 todemon-\nstratethattheycangeneralize welltodifferent test\nsets. As shown in Table 2, our best system es-\ntablishes a newSOTA performance of 25.9BLEU,\noutperforming the existing best system backed by\nNMTand a5-gram LMreranker by+ 1.0BLEU.\n4.3 German-English Results\nWe carry out a similar set of experiments for the\nWMT\u201915 translation task from German to En-\nglish. While our systems have not yet matched\nthe performance of the SOTA system, we never-\ntheless show the effectiveness of our approaches\nwithlargeandprogressivegainsintermsofBLEU\nas illustrated in Table 3. The attentional mech-\nanism gives us + 2.2BLEU gain and on top of\nthat, we obtain another boost of up to + 1.0BLEU\nfrom the input-feeding approach. Using a better\nalignment function, the content-based dotproduct\none, together with dropoutyields another gain of\n+2.7BLEU. Lastly, when applying the unknown\nword replacement technique, we seize an addi-\ntional +2.1BLEU, demonstrating the usefulness\nof attention in aligning rare words.\n5 Analysis\nWeconductextensiveanalysistobetterunderstand\nourmodelsintermsoflearning, theability tohan-\ndlelongsentences, choices ofattentional architec-\ntures, and alignment quality. All results reported\nhere are on English-German newstest2014.\n5.1 Learningcurves\nWecompare modelsbuilt ontopofone another as\nlisted in Table 1. It is pleasant to observe in Fig-\nure 5 a clear separation between non-attentional\nand attentional models. The input-feeding ap-\nproach and the local attention model also demon-\nstrate their abilities in driving the test costs lower.\nThe non-attentional model with dropout (the blueSystem Ppl. BLEU\nWMT\u201915systems\nSOTA\u2013phrase-based (Edinburgh) 29.2\nNMT+ 5-gram rerank (MILA) 27.6\nOurNMTsystems\nBase (reverse) 14.3 16.9\n+ global ( location) 12.7 19.1 ( +2.2)\n+ global ( location) + feed 10.9 20.1 ( +1.0)\n+ global ( dot) + drop + feed9.722.8 (+2.7)\n+ global ( dot) + drop + feed + unk 24.9 ( +2.1)\nTable 3: WMT'15 German-English results \u2013\nperformances of various systems (similar to Ta-\nble 1). The basesystem already includes source\nreversing on which we add globalattention,\ndropout, input feeding, andunkreplacement.\n0.20.40.60.811.21.41.61.8\nx 10523456\nMini\u2212batchesTest cost\n  \nbasic\nbasic+reverse\nbasic+reverse+dropout\nbasic+reverse+dropout+globalAttn\nbasic+reverse+dropout+globalAttn+feedInput\nbasic+reverse+dropout+pLocalAttn+feedInput\nFigure5: Learningcurves \u2013test cost ( lnperplex-\nity) on newstest2014 for English-German NMTs\nastraining progresses.\n+ curve) learns slower than other non-dropout\nmodels, but as time goes by, it becomes more ro-\nbust in termsof minimizing test errors.\n5.2 Effects of Translating LongSentences\nWe follow (Bahdanau et al., 2015) to group sen-\ntences of similar lengths together and compute\na BLEU score per group.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "793bbc40-715f-4ee9-b11a-f8c20ea5a916": {"__data__": {"id_": "793bbc40-715f-4ee9-b11a-f8c20ea5a916", "embedding": null, "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3c4a92-db82-4877-9e2c-f2908ca6bd1e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e84275c39cb998ec040d688ee07c94fd32ca0e6609be3325bc931dff47a96aa5"}, "2": {"node_id": "e274fee9-e0d4-4ae4-856e-bfc65684262f", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "98b80c41cd89d82e1749e26ed82933f25a522cf114118ced66641ba0f227abf1"}}, "hash": "c298350695eaa7b824d0fc2459812252682ce3583cbbcdbc110f0316d2744e96", "text": "Figure 6 shows that\nour attentional models are more effective than the\nnon-attentional one in handling long sentences:\nthe quality does not degrade as sentences become\nlonger. Our best model (the blue + curve) outper-\nformsall other systems in all length buckets.\n5.3 Choices of Attentional Architectures\nWe examine different attention models ( global,\nlocal-m, local-p ) and different alignment func-\ntions (location, dot, general, concat ) as described\nin Section 3. Due to limited resources, we can-\nnot run all the possible combinations. However,\nresults in Table 4 do give us some idea about dif-\nferent choices. The location-based function does", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "055e1273-c70c-4622-9e78-93718cac370d": {"__data__": {"id_": "055e1273-c70c-4622-9e78-93718cac370d", "embedding": null, "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73107650-0c1d-467a-8f9e-2a85dff78c1e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "184377a1960e821827c3aa01121705e63bf79d6c1b7ec43cd95693363a4533d7"}, "3": {"node_id": "b5b34ce3-0cb1-4672-97cf-11b544065441", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fa6a1f20409bfbefc649d1b5a65b37dc1a6408aecafec79c69b5597c45a65b9c"}}, "hash": "984ea0402834a87eb6a43837348a6f5e3710f11c0116cde81cf1f14345da9a38", "text": "10 20 30 40 50 60 7010152025\nSent LengthsBLEU\t\t\t\t\t\n  \nours, no attn (BLEU 13.9)\nours, local\u2212p attn (BLEU 20.9)\nours, best system (BLEU 23.0)\nWMT\u201914 best (BLEU 20.7)\nJeans et al., 2015 (BLEU 21.6)\nFigure 6: Length Analysis \u2013 translation qualities\nof different systems assentences become longer.\nSystem PplBLEU\nBefore After unk\nglobal (location) 6.4 18.1 19.3 (+1.2)\nglobal (dot) 6.1 18.6 20.5 (+1.9)\nglobal (general) 6.1 17.3 19.1 (+1.8)\nlocal-m (dot) >7.0 x x\nlocal-m (general) 6.2 18.6 20.4 (+1.8)\nlocal-p (dot) 6.6 18.0 19.6 (+1.9)\nlocal-p (general) 5.9 19 20.9(+1.9)\nTable 4: Attentional Architectures \u2013 perfor-\nmancesofdifferentattentionalmodels. Wetrained\ntwolocal-m (dot) models; both have ppl >7.0.\nnot learn good alignments: the global (location)\nmodel can only obtain a small gain when per-\nforming unknown word replacement compared to\nusing other alignment functions.14Forcontent-\nbasedfunctions, our implementation concatdoes\nnot yield good performances and more analysis\nshould be done to understand the reason.15It is\ninteresting to observe that dotworks well for the\nglobal attention and generalis better for the local\nattention. Among the different models, the local\nattention model withpredictive alignments ( local-\np)isbest, bothintermsofperplexities andBLEU.\n5.4 AlignmentQuality\nAby-productofattentionalmodelsarewordalign-\nments. While (Bahdanau et al., 2015) visualized\n14There is a subtle difference in how we retrieve align-\nments for the different alignment functions. At time step tin\nwhich we receive yt\u22121as input and then compute ht,at,ct,\nand\u02dchtbefore predicting yt, the alignment vector atis used\nas alignment weights for (a) the predicted word ytin the\nlocation-based alignment functions and (b) the input word\nyt\u22121inthecontent-based functions.\n15Withconcat, the perplexities achieved bydifferent mod-\nels are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). Such\nhighperplexities couldbedue tothefactthatwesimplifyth e\nmatrixWatoset the part that corresponds to \u00afhstoidentity.Method AER\nglobal (location) 0.39\nlocal-m (general) 0.34\nlocal-p (general) 0.36\nensemble 0.34\nBerkeley Aligner 0.32\nTable 6:AER scores \u2013 results of various models\nonthe RWTHEnglish-German alignment data.\nalignments for some sample sentences and ob-\nserved gains in translation quality as an indica-\ntionofaworkingattention model,noworkhasas-\nsessed the alignments learned as a whole. In con-\ntrast, we set out to evaluate the alignment quality\nusing the alignment error rate (AER)metric.\nGiven the gold alignment data provided by\nRWTH for 508 English-German Europarl sen-\ntences, we \u201cforce\u201d decode our attentional models\nto produce translations that match the references.\nWe extract only one-to-one alignments by select-\ning the source word with the highest alignment\nweight per target word. Nevertheless, as shown in\nTable6,wewereabletoachieveAERscorescom-\nparable to the one-to-many alignments obtained\nbythe Berkeley aligner (Liang et al., 2006).16\nWe also found that the alignments produced by\nlocal attention models achieve lower AERs than\nthose of the global one.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b5b34ce3-0cb1-4672-97cf-11b544065441": {"__data__": {"id_": "b5b34ce3-0cb1-4672-97cf-11b544065441", "embedding": null, "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73107650-0c1d-467a-8f9e-2a85dff78c1e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "184377a1960e821827c3aa01121705e63bf79d6c1b7ec43cd95693363a4533d7"}, "2": {"node_id": "055e1273-c70c-4622-9e78-93718cac370d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "984ea0402834a87eb6a43837348a6f5e3710f11c0116cde81cf1f14345da9a38"}}, "hash": "fa6a1f20409bfbefc649d1b5a65b37dc1a6408aecafec79c69b5597c45a65b9c", "text": "The AERobtained by the\nensemble, while good, is not better than the local-\nm AER, suggesting the well-known observation\nthat AER and translation scores are not well cor-\nrelated (Fraser and Marcu, 2007). We show some\nalignment visualizations inAppendix A.\n5.5 SampleTranslations\nWe show in Table 5 sample translations in both\ndirections. It it appealing to observe the ef-\nfect of attentional models in correctly translating\nnames such as \u201cMiranda Kerr\u201dand \u201cRoger Dow\u201d.\nNon-attentional models, while producing sensi-\nble names from a language model perspective,\nlack the direct connections from the source side\nto make correct translations. We also observed\nan interesting case in the second example, which\nrequires translating the doubly-negated phrase,\n\u201cnot incompatible\u201d. The attentional model cor-\nrectly produces \u201cnicht ...unvereinbar\u201d; whereas\nthenon-attentional model generates \u201cnicht verein-\n16Weconcatenatethe508sentencepairswith1Msentence\npairs fromWMT andrun the Berkeleyaligner.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35298a8b-7f91-4f4a-95bf-92983732010c": {"__data__": {"id_": "35298a8b-7f91-4f4a-95bf-92983732010c", "embedding": null, "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba6488a6-f975-4588-a75f-d0b082dcf28b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "259abbbca64538fcdcb788eb4c9ed9fe4f52dd112935139b640f423f3d758797"}, "3": {"node_id": "1040b50c-52a9-4920-beb4-dcd70eb2e18b", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e4906ba22e7fe24cc751578f067b46bf79410658d0c221e3a8ad51b724a45d88"}}, "hash": "b8da2661dd80d56abf9a32ee8aae81d1b40885b9baf85dc50ec809a8b3803a8c", "text": "English-German translations\nsrc Orlando Bloom and Miranda Kerr still loveeach other\nref Orlando Bloom und Miranda Kerr lieben sich noch immer\nbestOrlando Bloom und Miranda Kerr lieben einander noch immer.\nbase Orlando Bloom und LucasMiranda lieben einander noch immer.\nsrc\u2032\u2032We\u2032re pleased the FAA recognizes that an enjoyable passenger ex perience is not incompatible\nwithsafety and security ,\u2032\u2032said Roger Dow, CEOof the U.S.Travel Association .\nref \u201cWir freuen uns , dass die FAAerkennt , dass ein angenehme s Passagiererlebnis nicht im Wider-\nspruch zur Sicherheit steht \u201d,sagte Roger Dow , CEOder U.S.Travel Association .\nbest\u2032\u2032Wir freuen uns , dass die FAA anerkennt , dass ein angenehmes i st nicht mit Sicherheit und\nSicherheit unvereinbar ist\u2032\u2032, sagteRoger Dow ,CEOder US- die .\nbase\u2032\u2032Wirfreuenuns \u00a8 uberdie <unk>,dassein <unk><unk>mitSicherheit nicht vereinbar istmit\nSicherheit und Sicherheit\u2032\u2032,sagteRogerCameron ,CEOder US- <unk>.\nGerman-English translations\nsrc Ineinem Interview sagte Bloom jedoch ,dass er und Kerr si ch noch immerlieben .\nref However ,in aninterview , Bloom has said that he and Kerrstill love each other .\nbestInan interview ,however ,Bloom said that heand Kerrstill love.\nbase However ,in aninterview , Bloom said that he and Tinawerestill <unk>.\nsrc Wegen der von Berlin und der Europ\u00a8 aischen Zentralbank v erh\u00a8 angten strengen Sparpolitik in\nVerbindung mit der Zwangsjacke , in die die jeweilige nation ale Wirtschaft durch das Festhal-\nten an der gemeinsamen W\u00a8 ahrung gen\u00a8 otigt wird , sind viele M enschen der Ansicht , das Projekt\nEuropa sei zuweit gegangen\nref The austerity imposed by Berlin and the European Central Bank , c oupled with the straitjacket\nimposedonnational economies through adherence tothecomm oncurrency ,hasledmanypeople\ntothink Project Europe has gone too far .\nbestBecause of the strict austerity measures imposed by Berlin and the European Centr al Bank in\nconnection with the straitjacket in which the respective national economy is forced to adhere to\nthe common currency ,many people believe that theEuropean p roject has gone too far .\nbase Becauseofthepressure imposedbytheEuropeanCentralBankandtheFederalCentral Bank\nwith the strict austerity imposed on the national economy in the face of the single curr ency ,\nmanypeople believe that the European project has gone too fa r .\nTable5:Sampletranslations \u2013foreach example, weshow thesource ( src), thehumantranslation ( ref),\nthe translation from our best model ( best), and the translation of a non-attentional model ( base). We\nitalicize some correcttranslation segments and highlight a few wrongones in bold.\nbar\u201d,meaning\u201cnotcompatible\u201d.17Theattentional\nmodelalsodemonstratesitssuperiority intranslat-\ning long sentences as inthe last example.\n6 Conclusion\nInthispaper, wepropose twosimple andeffective\nattentional mechanisms for neural machine trans-\nlation: the globalapproach which always looks\nat all source positions and the localone that only\nattends to a subset of source positions at a time.\nWe test the effectiveness of our models in the\nWMT translation tasks between English and Ger-\nman in both directions. Our local attention yields\nlargegainsofupto 5.0BLEUovernon-attentional\n17The reference uses a more fancy translation of \u201cincom-\npatible\u201d, which is \u201cim Widerspruch zu etwas stehen\u201d. Both\nmodels, however, failedtotranslate \u201cpassenger experienc e\u201d.models which already incorporate known tech-\nniques such as dropout.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1040b50c-52a9-4920-beb4-dcd70eb2e18b": {"__data__": {"id_": "1040b50c-52a9-4920-beb4-dcd70eb2e18b", "embedding": null, "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba6488a6-f975-4588-a75f-d0b082dcf28b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "259abbbca64538fcdcb788eb4c9ed9fe4f52dd112935139b640f423f3d758797"}, "2": {"node_id": "35298a8b-7f91-4f4a-95bf-92983732010c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b8da2661dd80d56abf9a32ee8aae81d1b40885b9baf85dc50ec809a8b3803a8c"}}, "hash": "e4906ba22e7fe24cc751578f067b46bf79410658d0c221e3a8ad51b724a45d88", "text": "For the English to Ger-\nmantranslation direction, ourensemblemodelhas\nestablished new state-of-the-art results for both\nWMT\u201914 and WMT\u201915, outperforming existing\nbestsystems, backedbyNMTmodelsand n-gram\nLMrerankers, by more than 1.0BLEU.\nWehave compared various alignment functions\nand shed light on which functions are best for\nwhichattentional models. Ouranalysisshowsthat\nattention-based NMT models are superior to non-\nattentional ones in many cases, for example in\ntranslating names and handling long sentences.\nAcknowledgment\nWe gratefully acknowledge support from a gift\nfrom Bloomberg L.P. and the support of NVIDIA", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3764afa8-ff16-4f59-8a98-d5e8e030056a": {"__data__": {"id_": "3764afa8-ff16-4f59-8a98-d5e8e030056a", "embedding": null, "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57681db3-c712-4ebe-941e-901878af9b94", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "32fd042bc412263953d7480c1496447db554c819b04b9202e846d8272e4b2b19"}, "3": {"node_id": "4e5b2854-e70a-488d-a29c-2a9d112b08c9", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "2f00078fc5896c33dc42e37b52ecb27666d1b43627acfb21267839f84fc3af01"}}, "hash": "91bf8882f7bacfc10c3204b84ef00a24a675712726c0dd9a62624a128d12c97f", "text": "CorporationwiththedonationofTeslaK40GPUs.\nWe thank Andrew Ng and his group as well as\nthe Stanford Research Computing for letting us\nuse their computing resources. We thank Rus-\nsell Stewart forhelpful discussions onthemodels.\nLastly, we thank Quoc Le, Ilya Sutskever, Oriol\nVinyals, Richard Socher, Michael Kayser, Jiwei\nLi, Panupong Pasupat, Kelvin Guu, members of\nthe Stanford NLP Group and the annonymous re-\nviewersfortheirvaluablecommentsandfeedback.\nReferences\n[Bahdanauet al.2015] D. Bahdanau, K. Cho, and\nY. Bengio. 2015. Neural machine translation by\njointlylearningto alignandtranslate. In ICLR.\n[Bucket al.2014] Christian Buck, Kenneth Hea\ufb01eld,\nand Bas van Ooyen. 2014. N-gram counts and lan-\nguagemodelsfromthecommoncrawl. In LREC.\n[Choet al.2014] Kyunghyun Cho, Bart van Merrien-\nboer, Caglar Gulcehre, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nforstatistical machinetranslation. In EMNLP.\n[FraserandMarcu2007] Alexander Fraser and Daniel\nMarcu. 2007. Measuring word alignment quality\nfor statistical machine translation. Computational\nLinguistics ,33(3):293\u2013303.\n[Gregoretal.2015] Karol Gregor, Ivo Danihelka, Alex\nGraves, Danilo Jimenez Rezende, and Daan Wier-\nstra. 2015. DRAW: A recurrent neural network for\nimagegeneration. In ICML.\n[Jeanet al.2015] S\u00b4 ebastien Jean, Kyunghyun Cho,\nRoland Memisevic, and Yoshua Bengio. 2015. On\nusing very large target vocabulary for neural ma-\nchinetranslation. In ACL.\n[KalchbrennerandBlunsom2013] N.Kalchbrennerand\nP.Blunsom. 2013. Recurrentcontinuoustranslation\nmodels. In EMNLP.\n[Koehnet al.2003] Philipp Koehn, Franz Josef Och,\nand Daniel Marcu. 2003. Statistical phrase-based\ntranslation. In NAACL.\n[Liangetal.2006] P. Liang, B. Taskar, and D. Klein.\n2006. Alignmentbyagreement. In NAACL.\n[Luongetal.2015] M.-T.Luong,I.Sutskever,Q.V. Le,\nO. Vinyals, and W. Zaremba. 2015. Addressingthe\nrarewordproblemin neuralmachinetranslation. In\nACL.\n[Mnihetal.2014] Volodymyr Mnih, Nicolas Heess,\nAlex Graves, and Koray Kavukcuoglu. 2014. Re-\ncurrentmodelsofvisualattention. In NIPS.[Papineniet al.2002] Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine trans-\nlation. In ACL.\n[Sutskeveret al.2014] I. Sutskever, O. Vinyals, and\nQ.V.Le. 2014. Sequencetosequencelearningwith\nneuralnetworks. In NIPS.\n[Xuet al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,\nKyunghyun Cho, Aaron C. Courville, Ruslan\nSalakhutdinov,Richard S. Zemel, and Yoshua Ben-\ngio. 2015. Show,attendandtell: Neuralimagecap-\ntiongenerationwithvisualattention. In ICML.\n[Zarembaetal.2015] Wojciech Zaremba, Ilya\nSutskever, and Oriol Vinyals. 2015.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e5b2854-e70a-488d-a29c-2a9d112b08c9": {"__data__": {"id_": "4e5b2854-e70a-488d-a29c-2a9d112b08c9", "embedding": null, "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57681db3-c712-4ebe-941e-901878af9b94", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "32fd042bc412263953d7480c1496447db554c819b04b9202e846d8272e4b2b19"}, "2": {"node_id": "3764afa8-ff16-4f59-8a98-d5e8e030056a", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "91bf8882f7bacfc10c3204b84ef00a24a675712726c0dd9a62624a128d12c97f"}}, "hash": "2f00078fc5896c33dc42e37b52ecb27666d1b43627acfb21267839f84fc3af01", "text": "2015. Recurrent\nneuralnetworkregularization. In ICLR.\nA AlignmentVisualization\nWe visualize the alignment weights produced by\nour different attention models in Figure 7. The vi-\nsualization of the local attention model is much\nsharper than that of the global one. This contrast\nmatches our expectation that local attention is de-\nsigned to only focus on a subset of words each\ntime. Also,sincewetranslatefromEnglishtoGer-\nman and reverse the source English sentence, the\nwhite strides at the words \u201creality\u201d and\u201c.\u201din the\nglobal attention model reveals an interesting ac-\ncesspattern: ittendstoreferbacktothebeginning\nof the source sequence.\nCompared to the alignment visualizations in\n(Bahdanau et al.,2015), our alignment patterns\nare not as sharp as theirs. Such difference could\npossibly be due to the fact that translating from\nEnglish to German is harder than translating into\nFrench as done in (Bahdanau et al., 2015), which\nisan interesting point toexamine infuture work.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5c7b844d-4d02-44ef-9a17-8d40bc43a24d": {"__data__": {"id_": "5c7b844d-4d02-44ef-9a17-8d40bc43a24d", "embedding": null, "metadata": {"page_label": "11", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f4de1f6-d75d-481b-8233-4538aa6dd01a", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "17444a0ea4d604a2f4078500adf80ddfb802b0bea25594a61f99667d80e6036c"}}, "hash": "17444a0ea4d604a2f4078500adf80ddfb802b0bea25594a61f99667d80e6036c", "text": "TheydonotunderstandwhyEuropeexistsintheorybutnotinreality.\nSie\nverstehen\nnicht\n,\nwarum\nEuropa\ntheoretisch\nzwar\nexistiert\n,\naber\nnicht\nin\nWirklichkeit\n.\nTheydonotunderstandwhyEuropeexistsintheorybutnotinreality.\nSie\nverstehen\nnicht\n,\nwarum\nEuropa\ntheoretisch\nzwar\nexistiert\n,\naber\nnicht\nin\nWirklichkeit\n.\nTheydonotunderstandwhyEuropeexistsintheorybutnotinreality.\nSie\nverstehen\nnicht\n,\nwarum\nEuropa\ntheoretisch\nzwar\nexistiert\n,\naber\nnicht\nin\nWirklichkeit\n.\nTheydonotunderstandwhyEuropeexistsintheorybutnotinreality.\nSie\nverstehen\nnicht\n,\nwarum\nEuropa\ntheoretisch\nzwar\nexistiert\n,\naber\nnicht\nin\nWirklichkeit\n.\nFigure 7: Alignment visualizations \u2013 shown are images of the attention weights learned by variou s\nmodels: (topleft)global,(topright)local-m,and(bottom left)local-p. The goldalignmentsaredisplayed\nat the bottom right corner.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aae345c1-f574-4e6d-906e-554557a3bea3": {"__data__": {"id_": "aae345c1-f574-4e6d-906e-554557a3bea3", "embedding": null, "metadata": {"page_label": "1", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19139b82-d1b5-43d4-a7bd-e35297d8e82e", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0fa342dc9cbe831d68ec86228e2794be910ca74ec24bbfe460e39457b9ac6102"}}, "hash": "0fa342dc9cbe831d68ec86228e2794be910ca74ec24bbfe460e39457b9ac6102", "text": "arXiv:1508.07909v5  [cs.CL]  10 Jun 2016Neural Machine TranslationofRare Words withSubword Units\nRico Sennrich andBarry Haddow andAlexandra Birch\nSchoolofInformatics,UniversityofEdinburgh\n{rico.sennrich,a.birch}@ed.ac.uk ,bhaddow@inf.ed.ac.uk\nAbstract\nNeural machine translation (NMT) mod-\nels typically operate with a \ufb01xed vocabu-\nlary, but translation is an open-vocabulary\nproblem. Previous work addresses the\ntranslation of out-of-vocabulary words by\nbacking off to a dictionary. In this pa-\nper, we introduce a simpler and more ef-\nfective approach, making the NMT model\ncapable of open-vocabulary translation by\nencoding rare and unknown words as se-\nquencesofsubwordunits. Thisisbasedon\nthe intuition that various word classes are\ntranslatable via smaller units than words,\nfor instance names (via character copying\nor transliteration), compounds (via com-\npositional translation), and cognates and\nloanwords (via phonological and morpho-\nlogical transformations). We discuss the\nsuitability of different word segmentation\ntechniques, including simple character n-\ngram models andasegmentation based on\nthebyte pair encoding compression algo-\nrithm, and empirically show that subword\nmodelsimproveoveraback-off dictionary\nbaseline for the WMT15 translation tasks\nEnglish\u2192German and English \u2192Russian\nbyup to1.1 and 1.3 B LEU, respectively.\n1 Introduction\nNeural machine translation has recently shown\nimpressive results (Kalchbrenner and Blunsom,\n2013; Sutskever et al., 2014; Bahdanau et al.,\n2015). However, the translation of rare words\nis an open problem. The vocabulary of neu-\nral models is typically limited to 30000\u201350000\nwords,but translation isanopen-vocabulary prob-\nTheresearchpresentedinthispublicationwasconducted\nin cooperation with Samsung Electronics Polska sp. z o.o. -\nSamsung R&DInstitute Poland.lem, and especially for languages with produc-\ntive word formation processes such as aggluti-\nnation and compounding, translation models re-\nquire mechanisms that go below the word level.\nAs an example, consider compounds such as the\nGermanAbwasser |behandlungs |anlange\u2018sewage\nwater treatment plant\u2019, for which a segmented,\nvariable-length representation is intuitively more\nappealingthanencodingthewordasa\ufb01xed-length\nvector.\nFor word-level NMT models, the translation\nof out-of-vocabulary words has been addressed\nthrough aback-off toadictionary look-up (Jeanet\nal., 2015; Luong et al., 2015b). We note that such\ntechniques make assumptions that often do not\nhold true in practice. For instance, there is not al-\nwaysa1-to-1correspondence between sourceand\ntarget words because of variance in the degree of\nmorphological synthesis between languages, like\nin our introductory compounding example. Also,\nword-level models are unable to translate or gen-\nerateunseen words. Copying unknownwordsinto\nthetargettext,asdoneby(Jeanetal.,2015;Luong\net al., 2015b), is a reasonable strategy for names,\nbut morphological changes and transliteration is\noften required, especially if alphabets differ.\nWeinvestigate NMTmodelsthat operate onthe\nlevel of subword units. Our main goal is to model\nopen-vocabulary translation in the NMT network\nitself, without requiring a back-off model for rare\nwords. In addition to making the translation pro-\ncesssimpler,wealso\ufb01ndthatthesubwordmodels\nachieve better accuracy for the translation of rare\nwords than large-vocabulary models and back-off\ndictionaries, and are able to productively generate\nnewwordsthatwerenotseenattrainingtime. Our\nanalysisshowsthattheneural networksareableto\nlearn compounding and transliteration from sub-\nwordrepresentations.\nThispaper hastwo maincontributions:\n\u2022 We show that open-vocabulary neural ma-", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4bd16d19-fff7-4bfa-ab29-de5e2d6eea51": {"__data__": {"id_": "4bd16d19-fff7-4bfa-ab29-de5e2d6eea51", "embedding": null, "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "257e9425-9f18-4f4f-89bd-9aace4633dbe", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b5cfccf9a9ad3f6da35d1cb4181c167203653c289d41cf76c81fc289c31df67a"}, "3": {"node_id": "61591b94-fb42-4ab4-8bf6-4fc07098a3e3", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "5fca9b6e95629371a74399a40a049f813bdf5120ac6f6f837913b2bb4400d92d"}}, "hash": "5a1f045cb169cb9d064445a0facf94c73c7a5b6267b7e857db8d03edf9407403", "text": "chine translation is possible by encoding\n(rare) words via subword units. We \ufb01nd our\narchitecture simpler and more effective than\nusing large vocabularies and back-off dictio-\nnaries(Jeanetal.,2015;Luongetal.,2015b).\n\u2022 We adapt byte pair encoding (BPE) (Gage,\n1994), a compression algorithm, to the task\nof word segmentation. BPE allows for the\nrepresentation ofanopenvocabulary through\na \ufb01xed-size vocabulary of variable-length\ncharacter sequences, making it a very suit-\nable word segmentation strategy for neural\nnetwork models.\n2 Neural MachineTranslation\nWe follow the neural machine translation archi-\ntecture by Bahdanau et al. (2015), which we will\nbrie\ufb02ysummarizehere. However,wenotethatour\napproach isnot speci\ufb01c tothis architecture.\nTheneuralmachinetranslationsystemisimple-\nmentedasanencoder-decoder networkwithrecur-\nrent neural networks.\nThe encoder is a bidirectional neural network\nwith gated recurrent units (Cho et al., 2014)\nthat reads an input sequence x= (x1,...,xm)\nand calculates a forward sequence of hidden\nstates(\u2212 \u2192h1,...,\u2212 \u2192hm), and a backward sequence\n(\u2190 \u2212h1,...,\u2190 \u2212hm). The hidden states\u2212 \u2192hjand\u2190 \u2212hjare\nconcatenated to obtain the annotation vector hj.\nThe decoder is a recurrent neural network that\npredicts a target sequence y= (y1,...,yn). Each\nwordyiis predicted based on a recurrent hidden\nstatesi, the previously predicted word yi\u22121, and\na context vector ci.ciis computed as a weighted\nsum of the annotations hj. The weight of each\nannotation hjis computed through an alignment\nmodel\u03b1ij, which models the probability that yiis\naligned to xj. The alignment model is a single-\nlayer feedforward neural network that is learned\njointly with the rest of the network through back-\npropagation.\nA detailed description can be found in (Bah-\ndanau et al., 2015). Training is performed on a\nparallel corpus with stochastic gradient descent.\nFor translation, a beam search with small beam\nsize isemployed.\n3 Subword Translation\nThe main motivation behind this paper is that\nthe translation of some words is transparent inthat they are translatable by a competent transla-\ntor even if they are novel to him or her, based\non a translation of known subword units such as\nmorphemes or phonemes. Word categories whose\ntranslation is potentially transparent include:\n\u2022 namedentities. Betweenlanguagesthatshare\nan alphabet, names can often be copied from\nsourcetotargettext. Transcriptionortranslit-\neration may be required, especially if the al-\nphabets or syllabaries differ. Example:\nBarack Obama(English; German)\n\u0411\u0430\u0440\u0430\u043a \u041e\u0431\u0430\u043c\u0430 (Russian)\n\u30d0\u30e9\u30af\u30fb\u30aa\u30d0\u30de(ba-ra-kuo-ba-ma)(Japanese)\n\u2022 cognates and loanwords. Cognates and loan-\nwords with a common origin can differ in\nregular ways between languages, so that\ncharacter-level translation rules are suf\ufb01cient\n(Tiedemann, 2012). Example:\nclaustrophobia (English)\nKlaustrophobie (German)\n\u041a\u043b\u0430\u0443\u0441\u0442\u0440\u043e\u0444\u043e\u0431\u0438\u044f (Klaustrofobi\u00e2) (Russian)\n\u2022 morphologically complexwords. Wordscon-\ntaining multiple morphemes, for instance\nformed via compounding, af\ufb01xation, or in-\n\ufb02ection, may be translatable by translating\nthe morphemes separately. Example:\nsolar system (English)\nSonnensystem (Sonne + System) (German)\nNaprendszer (Nap+ Rendszer) (Hungarian)\nIn an analysis of 100 rare tokens (not among\nthe 50000 most frequent types) in our German\ntraining data1, the majority of tokens are poten-\ntially translatable from English through smaller\nunits.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61591b94-fb42-4ab4-8bf6-4fc07098a3e3": {"__data__": {"id_": "61591b94-fb42-4ab4-8bf6-4fc07098a3e3", "embedding": null, "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "257e9425-9f18-4f4f-89bd-9aace4633dbe", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b5cfccf9a9ad3f6da35d1cb4181c167203653c289d41cf76c81fc289c31df67a"}, "2": {"node_id": "4bd16d19-fff7-4bfa-ab29-de5e2d6eea51", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "5a1f045cb169cb9d064445a0facf94c73c7a5b6267b7e857db8d03edf9407403"}}, "hash": "5fca9b6e95629371a74399a40a049f813bdf5120ac6f6f837913b2bb4400d92d", "text": "We \ufb01nd 56 compounds, 21 names,\n6 loanwords with a common origin ( emanci-\npate\u2192emanzipieren ), 5 cases of transparent af\ufb01x-\nation (sweetish\u2018sweet\u2019 + \u2018-ish\u2019\u2192s\u00fc\u00dflich\u2018s\u00fc\u00df\u2019 +\n\u2018-lich\u2019), 1 number and 1 computer language iden-\nti\ufb01er.\nOur hypothesis is that a segmentation of rare\nwords into appropriate subword units is suf\ufb01-\ncient to allow for the neural translation network\nto learn transparent translations, and to general-\nizethisknowledgetotranslateandproduceunseen\nwords.2Weprovide empirical support for this hy-\n1Primarilyparliamentaryproceedingsandwebcrawldata.\n2Not every segmentation we produce is transparent.\nWhile we expect no performance bene\ufb01t from opaque seg-\nmentations, i.e. segmentations where the units cannot be\ntranslated independently, our NMT models show robustness\ntowards oversplitting.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7f6322eb-a831-4b34-a5d9-e7eec7b3f9c7": {"__data__": {"id_": "7f6322eb-a831-4b34-a5d9-e7eec7b3f9c7", "embedding": null, "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edff8844-dec3-413c-acb5-d95c49d37b7e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "40a96fc1b3d6ce8f12d43a6985c21b6e0feaa99b04bf83da082c8a7f32dbb095"}, "3": {"node_id": "64b0a2e2-8f35-42eb-8a38-ecd4aa48ef1d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "714129f3d59b3951305cb2273eaceefa8644872e699dab5d99d1eb3d5d82153e"}}, "hash": "49953cc6e4345f33f27e30220d6378414cc1708f589aceb0ed7cf191bfb8ebc3", "text": "pothesis in Sections 4and 5. First, wediscuss dif-\nferent subword representations.\n3.1 Related Work\nFor Statistical Machine Translation (SMT), the\ntranslation ofunknownwordshasbeenthesubject\nof intensive research.\nA large proportion of unknown words are\nnames, which can just be copied into the tar-\nget text if both languages share an alphabet. If\nalphabets differ, transliteration is required (Dur-\nrani et al., 2014). Character-based translation has\nalso been investigated with phrase-based models,\nwhich proved especially successful for closely re-\nlated languages (Vilar et al., 2007; Tiedemann,\n2009; Neubig et al.,2012).\nThe segmentation of morphologically complex\nwordssuchascompoundsiswidelyusedforSMT,\nand various algorithms for morpheme segmen-\ntation have been investigated (Nie\u00dfen and Ney,\n2000; Koehn and Knight, 2003; Virpioja et al.,\n2007; Stallard et al., 2012). Segmentation al-\ngorithms commonly used for phrase-based SMT\ntend tobeconservative intheir splitting decisions,\nwhereas we aim for an aggressive segmentation\nthat allows for open-vocabulary translation with a\ncompact network vocabulary, and without having\ntoresort to back-off dictionaries.\nThe best choice of subword units may be task-\nspeci\ufb01c. For speech recognition, phone-level lan-\nguage models have been used (Bazzi and Glass,\n2000). Mikolov et al. (2012) investigate subword\nlanguage models, and propose to use syllables.\nFor multilingual segmentation tasks, multilingual\nalgorithmshavebeenproposed(SnyderandBarzi-\nlay,2008). We\ufb01ndtheseintriguing, butinapplica-\nble at test time.\nVarious techniques have been proposed to pro-\nduce \ufb01xed-length continuous word vectors based\non characters or morphemes (Luong et al., 2013;\nBothaandBlunsom,2014;Lingetal.,2015a;Kim\net al., 2015). An effort to apply such techniques\nto NMT, parallel to ours, has found no signi\ufb01cant\nimprovement over word-based approaches (Ling\net al., 2015b). One technical difference from our\nwork is that the attention mechanism still oper-\nates on the level of words in the model by Ling\net al. (2015b), and that the representation of each\nword is \ufb01xed-length. We expect that the attention\nmechanism bene\ufb01ts from our variable-length rep-\nresentation: the network can learn to place atten-tion on different subword units at each step. Re-\ncall our introductory example Abwasserbehand-\nlungsanlange , for which a subword segmentation\navoidstheinformationbottleneckofa\ufb01xed-length\nrepresentation.\nNeural machine translation differs from phrase-\nbasedmethodsinthattherearestrongincentivesto\nminimize the vocabulary size of neural models to\nincreasetimeandspaceef\ufb01ciency,andtoallowfor\ntranslation without back-off models. At the same\ntime,wealsowantacompactrepresentation ofthe\ntext itself, since an increase in text length reduces\nef\ufb01ciency and increases the distances over which\nneural models need to pass information.\nAsimplemethodtomanipulatethetrade-offbe-\ntweenvocabulary sizeandtextsizeistouseshort-\nlists of unsegmented words, using subword units\nonly for rare words. As an alternative, we pro-\npose a segmentation algorithm based on byte pair\nencoding (BPE), which lets us learn a vocabulary\nthat provides agood compression rate of thetext.\n3.2 Byte PairEncoding(BPE)\nByte Pair Encoding (BPE) (Gage, 1994) is a sim-\nple data compression technique that iteratively re-\nplaces the most frequent pair of bytes in a se-\nquence with a single, unused byte. We adapt this\nalgorithmforwordsegmentation. Insteadofmerg-\ningfrequent pairsofbytes,wemergecharactersor\ncharacter sequences.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64b0a2e2-8f35-42eb-8a38-ecd4aa48ef1d": {"__data__": {"id_": "64b0a2e2-8f35-42eb-8a38-ecd4aa48ef1d", "embedding": null, "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edff8844-dec3-413c-acb5-d95c49d37b7e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "40a96fc1b3d6ce8f12d43a6985c21b6e0feaa99b04bf83da082c8a7f32dbb095"}, "2": {"node_id": "7f6322eb-a831-4b34-a5d9-e7eec7b3f9c7", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "49953cc6e4345f33f27e30220d6378414cc1708f589aceb0ed7cf191bfb8ebc3"}}, "hash": "714129f3d59b3951305cb2273eaceefa8644872e699dab5d99d1eb3d5d82153e", "text": "Firstly,weinitializethesymbolvocabularywith\nthe character vocabulary, and represent each word\nas a sequence of characters, plus a special end-of-\nword symbol \u2018\u00b7\u2019, which allows us to restore the\noriginal tokenization after translation. We itera-\ntively count all symbol pairs and replace each oc-\ncurrence of the most frequent pair (\u2018A\u2019, \u2018B\u2019) with\na new symbol \u2018AB\u2019. Each merge operation pro-\nduces a new symbol which represents a charac-\ntern-gram. Frequent character n-grams(or whole\nwords) are eventually merged into a single sym-\nbol,thusBPErequires noshortlist. The\ufb01nalsym-\nbolvocabularysizeisequaltothesizeoftheinitial\nvocabulary, plus the number of merge operations\n\u2013thelatter istheonly hyperparameter of thealgo-\nrithm.\nFor ef\ufb01ciency, we do not consider pairs that\ncross word boundaries. The algorithm can thus be\nrun on the dictionary extracted from a text, with\neach word being weighted by its frequency. A\nminimal Python implementation is shown in Al-", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d41f9130-62e5-4140-b7eb-3690aaa6f732": {"__data__": {"id_": "d41f9130-62e5-4140-b7eb-3690aaa6f732", "embedding": null, "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd9fdfe0-2305-4b69-97ec-f5eda22bc01c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e07d213da7a30bd4dc602df5840652901a8eae0607150638ee2eb5bc48665dfb"}, "3": {"node_id": "ea998542-7a20-41be-a64b-ca912e79c9d5", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8e39b5bbaa7b9fcb5ce841249f80bb333243911b35b4eb73ddca9123d2cf6f52"}}, "hash": "09e55f9cdcbb05630c786d5b0884cc267c6da9ef158dbe637759c5f38f8d20ba", "text": "Algorithm 1 Learn BPEoperations\nimport re, collections\ndefget_stats(vocab):\npairs = collections.defaultdict(int)\nforword, freqinvocab.items():\nsymbols = word.split()\nforiinrange(len(symbols)-1):\npairs[symbols[i],symbols[i+1]] += freq\nreturn pairs\ndefmerge_vocab(pair, v_in):\nv_out = {}\nbigram = re.escape(' '.join(pair))\np = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\nforwordinv_in:\nw_out = p.sub(''.join(pair), word)\nv_out[w_out] = v_in[word]\nreturn v_out\nvocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n'n e w e s t </w>':6, 'w i d e s t </w>':3}\nnum_merges = 10\nforiinrange(num_merges):\npairs = get_stats(vocab)\nbest = max(pairs, key=pairs.get)\nvocab = merge_vocab(best, vocab)\nprint(best)\nr\u00b7 \u2192 r\u00b7\nlo\u2192lo\nlow\u2192low\ne r\u00b7 \u2192 er\u00b7\nFigure1: BPEmergeoperations learned fromdic-\ntionary {\u2018low\u2019, \u2018lowest\u2019, \u2018newer\u2019, \u2018wider\u2019}.\ngorithm 1. In practice, we increase ef\ufb01ciency by\nindexing all pairs, and updating data structures in-\ncrementally.\nThe main difference to other compression al-\ngorithms, such as Huffman encoding, which have\nbeen proposed to produce a variable-length en-\ncoding of words for NMT (Chitnis and DeNero,\n2015), is that our symbol sequences are still in-\nterpretable as subword units, and that the network\ncangeneralize totranslate andproduce newwords\n(unseen attraining time) onthebasisofthesesub-\nword units.\nFigure 1 shows a toy example of learned BPE\noperations. At test time, we \ufb01rst split words into\nsequencesofcharacters,thenapplythelearnedop-\nerationstomergethecharacters intolarger, known\nsymbols. This is applicable to any word, and\nallows for open-vocabulary networks with \ufb01xed\nsymbol vocabularies.3In our example, the OOV\n\u2018lower\u2019 would be segmented into \u2018low er \u00b7\u2019.\n3The only symbols that will be unknown at test time are\nunknown characters, or symbols of which all occurrences\nin the training text have been merged into larger symbols,\nlike \u2018safeguar\u2019, which has alloccurrences in our training t ext\nmerged into \u2018safeguard\u2019. We observed no such symbols at\ntest time, but the issue could be easily solved by recursivel y\nreversing speci\ufb01c merges until allsymbols are known.We evaluate two methods of applying BPE:\nlearning two independent encodings, one for the\nsource, one for the target vocabulary, or learning\nthe encoding on the union of the two vocabular-\nies(whichwecall jointBPE ).4Theformerhasthe\nadvantage of being more compact in terms of text\nand vocabulary size, and having stronger guaran-\ntees that each subword unit has been seen in the\ntraining text of the respective language, whereas\nthelatterimprovesconsistencybetweenthesource\nand the target segmentation. If we apply BPE in-\ndependently, the same name may be segmented\ndifferently in the two languages, which makes it\nharder for the neural models to learn a mapping\nbetween the subword units.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ea998542-7a20-41be-a64b-ca912e79c9d5": {"__data__": {"id_": "ea998542-7a20-41be-a64b-ca912e79c9d5", "embedding": null, "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd9fdfe0-2305-4b69-97ec-f5eda22bc01c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e07d213da7a30bd4dc602df5840652901a8eae0607150638ee2eb5bc48665dfb"}, "2": {"node_id": "d41f9130-62e5-4140-b7eb-3690aaa6f732", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "09e55f9cdcbb05630c786d5b0884cc267c6da9ef158dbe637759c5f38f8d20ba"}}, "hash": "8e39b5bbaa7b9fcb5ce841249f80bb333243911b35b4eb73ddca9123d2cf6f52", "text": "To increase the con-\nsistency between English and Russian segmenta-\ntion despite the differing alphabets, we transliter-\nate the Russian vocabulary into Latin characters\nwith ISO-9 to learn the joint BPE encoding, then\ntransliterate the BPE merge operations back into\nCyrillictoapplythemtotheRussiantrainingtext.5\n4 Evaluation\nWe aim to answer the following empirical ques-\ntions:\n\u2022 Can we improve the translation of rare and\nunseen words in neural machine translation\nby representing them via subword units?\n\u2022 Which segmentation into subword units per-\nforms best in terms of vocabulary size, text\nsize, and translation quality?\nWe perform experiments on data from the\nshared translation task of WMT 2015. For\nEnglish\u2192German, our training set consists of 4.2\nmillion sentence pairs, or approximately 100 mil-\nliontokens. ForEnglish \u2192Russian, thetrainingset\nconsists of 2.6 million sentence pairs, or approx-\nimately 50 million tokens. We tokenize and true-\ncase the data with the scripts provided in Moses\n(Koehn et al., 2007). We use newstest2013 as de-\nvelopment set, and report results on newstest2014\nand newstest2015.\nWe report results with B LEU(mteval-v13a.pl ),\nandCHRF3 (Popovi \u00b4c, 2015), a character n-gram\nF3score which was found to correlate well with\n4In practice, we simply concatenate the source and target\nside of the trainingsettolearnjoint BPE.\n5Since the Russian training text also contains words that\nuse the Latin alphabet, we also apply the Latin BPE opera-\ntions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "74ddba6b-99ca-44c9-8449-11cc73cda500": {"__data__": {"id_": "74ddba6b-99ca-44c9-8449-11cc73cda500", "embedding": null, "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a402834-f7ed-4f26-8200-9b91cdf11b4c", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "65f9073f2453e65782bbe9eb2e88c6371d5f2efa011ee0a3e787ca59556db89c"}, "3": {"node_id": "d5428378-ba1f-4a18-a8a2-299298de8143", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b98866b7321221721a52e2871a569654a3ad22e75eb65782fcf0406a4ddbc805"}}, "hash": "32f451664dd2196b4cad82d877bbe7471a88e616e79f662929c509c1fceaac0e", "text": "human judgments, especially for translations out\nof English (Stanojevi \u00b4c et al., 2015). Since our\nmain claim is concerned with the translation of\nrare and unseen words, we report separate statis-\ntics for these. We measure these through unigram\nF1, which we calculate as the harmonic mean of\nclipped unigram precision and recall.6\nWe perform all experiments with Groundhog7\n(Bahdanau et al., 2015). We generally follow set-\ntings by previous work (Bahdanau et al., 2015;\nJean et al., 2015). All networks have a hidden\nlayer size of 1000, and an embedding layer size\nof620. FollowingJeanetal.(2015), weonlykeep\nashortlist of \u03c4= 30000 words inmemory.\nDuringtraining,weuseAdadelta(Zeiler,2012),\na minibatch size of 80, and reshuf\ufb02e the train-\ning set between epochs. We train a network for\napproximately 7 days, then take the last 4 saved\nmodels (models being saved every 12 hours), and\ncontinue training each with a \ufb01xed embedding\nlayer (as suggested by (Jean et al., 2015)) for 12\nhours. We perform two independent training runs\nfor each models, once with cut-off for gradient\nclipping (Pascanu et al., 2013) of 5.0, once with\na cut-off of 1.0 \u2013 the latter produced better single\nmodels for most settings. We report results of the\nsystemthatperformedbestonourdevelopmentset\n(newstest2013), and of an ensemble of all 8 mod-\nels.\nWe use a beam size of 12 for beam search,\nwith probabilities normalized by sentence length.\nWe use a bilingual dictionary based on fast-align\n(Dyer et al., 2013). For our baseline, this serves\nas back-off dictionary for rare words. Wealso use\nthe dictionary to speed up translation for all ex-\nperiments, only performing thesoftmax over a\ufb01l-\ntered list of candidate translations (like Jean et al.\n(2015), weuse K= 30000;K\u2032= 10).\n4.1 Subwordstatistics\nApart from translation quality, which we will ver-\nify empirically, our main objective is to represent\nan open vocabulary through a compact \ufb01xed-size\nsubword vocabulary, and allow for ef\ufb01cient train-\ning and decoding.8\nStatisticsfordifferentsegmentationsoftheGer-\n6Clipped unigram precision is essentially 1-gram BLEU\nwithout brevity penalty.\n7github.com/sebastien-j/LV_groundhog\n8The timecomplexity ofencoder-decoder architectures is\nat least linear tosequence length, and oversplittingharms ef-\n\ufb01ciency.man side of the parallel data are shown in Table\n1. Asimple baseline is the segmentation of words\nintocharacter n-grams.9Character n-gramsallow\nfor different trade-offs between sequence length\n(# tokens) and vocabulary size (# types), depend-\ning on the choice of n. The increase in sequence\nlength is substantial; one way to reduce sequence\nlength istoleave ashortlist of the kmost frequent\nwordtypesunsegmented. Onlytheunigramrepre-\nsentation is truly open-vocabulary. However, the\nunigram representation performed poorly in pre-\nliminaryexperiments,andwereporttranslationre-\nsultswithabigramrepresentation, whichisempir-\nicallybetter, but unabletoproduce sometokens in\nthetest set with thetraining set vocabulary.\nWe report statistics for several word segmenta-\ntiontechniquesthathaveprovenusefulinprevious\nSMT research, including frequency-based com-\npound splitting (Koehn and Knight, 2003), rule-\nbased hyphenation (Liang, 1983), and Morfessor\n(Creutz and Lagus, 2002). We \ufb01nd that they only\nmoderately reduce vocabulary size, and do not\nsolvetheunknownwordproblem,andwethus\ufb01nd\nthem unsuitable for our goal of open-vocabulary\ntranslation without back-off dictionary.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d5428378-ba1f-4a18-a8a2-299298de8143": {"__data__": {"id_": "d5428378-ba1f-4a18-a8a2-299298de8143", "embedding": null, "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a402834-f7ed-4f26-8200-9b91cdf11b4c", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "65f9073f2453e65782bbe9eb2e88c6371d5f2efa011ee0a3e787ca59556db89c"}, "2": {"node_id": "74ddba6b-99ca-44c9-8449-11cc73cda500", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "32f451664dd2196b4cad82d877bbe7471a88e616e79f662929c509c1fceaac0e"}}, "hash": "b98866b7321221721a52e2871a569654a3ad22e75eb65782fcf0406a4ddbc805", "text": "BPE meets our goal of being open-vocabulary,\nand the learned merge operations can be applied\nto the test set to obtain a segmentation with no\nunknown symbols.10Its main difference from\nthe character-level model is that the more com-\npact representation of BPE allows for shorter se-\nquences, and that the attention model operates\non variable-length units.11Table 1 shows BPE\nwith59500 merge operations, and joint BPEwith\n89500 operations.\nIn practice, we did not include infrequent sub-\nword units in the NMT network vocabulary, since\nthere is noise in the subword symbol sets, e.g.\nbecause of characters from foreign alphabets.\nHence, our network vocabularies in Table 2 are\ntypically slightly smallerthanthenumber oftypes\ninTable 1.\n9Ourcharactern-gramsdonotcrosswordboundaries. We\nmark whether a subword is word-\ufb01nal or not with a special\ncharacter,whichallowsustorestoretheoriginaltokeniza tion.\n10Joint BPE can produce segments that are unknown be-\ncause they only occur in the English training text, but these\nare rare(0.05% of testtokens).\n11We highlighted the limitations of word-level attentionin\nsection 3.1. At the other end of the spectrum, the character\nlevel issuboptimal for alignment (Tiedemann, 2009).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8eb9e29d-7e82-4a3e-8059-2b979e80ac68": {"__data__": {"id_": "8eb9e29d-7e82-4a3e-8059-2b979e80ac68", "embedding": null, "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a051f339-922c-4b89-b4fc-b6940a45dc5a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b847093ebc374ac4b98a54f6ddb75083f3b78a1a511d50d47fd192950a55ecd1"}, "3": {"node_id": "857ade76-a6a8-4a28-bd11-6ed6c78e123f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8f6a03585f186e471e7b096c300a8c56c347dc4efb42e4d6b3d4fbe050bf0626"}}, "hash": "c44ba4af5f2d3ac9c1d7b8ec09d32489ac59ed2ebeadbde412e197e153769984", "text": "vocabulary BLEU CHRF3unigram F 1(%)\nname segmentation shortlist source target single ens-8 single ens-8 all rare OOV\nsyntax-based (Sennrichand Haddow, 2015) 24.4 - 55.3 - 59.1 46.0 37.7\nWUnk - - 300000 500000 20.6 22.8 47.2 48.9 56.7 20.4 0.0\nWDict - - 300000 500000 22.0 24.2 50.5 52.4 58.1 36.8 36.8\nC2-50k char-bigram 50000 60000 60000 22.8 25.3 51.9 53.5 58.4 40.5 30.9\nBPE-60k BPE - 60000 60000 21.5 24.5 52.053.958.4 40.9 29.3\nBPE-J90k BPE(joint) - 90000 90000 22.824.751.754.158.5 41.8 33.6\nTable 2: English\u2192German translation performance (B LEU,CHRF3 and unigram F 1) on newstest2015.\nEns-8: ensemble of 8 models. Best NMTsystem in bold. Unigram F1(with ensembles) iscomputed for\nall words ( n= 44085), rare words (not among top 50000 in training set; n= 2900), and OOVs (not in\ntraining set; n= 1168).\nsegmentation #tokens #types #UNK\nnone 100m 1750000 1079\ncharacters 550m 3000 0\ncharacter bigrams 306m 20000 34\ncharacter trigrams 214m 120000 59\ncompound splitting\u25b3102m 1100000 643\nmorfessor* 109m 544000 237\nhyphenation\u22c4186m 404000 230\nBPE 112m 63000 0\nBPE(joint) 111m 82000 32\ncharacter bigrams129m 69000 34(shortlist: 50000)\nTable 1: Corpus statistics for German training\ncorpus with different word segmentation tech-\nniques. #UNK: number of unknown tokens in\nnewstest2013.\u25b3: (Koehn and Knight, 2003); *:\n(Creutz and Lagus, 2002); \u22c4: (Liang, 1983).\n4.2 Translation experiments\nEnglish\u2192German translation results are shown in\nTable2; English\u2192Russian results in Table3.\nOur baseline WDictisaword-level model with\naback-offdictionary. Itdiffersfrom WUnkinthat\nthelatter usesnoback-off dictionary, andjust rep-\nresents out-of-vocabulary words as UNK12. The\nback-off dictionary improves unigram F 1for rare\nand unseen words, although the improvement is\nsmaller for English \u2192Russian, since the back-off\ndictionary is incapable of transliterating names.\nAllsubwordsystemsoperatewithoutaback-off\ndictionary. We \ufb01rst focus on unigram F 1, where\nall systems improve over the baseline, especially\nfor rare words (36.8% \u219241.8% for EN\u2192DE;\n26.5%\u219229.7% for EN\u2192RU). For OOVs, the\nbaseline strategy of copying unknown words\nworkswellforEnglish \u2192German. However,when\nalphabets differ, like in English \u2192Russian, the\nsubword models do much better.\n12We useUNKfor words that are outside the model vo-\ncabulary, and OOVfor those that do not occur inthe training\ntext.Unigram F 1scores indicate that learning the\nBPE symbols on the vocabulary union ( BPE-\nJ90k) is more effective than learning them sep-\narately (BPE-60k ), and more effective than using\ncharacter bigramswithashortlist of50000unseg-\nmented words ( C2-50k), but all reported subword\nsegmentations are viable choices and outperform\ntheback-off dictionary baseline.\nOur subword representations cause big im-\nprovements in the translation of rare and unseen\nwords, but these only constitute 9-11% of the test\nsets.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "857ade76-a6a8-4a28-bd11-6ed6c78e123f": {"__data__": {"id_": "857ade76-a6a8-4a28-bd11-6ed6c78e123f", "embedding": null, "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a051f339-922c-4b89-b4fc-b6940a45dc5a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b847093ebc374ac4b98a54f6ddb75083f3b78a1a511d50d47fd192950a55ecd1"}, "2": {"node_id": "8eb9e29d-7e82-4a3e-8059-2b979e80ac68", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c44ba4af5f2d3ac9c1d7b8ec09d32489ac59ed2ebeadbde412e197e153769984"}}, "hash": "8f6a03585f186e471e7b096c300a8c56c347dc4efb42e4d6b3d4fbe050bf0626", "text": "Since rare words tend to carry central in-\nformation in a sentence, we suspect that B LEU\nandCHRF3 underestimate their effect on transla-\ntion quality. Still, we also see improvements over\nthe baseline in total unigram F 1, as well as B LEU\nandCHRF3, and the subword ensembles outper-\nform the WDict baseline by 0.3\u20131.3 B LEUand\n0.6\u20132CHRF3. There is some inconsistency be-\ntweenB LEUandCHRF3,whichweattributetothe\nfact that B LEUhas a precision bias, and CHRF3 a\nrecall bias.\nFor English\u2192German, we observe the best\nBLEUscore of 25.3 with C2-50k, but the best\nCHRF3 score of 54.1 with BPE-J90k. For com-\nparison to the (to our knowledge) best non-neural\nMT system on this data set, we report syntax-\nbased SMT results (Sennrich and Haddow, 2015).\nWe observe that our best systems outperform the\nsyntax-based system in terms of B LEU, but not\nin terms of CHRF3. Regarding other neural sys-\ntems, Luong et al. (2015a) report a B LEUscore of\n25.9onnewstest2015, butwenotethattheyusean\nensemble of 8 independently trained models, and\nalso report strong improvements from applying\ndropout, which we did not use. We are con\ufb01dent\nthat our improvements to the translation of rare\nwords are orthogonal to improvements achievable\nthrough other improvements in the network archi-", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "be4f57e5-5637-473d-a7ae-ac79fe464385": {"__data__": {"id_": "be4f57e5-5637-473d-a7ae-ac79fe464385", "embedding": null, "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7bc9bee8-cb63-4c8a-8b58-15e0baf51f92", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "15756d60724040ac1be7384080859e539f530a5ab38c462eacf1296e8bac95c3"}, "3": {"node_id": "85cca884-cba4-43e1-afde-d2713a0c1180", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4518ba82cf50067997ab05aeaeac2faceea8457574e2cbb5f9c2a5bb45c9d18e"}}, "hash": "3a27143f4200d4f0e9290cfbed2aff1cd74898fef899d74ad60e48ed5283ab23", "text": "tecture, training algorithm, or better ensembles.\nFor English\u2192Russian, the state of the art is\nthe phrase-based system by Haddow et al. (2015).\nIt outperforms our WDict baseline by 1.5 B LEU.\nThe subword models are a step towards closing\nthis gap, and BPE-J90k yields an improvement of\n1.3 BLEU, and 2.0 CHRF3, over WDict.\nAsafurther commentonourtranslation results,\nwe want to emphasize that performance variabil-\nity is still an open problem with NMT.Onour de-\nvelopment set, we observe differences of up to 1\nBLEUbetween different models. For single sys-\ntems, we report the results of the model that per-\nforms best on dev (out of 8), which has a stabi-\nlizing effect, but how to control for randomness\ndeserves further attention in future research.\n5 Analysis\n5.1 Unigram accuracy\nOurmainclaimsarethatthetranslationofrareand\nunknown words is poor in word-level NMT mod-\nels, and that subword models improve the trans-\nlation of these word types. To further illustrate\nthe effect of different subword segmentations on\nthe translation of rare and unseen words, we plot\ntarget-side words sorted by their frequency in the\ntraining set.13To analyze the effect of vocabulary\nsize,wealsoincludethesystem C2-3/500k ,which\nis a system with the same vocabulary size as the\nWDict baseline, and character bigrams to repre-\nsent unseen words.\nFigure 2 shows results for the English\u2013German\nensemble systems on newstest2015. Unigram\nF1of all systems tends to decrease for lower-\nfrequency words. Thebaseline system has a spike\nin F1for OOVs, i.e. words that do not occur in\nthe training text. This is because a high propor-\ntion of OOVs are names, for which a copy from\nthe source to the target text is a good strategy for\nEnglish\u2192German.\nThesystemswithatargetvocabularyof500000\nwords mostly differ in how well they translate\nwords with rank > 500000. A back-off dictionary\nis an obvious improvement over producing UNK,\nbutthesubwordsystemC2-3/500k achievesbetter\nperformance. Note that all OOVs that the back-\noff dictionary produces are words that are copied\nfromthesource,usuallynames,whilethesubword\n13We perform binning of words withthe same training set\nfrequency, and applybezier smoothing tothe graph.systems can productively form new words such as\ncompounds.\nFor the 50000 most frequent words, the repre-\nsentation is the same for all neural networks, and\nall neural networks achieve comparable unigram\nF1for this category. For the interval between fre-\nquency rank 50000 and 500000, the comparison\nbetween C2-3/500k and C2-50k unveils an inter-\nesting difference. The two systems only differ in\nthesizeoftheshortlist, withC2-3/500k represent-\ning words in this interval as single units, and C2-\n50k via subword units. We \ufb01nd that the perfor-\nmance of C2-3/500k degrades heavily up to fre-\nquency rank 500000, at which point the model\nswitches to a subword representation and perfor-\nmance recovers. The performance of C2-50k re-\nmains more stable. We attribute this to the fact\nthat subword units are less sparse than words. In\nour training set, the frequency rank 50000 corre-\nsponds to a frequency of 60 in the training data;\nthe frequency rank 500000 to a frequency of 2.\nBecause subword representations are less sparse,\nreducing the size of the network vocabulary, and\nrepresenting more words via subword units, can\nlead tobetter performance.\nThe F1numbers hide some qualitative differ-\nences between systems. For English \u2192German,\nWDict produces few OOVs (26.5% recall), but\nwithhighprecision(60.6%),whereasthesubword\nsystemsachievehigher recall, butlowerprecision.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "85cca884-cba4-43e1-afde-d2713a0c1180": {"__data__": {"id_": "85cca884-cba4-43e1-afde-d2713a0c1180", "embedding": null, "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7bc9bee8-cb63-4c8a-8b58-15e0baf51f92", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "15756d60724040ac1be7384080859e539f530a5ab38c462eacf1296e8bac95c3"}, "2": {"node_id": "be4f57e5-5637-473d-a7ae-ac79fe464385", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "3a27143f4200d4f0e9290cfbed2aff1cd74898fef899d74ad60e48ed5283ab23"}}, "hash": "4518ba82cf50067997ab05aeaeac2faceea8457574e2cbb5f9c2a5bb45c9d18e", "text": "We note that the character bigram model C2-50k\nproduces the most OOV words, and achieves rel-\natively low precision of 29.1% for this category.\nHowever, it outperforms the back-off dictionary\nin recall (33.0%). BPE-60k, which suffers from\ntransliteration (or copy) errors due to segmenta-\ntion inconsistencies, obtains a slightly better pre-\ncision(32.4%),butaworserecall(26.6%). Incon-\ntrast to BPE-60k, the joint BPEencoding of BPE-\nJ90k improves both precision (38.6%) and recall\n(29.8%).\nFor English\u2192Russian, unknown names can\nonly rarely be copied, and usually require translit-\neration. Consequently, the WDict baseline per-\nforms more poorly for OOVs (9.2% precision;\n5.2% recall), and the subword models improve\nboth precision and recall (21.9% precision and\n15.6% recall for BPE-J90k). The full unigram F 1\nplot isshown inFigure 3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "041f3096-e99f-4022-bdc7-2c70a15b014f": {"__data__": {"id_": "041f3096-e99f-4022-bdc7-2c70a15b014f", "embedding": null, "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "406a4296-6669-468a-bac1-42083f0c6181", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e0cfef08d3bc2ceec9c4c4991c76f01561e1d0d9a308814b7a285b73d6d67b4d"}, "3": {"node_id": "63f19542-2331-4d54-b081-881dd6d50775", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ab5c33e543ae93090f68f398cb2ddc6b135e287eb9e7fb2f45a28a6250b099ce"}}, "hash": "5bfe9aba57b7d150946832af9a145c5dbee11da824e8f8fb92d13d95f295d0f9", "text": "vocabulary BLEU CHRF3unigram F 1(%)\nname segmentation shortlist source target single ens-8 single ens-8 all rare OOV\nphrase-based (Haddow et al.,2015) 24.3 - 53.8 - 56.0 31.3 16.5\nWUnk - - 300000 500000 18.8 22.4 46.5 49.9 54.2 25.2 0.0\nWDict - - 300000 500000 19.1 22.8 47.5 51.0 54.8 26.5 6.6\nC2-50k char-bigram 50000 60000 60000 20.9 24.1 49.0 51.6 55.2 27.8 17.4\nBPE-60k BPE - 60000 60000 20.5 23.6 49.852.755.3 29.7 15.6\nBPE-J90k BPE(joint) - 90000 100000 20.424.149.753.055.8 29.7 18.3\nTable 3: English\u2192Russian translation performance (B LEU,CHRF3 and unigram F 1) on newstest2015.\nEns-8: ensemble of 8 models. Best NMTsystem in bold. Unigram F1(with ensembles) iscomputed for\nall words ( n= 55654), rare words (not among top 50000 in training set; n= 5442), and OOVs (not in\ntraining set; n= 851).\n10010110210310410510600.20.40.60.81\n50000 500000\ntraining set frequency rankunigram F 1\nBPE-J90k\nC2-50k\nC2-300/500k\nWDict\nWUnk\nFigure 2: English\u2192German unigram F 1on new-\nstest2015 plotted by training set frequency rank\nfor different NMTsystems.\n10010110210310410510600.20.40.60.81\n50000 500000\ntraining set frequency rankunigram F 1\nBPE-J90k\nC2-50k\nWDict\nWUnk\nFigure 3: English\u2192Russian unigram F 1on new-\nstest2015 plotted by training set frequency rank\nfor different NMTsystems.5.2 ManualAnalysis\nTable 4 shows two translation examples for\nthe translation direction English \u2192German, Ta-\nble 5 for English\u2192Russian. The baseline sys-\ntem fails for all of the examples, either by delet-\ning content ( health), or by copying source words\nthat should be translated or transliterated. The\nsubword translations of health research insti-\ntutesshow that the subword systems are capa-\nbleoflearning translations whenoversplitting ( re-\nsearch\u2192Fo|rs|ch|un|g), or whenthesegmentation\ndoes not match morpheme boundaries: the seg-\nmentation Forschungs |instituten wouldbelinguis-\ntically more plausible, and simpler to align to the\nEnglishresearch institutes , than the segmentation\nForsch|ungsinstitu |tenintheBPE-60ksystem, but\nstill, a correct translation is produced. If the sys-\ntems have failed to learn a translation due to data\nsparseness,likefor asinine,whichshouldbetrans-\nlated asdumm,weseetranslations that are wrong,\nbutcouldbeplausible for(partial) loanwords ( asi-\nnine Situation\u2192Asinin-Situation ).\nThe English\u2192Russian examples show that\nthe subword systems are capable of translitera-\ntion. However, transliteration errors do occur,\neither due to ambiguous transliterations, or be-\ncause of non-consistent segmentations between\nsource and target text which make it hard for\nthe system to learn a transliteration mapping.\nNote that the BPE-60k system encodes Mirza-\nyevainconsistently for the two language pairs\n(Mirz|ayeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430Mir|za|eva). This ex-\nample is still translated correctly, but we observe\nspurious insertions and deletions of characters in\nthe BPE-60k system.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63f19542-2331-4d54-b081-881dd6d50775": {"__data__": {"id_": "63f19542-2331-4d54-b081-881dd6d50775", "embedding": null, "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "406a4296-6669-468a-bac1-42083f0c6181", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e0cfef08d3bc2ceec9c4c4991c76f01561e1d0d9a308814b7a285b73d6d67b4d"}, "2": {"node_id": "041f3096-e99f-4022-bdc7-2c70a15b014f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "5bfe9aba57b7d150946832af9a145c5dbee11da824e8f8fb92d13d95f295d0f9"}}, "hash": "ab5c33e543ae93090f68f398cb2ddc6b135e287eb9e7fb2f45a28a6250b099ce", "text": "An example is the translit-\neration of rak\ufb01sk, where a \u043fis inserted and a \u043a\nis deleted. We trace this error back to transla-\ntion pairs in the training data with inconsistent\nsegmentations, such as ( p|rak|ri|ti\u2192\u043f\u0440\u0430|\u043a\u0440\u0438\u0442|\u0438", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd1a448b-490c-40cb-b9fc-f8fca2d6629c": {"__data__": {"id_": "dd1a448b-490c-40cb-b9fc-f8fca2d6629c", "embedding": null, "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65c138df-617c-4874-8abf-4d6feef1fe9e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b78cc72acbc6e5b85497d39b3a50c9f1d54c63067257c3327ef2148737dce006"}, "3": {"node_id": "2c16c064-0a0f-48a1-a538-25289bd81aed", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "2ee5663cbb89735a7a42db502393016dffb37e03e52bac12b4ebb9be2b6787fe"}}, "hash": "d9d175e49247292f137b2dc07e905c1a8f59644e7c4225ef23b858680e9487d7", "text": "system sentence\nsource healthresearchinstitutes\nreference Gesundheitsforschungsinstitute\nWDict Forschungsinstitute\nC2-50k Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n\nBPE-60k Gesundheits |forsch|ungsinstitu |ten\nBPE-J90k Gesundheits |forsch|ungsin|stitute\nsource asinine situation\nreference dumme Situation\nWDict asinine situation \u2192UNK\u2192asinine\nC2-50k as|in|in|e situation \u2192As|in|en|si|tu|at|io|n\nBPE-60k as|in|ine situation \u2192A|in|line-|Situation\nBPE-J90K as|in|ine situation \u2192As|in|in-|Situation\nTable 4: English\u2192German translation example.\n\u201c|\u201d marks subword boundaries.\nsystem sentence\nsource Mirzayeva\nreference \u041c\u0438\u0440\u0437\u0430\u0435\u0432\u0430 (Mirzaeva)\nWDict Mirzayeva \u2192UNK\u2192Mirzayeva\nC2-50k Mi|rz|ay|ev|a\u2192\u041c\u0438|\u0440\u0437|\u0430\u0435|\u0432\u0430(Mi|rz|ae|va)\nBPE-60k Mirz|ayeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430(Mir|za|eva)\nBPE-J90k Mir|za|yeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430(Mir|za|eva)\nsource rak\ufb01sk\nreference \u0440\u0430\u043a\u0444\u0438\u0441\u043a\u0430 (rak\ufb01ska)\nWDict rak\ufb01sk\u2192UNK\u2192rak\ufb01sk\nC2-50k ra|kf|is|k\u2192\u0440\u0430|\u043a\u0444|\u0438\u0441|\u043a(ra|kf|is|k)\nBPE-60k rak|f|isk\u2192\u043f\u0440\u0430|\u0444|\u0438\u0441\u043a(pra|f|isk)\nBPE-J90k rak|f|isk\u2192\u0440\u0430\u043a|\u0444|\u0438\u0441\u043a\u0430(rak|f|iska)\nTable 5: English\u2192Russian translation examples.\n\u201c|\u201d marks subword boundaries.\n(pra|krit|i)),fromwhichthetranslation( rak\u2192\u043f\u0440\u0430)\nis erroneously learned. The segmentation of the\njoint BPE system (BPE-J90k) is more consistent\n(pra|krit|i\u2192\u043f\u0440\u0430|\u043a\u0440\u0438\u0442|\u0438(pra|krit|i)).\n6 Conclusion\nThe main contribution of this paper is that we\nshow that neural machine translation systems are\ncapable of open-vocabulary translation by repre-\nsenting rare and unseen words as a sequence of\nsubword units.14This is both simpler and more\neffective than using a back-off translation model.\nWe introduce a variant of byte pair encoding for\nword segmentation, which is capable of encod-\ning open vocabularies with a compact symbol vo-\ncabulary of variable-length subword units. We\nshow performance gains over the baseline with\nbothBPEsegmentation, andasimplecharacter bi-\ngram segmentation.\nOur analysis shows that not only out-of-\nvocabulary words, but also rare in-vocabulary\nwords are translated poorly by our baseline NMT\n14The source code of the segmentation algorithms\nis available at https://github.com/rsennrich/\nsubword-nmt .system, and that reducing the vocabulary size\nof subword models can actually improve perfor-\nmance. Inthiswork,ourchoiceofvocabulary size\nis somewhat arbitrary, and mainly motivated by\ncomparison to prior work. One avenue of future\nresearchistolearntheoptimalvocabulary sizefor\na translation task, which we expect to depend on\nthe language pair and amount of training data, au-\ntomatically. We also believe there is further po-\ntential in bilingually informed segmentation algo-\nrithms to create more alignable subword units, al-\nthough the segmentation algorithm cannot rely on\nthetarget text at runtime.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2c16c064-0a0f-48a1-a538-25289bd81aed": {"__data__": {"id_": "2c16c064-0a0f-48a1-a538-25289bd81aed", "embedding": null, "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65c138df-617c-4874-8abf-4d6feef1fe9e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b78cc72acbc6e5b85497d39b3a50c9f1d54c63067257c3327ef2148737dce006"}, "2": {"node_id": "dd1a448b-490c-40cb-b9fc-f8fca2d6629c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d9d175e49247292f137b2dc07e905c1a8f59644e7c4225ef23b858680e9487d7"}}, "hash": "2ee5663cbb89735a7a42db502393016dffb37e03e52bac12b4ebb9be2b6787fe", "text": "While the relative effectiveness will depend on\nlanguage-speci\ufb01c factors such as vocabulary size,\nwe believe that subword segmentations are suit-\nable for most language pairs, eliminating the need\nfor large NMTvocabularies or back-off models.\nAcknowledgments\nWe thank Maja Popovi \u00b4c for her implementa-\ntion of CHRF, with which we veri\ufb01ed our re-\nimplementation. The research presented in this\npublication was conducted in cooperation with\nSamsung Electronics Polska sp. z o.o. - Sam-\nsung R&D Institute Poland. This project received\nfunding from theEuropean Union\u2019s Horizon 2020\nresearch and innovation programme under grant\nagreement 645452 (QT21).\nReferences\nDzmitryBahdanau,KyunghyunCho,andYoshuaBen-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In Proceedings of\ntheInternationalConferenceonLearningRepresen-\ntations(ICLR) .\nIssamBazzi andJamesR. Glass. 2000. Modelingout-\nof-vocabulary words for robust speech recognition.\nInSixth International Conference on Spoken Lan-\nguage Processing, ICSLP 2000 / INTERSPEECH\n2000,pages401\u2013404,Beijing,China.\nJan A. Botha and Phil Blunsom. 2014. Compositional\nMorphology for Word Representations and Lan-\nguage Modelling. In Proceedings of the 31st Inter-\nnational Conference on Machine Learning (ICML) ,\nBeijing,China.\nRohan Chitnis and John DeNero. 2015. Variable-\nLength Word Encodings for Neural Translation\nModels. In Proceedings of the 2015 Conference on\nEmpiricalMethodsinNaturalLanguageProcessing\n(EMNLP) .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "13fb6ce1-69ae-4efc-b403-c84b67c09020": {"__data__": {"id_": "13fb6ce1-69ae-4efc-b403-c84b67c09020", "embedding": null, "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a08a854c-ce44-4fe0-adbe-4c8c4fd402e0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9c20d38f211497bbb4d7110e958e88ac66503c564128a5aad058b8ddaefa9930"}, "3": {"node_id": "6e4a52a6-6ca3-45d7-8222-fa3a534edd16", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4101f7aef6a9fb95f2fae1e5aa722d3a3868dcb0147274612b58c72292c5811a"}}, "hash": "7daacb8f72585d16fe506fab5abf1e2dcd2ed28a855125ffa0d84b29352266d7", "text": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder\u2013\nDecoderforStatisticalMachineTranslation. In Pro-\nceedingsofthe2014ConferenceonEmpiricalMeth-\nods in Natural Language Processing (EMNLP) ,\npages 1724\u20131734, Doha, Qatar. Association for\nComputationalLinguistics.\nMathiasCreutzandKristaLagus. 2002. Unsupervised\nDiscovery of Morphemes. In Proceedings of the\nACL-02WorkshoponMorphologicalandPhonolog-\nicalLearning ,pages21\u201330.AssociationforCompu-\ntationalLinguistics.\nNadirDurrani,HassanSajjad,HieuHoang,andPhilipp\nKoehn. 2014. IntegratinganUnsupervisedTranslit-\neration Model into Statistical Machine Translation.\nInProceedings of the 14th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, EACL 2014 , pages 148\u2013153, Gothen-\nburg,Sweden.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A Simple, Fast, and Effective Reparame-\nterization of IBM Model 2. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for ComputationalLinguistics: Hu-\nman Language Technologies , pages 644\u2013648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nPhilip Gage. 1994. A New Algorithm for Data Com-\npression. CUsers J. ,12(2):23\u201338,February.\nBarryHaddow,MatthiasHuck,AlexandraBirch,Niko-\nlay Bogoychev, and Philipp Koehn. 2015. The\nEdinburgh/JHU Phrase-based Machine Translation\nSystems for WMT 2015. In Proceedings of the\nTenth Workshop on Statistical MachineTranslation ,\npages 126\u2013133, Lisbon, Portugal. Association for\nComputationalLinguistics.\nS\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On Using Very Large\nTarget Vocabulary for Neural Machine Translation.\nInProceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1\u201310, Beijing, China. Association for Computa-\ntionalLinguistics.\nNal Kalchbrennerand Phil Blunsom. 2013. Recurrent\nContinuous Translation Models. In Proceedings of\nthe 2013 Conference on Empirical Methods in Nat-\nural Language Processing , Seattle. Association for\nComputationalLinguistics.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2015. Character-Aware Neural Lan-\nguageModels. CoRR,abs/1508.06615.Philipp Koehn and Kevin Knight. 2003. Empirical\nMethods for Compound Splitting. In EACL \u201903:\nProceedings of the Tenth Conference on European\nChapter of the Association for Computational Lin-\nguistics, pages 187\u2013193, Budapest, Hungary. Asso-\nciationforComputationalLinguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch,MarcelloFederico,NicolaBertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ond \u02c7rej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nSource Toolkit for Statistical Machine Translation.\nInProceedings of the ACL-2007 Demo and Poster\nSessions, pages 177\u2013180, Prague, Czech Republic.\nAssociationforComputationalLinguistics.\nFranklin M. Liang. 1983.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e4a52a6-6ca3-45d7-8222-fa3a534edd16": {"__data__": {"id_": "6e4a52a6-6ca3-45d7-8222-fa3a534edd16", "embedding": null, "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a08a854c-ce44-4fe0-adbe-4c8c4fd402e0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9c20d38f211497bbb4d7110e958e88ac66503c564128a5aad058b8ddaefa9930"}, "2": {"node_id": "13fb6ce1-69ae-4efc-b403-c84b67c09020", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7daacb8f72585d16fe506fab5abf1e2dcd2ed28a855125ffa0d84b29352266d7"}}, "hash": "4101f7aef6a9fb95f2fae1e5aa722d3a3868dcb0147274612b58c72292c5811a", "text": "Franklin M. Liang. 1983. Word hy-phen-a-tion by\ncom-put-er . Ph.D. thesis, Stanford University, De-\npartmentofLinguistics,Stanford,CA.\nWang Ling, Chris Dyer, Alan W. Black, Isabel Tran-\ncoso,RamonFermandez,SilvioAmir,LuisMarujo,\nand Tiago Luis. 2015a. Finding Function in Form:\nCompositional Character Models for Open Vocab-\nulary Word Representation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP) , pages 1520\u2013\n1530, Lisbon, Portugal. Association for Computa-\ntionalLinguistics.\nWang Ling, Isabel Trancoso, Chris Dyer, and Alan W.\nBlack. 2015b. Character-based Neural Machine\nTranslation. ArXiv e-prints ,November.\nThang Luong, Richard Socher, and Christopher D.\nManning. 2013. Better Word Representations\nwith Recursive Neural Networks for Morphology.\nInProceedings of the Seventeenth Conference on\nComputationalNaturalLanguageLearning,CoNLL\n2013,So\ufb01a,Bulgaria,August8-9,2013 ,pages104\u2013\n113.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015a. Effective Approaches to Attention-\nbased Neural Machine Translation. In Proceed-\nings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1412\u2013\n1421, Lisbon, Portugal. Association for Computa-\ntionalLinguistics.\nThang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\nand Wojciech Zaremba. 2015b. Addressing the\nRare Word Problemin NeuralMachine Translation.\nInProceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n11\u201319, Beijing, China. Association for Computa-\ntionalLinguistics.\nTomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSonLe, StefanKombrink,andJan Cernock\u00fd. 2012.\nSubword Language Modeling with Neural Net-\nworks. Unpublished.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "925368db-e7ca-4466-af71-1e77a5b65668": {"__data__": {"id_": "925368db-e7ca-4466-af71-1e77a5b65668", "embedding": null, "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0435b871-4048-4e5a-aa21-82cb34b77642", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "10014486c9baeeee64b08545d573c35611589b59cbf4a0dc86c0293a8e040bcd"}, "3": {"node_id": "57595640-e086-42ce-a3e5-4f12fcb124a3", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fc9fe24a9530d91adde6e81f5dc869f9132c617d13f22c8d567f2f6b85a1d626"}}, "hash": "3b854477ab79723ba80154f66463120754b76b219e1da1840af4720c2844add8", "text": "Graham Neubig, Taro Watanabe, Shinsuke Mori, and\nTatsuya Kawahara. 2012. Machine Translation\nwithoutWordsthroughSubstringAlignment. In The\n50thAnnualMeeting of the AssociationforCompu-\ntationalLinguistics, Proceedingsof the Conference,\nJuly8-14,2012,JejuIsland,Korea-Volume1: Long\nPapers,pages165\u2013174.\nSonja Nie\u00dfen and Hermann Ney. 2000. Improving\nSMT quality with morpho-syntactic analysis. In\n18thInt. Conf. on ComputationalLinguistics , pages\n1081\u20131085.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Ben-\ngio. 2013. On the dif\ufb01culty of training recurrent\nneural networks. In Proceedings of the 30th Inter-\nnational Conference on Machine Learning, ICML\n2013,pages1310\u20131318,Atlanta,USA.\nMaja Popovi \u00b4c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedingsof the\nTenth Workshop on Statistical MachineTranslation ,\npages 392\u2013395, Lisbon, Portugal. Association for\nComputationalLinguistics.\nRico Sennrich and Barry Haddow. 2015. A Joint\nDependency Model of Morphological and Syntac-\ntic Structure for Statistical Machine Translation. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages\n2081\u20132087,Lisbon,Portugal.AssociationforCom-\nputationalLinguistics.\nBenjamin Snyder and Regina Barzilay. 2008. Unsu-\npervised Multilingual Learning for Morphological\nSegmentation. In Proceedings of ACL-08: HLT ,\npages 737\u2013745, Columbus, Ohio. Association for\nComputationalLinguistics.\nDavid Stallard, Jacob Devlin, Michael Kayser,\nYoongKeokLee,andReginaBarzilay. 2012. Unsu-\npervised Morphology Rivals Supervised Morphol-\nogy for Arabic MT. In The 50th Annual Meeting of\nthe Association for ComputationalLinguistics, Pro-\nceedings of the Conference, July 8-14, 2012, Jeju\nIsland,Korea -Volume2: ShortPapers , pages322\u2013\n327.\nMilo\u0161 Stanojevi \u00b4c, Amir Kamran, Philipp Koehn, and\nOnd\u02c7rej Bojar. 2015. Results of the WMT15 Met-\nricsSharedTask. In ProceedingsoftheTenthWork-\nshoponStatisticalMachineTranslation ,pages256\u2013\n273, Lisbon, Portugal. Association for Computa-\ntionalLinguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.\nSequence to Sequence Learning with Neural Net-\nworks. In Advancesin Neural InformationProcess-\ningSystems27: AnnualConferenceonNeuralInfor-\nmationProcessingSystems2014 ,pages3104\u20133112,\nMontreal,Quebec,Canada.\nJ\u00f6rg Tiedemann. 2009. Character-based PSMT for\nClosely Related Languages. In Proceedingsof 13th\nAnnualConference of the EuropeanAssociation for\nMachineTranslation(EAMT\u201909) ,pages12\u201319.J\u00f6rg Tiedemann. 2012. Character-Based Pivot Trans-\nlation for Under-Resourced Languages and Do-\nmains. In Proceedingsofthe13thConferenceofthe\nEuropean Chapter of the Association for Computa-\ntionalLinguistics ,pages141\u2013151,Avignon,France.\nAssociationforComputationalLinguistics.\nDavid Vilar, Jan-Thorsten Peter, and Hermann Ney.\n2007. Can We Translate Letters? In Second Work-\nshop on Statistical Machine Translation , pages 33\u2013\n39, Prague, Czech Republic. Association for Com-\nputationalLinguistics.\nSami Virpioja, Jaakko J. V\u00e4yrynen, Mathias Creutz,\nand Markus Sadeniemi. 2007.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57595640-e086-42ce-a3e5-4f12fcb124a3": {"__data__": {"id_": "57595640-e086-42ce-a3e5-4f12fcb124a3", "embedding": null, "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0435b871-4048-4e5a-aa21-82cb34b77642", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "10014486c9baeeee64b08545d573c35611589b59cbf4a0dc86c0293a8e040bcd"}, "2": {"node_id": "925368db-e7ca-4466-af71-1e77a5b65668", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "3b854477ab79723ba80154f66463120754b76b219e1da1840af4720c2844add8"}}, "hash": "fc9fe24a9530d91adde6e81f5dc869f9132c617d13f22c8d567f2f6b85a1d626", "text": "2007. Morphology-Aware\nStatistical Machine Translation Based on Morphs\nInduced in an Unsupervised Manner. In Proceed-\nings of the Machine Translation Summit XI , pages\n491\u2013498,Copenhagen,Denmark.\nMatthew D. Zeiler. 2012. ADADELTA: An Adaptive\nLearningRate Method. CoRR,abs/1212.5701.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46fe14bb-fad4-4f6f-8c4d-2531488cc735": {"__data__": {"id_": "46fe14bb-fad4-4f6f-8c4d-2531488cc735", "embedding": null, "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97d10ad-fd39-4d39-bec4-f1c888d80c37", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4bd5e7287e63f79941810f2464d4634a36bc23aa227a7140a55570d934e8eee3"}, "3": {"node_id": "04a952db-d803-4a90-8852-a299026eedfc", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a5206a800c0e53c6860c74f622b3cf2d85fa834233adba28f9fd9eb18296529b"}}, "hash": "54e707e36290845aa6cfa5fe6c780b734f97ced70177f93509b5d3979e9fbd05", "text": "arXiv:1511.06114v4  [cs.LG]  1 Mar 2016Publishedasa conferencepaperat ICLR2016\nMULTI-TASKSEQUENCE TO SEQUENCE LEARNING\nMinh-Thang Luong\u2217,QuocV.Le, IlyaSutskever, OriolVinyals,Lukasz Kaiser\nGoogleBrain\nlmthang@stanford.edu, {qvl,ilyasu,vinyals,lukaszkaiser }@google.com\nABSTRACT\nSequenceto sequencelearninghasrecentlyemergedasa new p aradigminsuper-\nvised learning. To date, most ofits applicationsfocusedon onlyone task and not\nmuchworkexploredthisframeworkformultipletasks. Thisp aperexaminesthree\nmulti-tasklearning(MTL)settingsforsequenceto sequenc emodels: (a)the one-\nto-manysetting \u2013 where the encoder is shared between several tasks s uch as ma-\nchinetranslationandsyntacticparsing,(b)the many-to-one setting\u2013 usefulwhen\nonly the decoder can be shared, as in the case of translation a nd image caption\ngeneration, and (c) the many-to-many setting \u2013 where multiple encoders and de-\ncodersareshared,whichis the case with unsupervisedobjec tivesandtranslation.\nOur results show that training on a small amount of parsing an d image caption\ndatacanimprovethetranslationqualitybetweenEnglishan dGermanbyupto 1.5\nBLEUpointsoverstrongsingle-taskbaselinesontheWMTben chmarks. Further-\nmore,wehaveestablishedanew state-of-the-art resultinconstituentparsingwith\n93.0F1. Lastly, we reveal interestingpropertiesof the two unsupe rvisedlearning\nobjectives,autoencoderandskip-thought,inthe MTLconte xt: autoencoderhelps\nlessin termsofperplexitiesbutmoreonBLEUscorescompare dtoskip-thought.\n1 INTRODUCTION\nMulti-task learning (MTL) is an important machine learning paradigm that aims at improving\nthe generalization performance of a task using other relate d tasks. Such framework has been\nwidelystudiedbyThrun(1996);Caruana(1997);Evgeniou&P ontil(2004);Ando& Zhang(2005);\nArgyriouetal. (2007); Kumar& III (2012), among manyothers . In the context of deep neural net-\nworks, MTL has been applied successfully to various problem s ranging from language (Liuet al.,\n2015),to vision(Donahueet al.,2014), andspeech(Heigold etal., 2013;Huangetal.,2013).\nRecently, sequence to sequence ( seq2seq) learning, proposed by Kalchbrenner&Blunsom (2013),\nSutskeveret al. (2014), and Cho etal. (2014), emerges as an e ffective paradigm for dealing with\nvariable-length inputs and outputs. seq2seqlearning, at its core, uses recurrent neural networks\nto map variable-length input sequences to variable-length output sequences. While relatively new,\ntheseq2seqapproachhas achieved state-of-the-artresults in not only its original application \u2013 ma-\nchine translation \u2013 (Luonget al., 2015b; Jean etal., 2015a; Luongetal., 2015a; Jean etal., 2015b;\nLuong&Manning, 2015), but also image caption generation (V inyalsetal., 2015b), and con-\nstituencyparsing(Vinyalset al., 2015a).\nDespitethepopularityofmulti-tasklearningandsequence tosequencelearning,therehasbeenlittle\nwork in combining MTL with seq2seqlearning. To the best of our knowledge, there is only one\nrecent publication by Donget al. (2015) which applies a seq2seqmodels for machine translation,\nwhere the goal is to translate from one language to multiple l anguages.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "04a952db-d803-4a90-8852-a299026eedfc": {"__data__": {"id_": "04a952db-d803-4a90-8852-a299026eedfc", "embedding": null, "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97d10ad-fd39-4d39-bec4-f1c888d80c37", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4bd5e7287e63f79941810f2464d4634a36bc23aa227a7140a55570d934e8eee3"}, "2": {"node_id": "46fe14bb-fad4-4f6f-8c4d-2531488cc735", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "54e707e36290845aa6cfa5fe6c780b734f97ced70177f93509b5d3979e9fbd05"}}, "hash": "a5206a800c0e53c6860c74f622b3cf2d85fa834233adba28f9fd9eb18296529b", "text": "In this work, we propose\nthree MTL approachesthat complementone another: (a) the one-to-many approach\u2013 for tasksthat\ncan have an encoder in common, such as translation and parsin g; this applies to the multi-target\ntranslation setting in (Donget al., 2015) as well, (b) the many-to-one approach \u2013 useful for multi-\nsource translation or tasks in which only the decoder can be e asily shared, such as translation and\nimage captioning, and lastly, (c) the many-to-many approach \u2013 which share multiple encoders and\ndecoders through which we study the effect of unsupervised l earning in translation. We show that\nsyntactic parsing and image caption generation improves th e translation quality between English\n\u2217Minh-Thang Luong isalsoa student atStanfordUniversity.\n1", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3473b3cd-4dfe-4f70-a391-44577870b020": {"__data__": {"id_": "3473b3cd-4dfe-4f70-a391-44577870b020", "embedding": null, "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05696886-7400-41a8-9087-a90319edb626", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4279281685d2be0e382f7378dc9bf8a4e8c2fcfa4ee4b5d8e39caf41951600ab"}, "3": {"node_id": "39d7b088-bda1-4b5e-810d-133b42eeacff", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e1e3600f6544d5d9c39a8cab9b8451dd758cf3517398ea9ec2fed89d8339bb28"}}, "hash": "53cf6471f76ad6671a73e08684e5a6bb35ac5003fdc6d55cd9bf34337ab58d77", "text": "Publishedasa conferencepaperat ICLR2016\nFigure 1: Sequence to sequence learning examples \u2013 (left) machine translation (Sutskeveret al.,\n2014)and( right)constituentparsing(Vinyalset al.,2015a).\nandGermanbyupto+ 1.5BLEUpointsoverstrongsingle-taskbaselinesontheWMTben chmarks.\nFurthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1.\nWealsoexploretwounsupervisedlearningobjectives,sequ enceautoencoders(Dai &Le,2015)and\nskip-thoughtvectors (Kiroset al., 2015), and reveal their interesting propertiesin the MTL setting:\nautoencoderhelpslessintermsofperplexitiesbutmoreonB LEUscorescomparedtoskip-thought.\n2 SEQUENCE TO SEQUENCE LEARNING\nSequencetosequencelearning( seq2seq)aimstodirectlymodeltheconditionalprobability p(y|x)of\nmappingan inputsequence, x1,...,x n, into an outputsequence, y1,...,y m. It accomplishessuch\ngoal through the encoder-decoder framework proposed by Sutskeveretal. (2014) and Cho etal.\n(2014). AsillustratedinFigure1,the encodercomputesarepresentation sforeachinputsequence.\nBasedonthatinputrepresentation,the decodergeneratesanoutputsequence,oneunitatatime,and\nhence,decomposestheconditionalprobabilityas:\nlogp(y|x) =/summationdisplaym\nj=1logp(yj|y<j,x,s) (1)\nAnaturalmodelforsequentialdataistherecurrentneuraln etwork(RNN),whichisusedbymostof\nthe recent seq2seqwork. These work, however,differin termsof: (a) architecture \u2013 fromunidirec-\ntional, to bidirectional, and deep multi-layer RNNs; and (b )RNN type \u2013 which are long-short term\nmemory(LSTM)(Hochreiter& Schmidhuber,1997)andthegate drecurrentunit(Choetal.,2014).\nAnother important difference between seq2seqwork lies in what constitutes the input represen-\ntations. The early seq2seqwork (Sutskeveret al., 2014; Choet al., 2014; Luonget al., 2 015b;\nVinyalset al., 2015b) uses only the last encoder state to ini tialize the decoder and sets s= [ ]\nin Eq. (1). Recently, Bahdanauet al. (2015) proposes an attention mechanism , a way to provide\nseq2seqmodelswitharandomaccessmemory,tohandlelonginputsequ ences. Thisisaccomplished\nbysetting sinEq.(1)tobetheset ofencoderhiddenstatesalreadycompu ted. Onthedecoderside,\nat each time step, the attention mechanism will decide how mu ch informationto retrieve from that\nmemory by learning where to focus, i.e., computing the align ment weights for all input positions.\nRecent work such as (Xuet al., 2015; Jeanet al., 2015a; Luong et al., 2015a; Vinyalsetal., 2015a)\nhasfoundthatit iscrucialtoempower seq2seqmodelswiththe attentionmechanism.\n3 MULTI-TASKSEQUENCE -TO-SEQUENCE LEARNING\nWe generalize the work of Donget al. (2015) to the multi-task sequence-to-sequencelearning set-\nting that includes the tasks of machine translation (MT), co nstituency parsing, and image caption\ngeneration. Dependingwhich tasks involved,we proposeto c ategorize multi-task seq2seqlearning\ninto three general settings. In addition, we will discuss th e unsupervised learning tasks considered\naswell asthelearningprocess.\n3.1 O NE-TO-MANYSETTING\nThis scheme involves one encoder andmultiple decoders for tasks in which the encoder can be\nshared,asillustratedin Figure2. The inputtoeach task isa sequenceofEnglishwords.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "39d7b088-bda1-4b5e-810d-133b42eeacff": {"__data__": {"id_": "39d7b088-bda1-4b5e-810d-133b42eeacff", "embedding": null, "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05696886-7400-41a8-9087-a90319edb626", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4279281685d2be0e382f7378dc9bf8a4e8c2fcfa4ee4b5d8e39caf41951600ab"}, "2": {"node_id": "3473b3cd-4dfe-4f70-a391-44577870b020", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "53cf6471f76ad6671a73e08684e5a6bb35ac5003fdc6d55cd9bf34337ab58d77"}}, "hash": "e1e3600f6544d5d9c39a8cab9b8451dd758cf3517398ea9ec2fed89d8339bb28", "text": "The inputtoeach task isa sequenceofEnglishwords. A separate\ndecoderisusedtogenerateeachsequenceofoutputunitswhi chcanbeeither(a)asequenceoftags\n2", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "59d3092a-68df-4820-83e0-80278ab6acb1": {"__data__": {"id_": "59d3092a-68df-4820-83e0-80278ab6acb1", "embedding": null, "metadata": {"page_label": "3", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52a93de9-980f-425f-a7ed-1c2e53757d68", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fe24ad80dd665e523a020c99730330d3764cb2d059f08858eb148d99db502169"}}, "hash": "fe24ad80dd665e523a020c99730330d3764cb2d059f08858eb148d99db502169", "text": "Publishedasa conferencepaperat ICLR2016\nEnglish (unsupervised)German (translation)\nTags (parsing) English\nFigure 2: One-to-manySetting \u2013 one encoder,multiple decoders. Thisscheme is useful for e ither\nmulti-target translation as in Dongetal. (2015) or between different tasks. Here, English and Ger-\nman imply sequences of words in the respective languages. Th e\u03b1values give the proportions of\nparameterupdatesthatareallocatedforthe differenttask s.\nforconstituencyparsingas usedin (Vinyalset al., 2015a), (b)a sequenceofGermanwordsforma-\nchinetranslation(Luongetal.,2015a),and(c)thesameseq uenceofEnglishwordsforautoencoders\nora relatedsequenceofEnglishwordsfortheskip-thoughto bjective(Kiroset al.,2015).\n3.2 M ANY-TO-ONESETTING\nThisschemeistheoppositeofthe one-to-many setting. AsillustratedinFigure3,itconsistsof mul-\ntipleencoders andonedecoder . Thisisusefulfortasksinwhichonlythedecodercanbeshar ed,for\nexample, when our tasks include machine translation and ima ge caption generation (Vinyalset al.,\n2015b). In addition, from a machine translation perspectiv e, this setting can bene\ufb01t from a large\namount of monolingual data on the target side, which is a stan dard practice in machine translation\nsystemandhasalso beenexploredforneuralMT byGulcehreet al.(2015).\nEnglish (unsupervised)Image (captioning) EnglishGerman (translation)\nFigure3: Many-to-onesetting \u2013multipleencoders,onedecoder. Thisschemeishandyforta sksin\nwhichonlythedecoderscanbeshared.\n3.3 M ANY-TO-MANYSETTING\nLastly,asthenamedescribes,thiscategoryisthemostgene ralone,consistingofmultipleencoders\nand multiple decoders. We will explore this scheme in a trans lation setting that involves sharing\nmultipleencodersandmultipledecoders. Inadditiontothe machinetranslationtask,wewillinclude\ntwounsupervisedobjectivesoverthesourceandtargetlang uagesasillustratedin Figure4.\n3.4 U NSUPERVISED LEARNING TASKS\nOurvery\ufb01rstunsupervisedlearningtaskinvolveslearning autoencoders frommonolingualcorpora,\nwhich has recently been applied to sequence to sequence lear ning (Dai &Le, 2015). However, in\nDai &Le (2015)\u2019s work, the authors only experiment with pret raining and then \ufb01netuning, but not\njoint training which can be viewed as a form of multi-task lea rning (MTL). As such, we are very\ninterestedinknowingwhetherthe sametrendextendsto ourM TLsettings.\nAdditionally,weinvestigatetheuseofthe skip-thought vectors(Kiroset al.,2015) inthecontextof\nourMTL framework. Skip-thoughtvectorsare trained by trai ningsequenceto sequencemodelson\npairs of consecutive sentences, which makes the skip-thoug ht objective a natural seq2seqlearning\ncandidate. A minor technical dif\ufb01culty with skip-thought o bjective is that the training data must\n3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2af8075-0c76-4b88-967e-c7b58e914ea8": {"__data__": {"id_": "a2af8075-0c76-4b88-967e-c7b58e914ea8", "embedding": null, "metadata": {"page_label": "4", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3044d36b-22ab-4bf1-9baa-3218119a8e05", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "addd79111a14fffa81e6190ab24df13a7008952eeff856012d716733ff13fd99"}}, "hash": "addd79111a14fffa81e6190ab24df13a7008952eeff856012d716733ff13fd99", "text": "Publishedasa conferencepaperat ICLR2016\nGerman (translation)\nEnglish (unsupervised) German (unsupervised)English\nFigure4: Many-to-manysetting \u2013 multipleencoders,multipledecoders. We considerthissc heme\nin a limited context of machine translation to utilize the la rge monolingual corpora in both the\nsource and the target languages. Here, we consider a single t ranslation task and two unsupervised\nautoencodertasks.\nconsist of ordered sentences, e.g., paragraphs. Unfortuna tely, in many applications that include\nmachinetranslation,weonlyhavesentence-leveldatawher ethesentencesareunordered. Toaddress\nthat,we spliteachsentenceintotwohalves;we thenuse oneh alftopredicttheotherhalf.\n3.5 L EARNING\nDonget al. (2015) adopted an alternating training approach, where they optimize each task for a\n\ufb01xed number of parameter updates (or mini-batches) before s witching to the next task (which is a\ndifferentlanguagepair). In oursetting, ourtasks are more diverseand containdifferentamountsof\ntraining data. As a result, we allocate different numbersof parameter updates for each task, which\nare expressed with the mixingratio values \u03b1i(for each task i). Each parameter update consists of\ntraining data from one task only. When switching between tas ks, we select randomly a new task i\nwithprobability\u03b1i/summationtext\nj\u03b1j.\nOur conventionis that the \ufb01rst task is the reference task with \u03b11= 1.0and the number of training\nparameterupdatesforthattaskisprespeci\ufb01edtobe N. Atypicaltask iwillthenbetrainedfor\u03b1i\n\u03b11\u00b7N\nparameterupdates. Suchconventionmakesiteasierforusto fairlycomparethesamereferencetask\nina single-tasksetting whichhasalsobeentrainedforexac tlyNparameterupdates.\nWhensharinganencoderoradecoder,weshareboththerecurr entconnectionsandthecorrespond-\ningembeddings.\n4 EXPERIMENTS\nWe evaluate the multi-task learning setup on a wide variety o f sequence-to-sequence tasks: con-\nstituency parsing, image caption generation, machine tran slation, and a number of unsupervised\nlearningassummarizedin Table1.\n4.1 D ATA\nOurexperimentsarecenteredaroundthe translation task,whereweaimtodeterminewhetherother\ntasks can improve translation and vice versa. We use the WMT\u2019 15 data (Bojaret al., 2015) for\nthe English \u21c6German translation problem. Following Luonget al. (2015a) , we use the 50K most\nfrequent words for each language from the training corpus.1These vocabularies are then shared\nwithothertasks,exceptforparsinginwhichthetarget\u201clan guage\u201dhasavocabularyof104tags. We\nuse newstest2013 (3000 sentences) as a validation set to sel ect our hyperparameters, e.g., mixing\ncoef\ufb01cients. For testing, to be comparablewith existing re sultsin (Luonget al., 2015a), we use the\n\ufb01lterednewstest2014(2737sentences)2fortheEnglish \u2192Germantranslationtaskandnewstest2015\n(2169sentences)3fortheGerman \u2192Englishtask. See thesummaryinTable1.\n1The corpus has already beentokenized using the default toke nizer from Moses. Wordsnot inthese vocab-\nularies are represented bythe token <unk>.\n2http://statmt.org/wmt14/test-filtered.tgz\n3http://statmt.org/wmt15/test.tgz\n4", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43602b72-2b7a-40d4-aae1-2bb3109bdbfe": {"__data__": {"id_": "43602b72-2b7a-40d4-aae1-2bb3109bdbfe", "embedding": null, "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10f495a8-e5a3-4331-b4c3-979249d8e5df", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6403d4920176df5ac86c41c8a37918e7d1eb7199308389b4e99319ec502383e1"}, "3": {"node_id": "958108fd-9415-41b9-bba1-cfead2a09248", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "16844d0a21e0ff17f928b3ca20efda4dbe5338c6dcde0e737cf1fba15a7026a3"}}, "hash": "0510e2cabcc392a7acaa9507d877c29908510651c7f1a6aa503006e1a2292035", "text": "Publishedasa conferencepaperat ICLR2016\nTaskTrain Valid Test VocabSize Train Finetune\nSize Size Size Source Target Epoch Start Cycle\nEnglish\u2192GermanTranslation 4.5M 3000 3003 50K 50K 12 8 1\nGerman\u2192EnglishTranslation 4.5M 3000 2169 50K 50K 12 8 1\nEnglishunsupervised 12.1MDetailsintext50K 50K 6 4 0.5\nGermanunsupervised 13.8M 50K 50K 6 4 0.5\nPennTreeBankParsing 40K 1700 2416 50K 104 40 20 4\nHigh-Con\ufb01denceCorpusParsing 11.0M 1700 2416 50K 104 6 4 0.5\nImageCaptioning 596K 4115 - - 50K 10 5 1\nTable 1:Data& TrainingDetails \u2013 Informationaboutthe differentdatasetsused inthis work . For\neach task, we display the following statistics: (a) the numb er of training examples, (b) the sizes of\nthe vocabulary, (c) the number of training epochs, and (d) de tails on when and how frequent we\nhalvethe learningrates( \ufb01netuning ).\nFor theunsupervised tasks, we use the English and German monolingualcorpora fro m WMT\u201915.4\nSince in our experiments, unsupervisedtasks are always cou pled with translation tasks, we use the\nsamevalidationandtest setsastheaccompaniedtranslatio ntasks.\nForconstituencyparsing ,we experimentwithtwotypesofcorpora:\n1. asmall corpus\u2013 thewidelyusedPennTreeBank(PTB)datase t (Marcuset al.,1993)and,\n2. alargecorpus\u2013thehigh-con\ufb01dence(HC)parsetreesprovi dedbyVinyalset al.(2015a).\nThe two parsing tasks, however, are evaluated on the same val idation (section 22) and test (sec-\ntion 23) sets from the PTB data. Note also that the parse trees have been linearized following\nVinyalset al. (2015a). Lastly, for image captiongeneration , we use a dataset of image and caption\npairsprovidedbyVinyalset al. (2015b).\n4.2 T RAINING DETAILS\nInallexperiments,followingSutskeveret al.(2014)andLu ongetal.(2015b),wetraindeepLSTM\nmodelsasfollows: (a)weuse4LSTMlayerseachofwhichhas10 00-dimensionalcellsandembed-\ndings,5(b)parametersareuniformlyinitializedin[-0.06,0.06], (c)weuseamini-batchsize of128,\n(d) dropout is applied with probability of 0.2 over vertical connections (Phamet al., 2014), (e) we\nuse SGD with a \ufb01xed learning rate of 0.7, (f) input sequencesa re reversed, and lastly, (g) we use a\nsimple\ufb01netuningschedule\u2013after xepochs,wehalvethelearningrateevery yepochs. Thevalues x\nandyarereferredas \ufb01netunestart and\ufb01netunecycle inTable1togetherwiththenumberoftraining\nepochspertask.\nAsdescribedinSection3,foreachmulti-taskexperiment,w eneedtochooseonetasktobethe refer-\nencetask (whichcorrespondsto \u03b11= 1). Thechoiceofthereferencetaskhelpsspecifythenumber\nof training epochs and the \ufb01netune start/cycle values which we also when training that reference\ntask alone for fair comparison. To make sure our \ufb01ndings are r eliable, we run each experimental\ncon\ufb01gurationtwiceandreporttheaverageperformancein th eformatmean(stddev) .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "958108fd-9415-41b9-bba1-cfead2a09248": {"__data__": {"id_": "958108fd-9415-41b9-bba1-cfead2a09248", "embedding": null, "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10f495a8-e5a3-4331-b4c3-979249d8e5df", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6403d4920176df5ac86c41c8a37918e7d1eb7199308389b4e99319ec502383e1"}, "2": {"node_id": "43602b72-2b7a-40d4-aae1-2bb3109bdbfe", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0510e2cabcc392a7acaa9507d877c29908510651c7f1a6aa503006e1a2292035"}}, "hash": "16844d0a21e0ff17f928b3ca20efda4dbe5338c6dcde0e737cf1fba15a7026a3", "text": "4.3 R ESULTS\nWe explore several multi-task learning scenarios by combin ing alargetask (machine translation)\nwith: (a) a smalltask \u2013 Penn Tree Bank (PTB) parsing, (b) a medium-sized task \u2013 image caption\ngeneration, (c) another largetask \u2013 parsing on the high-con\ufb01dence (HC) corpus, and (d) las tly,\nunsupervised tasks , such as autoencodersand skip-thoughtvectors. In terms of evaluation metrics,\nwereportbothvalidationandtestperplexitiesforalltask s. Additionally,wealsocomputetestBLEU\nscores(Papineniet al., 2002)forthetranslationtask.\n4The training sizes reported for the unsupervised tasks are o nly 10% of the original WMT\u201915 monolingual\ncorpora which we randomly sample from. Such reduced sizes ar e for faster training time and already about\nthree times largerthan thatof the parallel data. We conside r usingall the monolingual data infuture work.\n5For image caption generation, weuse 1024 dimensions, which isalsothe size of the image embeddings.\n5", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "435d8253-c2ed-4503-8c24-b453b9b93b64": {"__data__": {"id_": "435d8253-c2ed-4503-8c24-b453b9b93b64", "embedding": null, "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59f34a73-617d-4d45-90d0-2dfd09f9a620", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8e7e6575521748d985f2c6c8e7cc3b23c8d71d3caa73186760f664470b3cf1e9"}, "3": {"node_id": "cf26d3da-a8f2-4340-82da-b07e523bed31", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b51dda1cc3a8627f6b51d024022c39709e6443ba627e5180f43e64e7a5a4bb2a"}}, "hash": "a487464b43ed92d7dff2ecf096bdb67e7852fc32c6c48ad69043e78c741a6324", "text": "Publishedasa conferencepaperat ICLR2016\n4.3.1 L ARGETASKS WITH SMALLTASKS\nIn this setting, we want to understand if a small task such as PTB parsing can help improve the\nperformance of a large task such as translation. Since the pa rsing task maps from a sequence of\nEnglish wordsto a sequenceof parsingtags (Vinyalset al., 2 015a), only the encodercan be shared\nwithanEnglish \u2192Germantranslationtask. Asa result,thisisa one-to-many MTLscenario( \u00a73.1).\nTo our surprise, the results in Table 2 suggest that by adding a very small number of parsing mini-\nbatches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-bat ches), we\ncan improvethe translationquality substantially. Moreco ncretely,ourbest multi-task modelyields\na gain of + 1.5BLEU points over the single-task baseline. It is worth point ing out that as shown in\nTable 2, our single-taskbaseline is verystrong,evenbette r thanthe equivalentnon-attentionmodel\nreportedin(Luonget al.,2015a). Largermixingcoef\ufb01cient s,however,over\ufb01tthesmallPTBcorpus;\nhence,achievesmallergainsintranslationquality.\nFor parsing, as Vinyalset al. (2015a) have shown that attent ion is crucial to achieve good parsing\nperformancewhentrainingonthe small PTB corpus,we donots et a highbarforourattention-free\nsystems in this setup (better performancesare reported in S ection 4.3.3). Nevertheless, the parsing\nresultsin Table2indicatethatMTLisalso bene\ufb01cialforpar sing,yieldinganimprovementofupto\n+8.9F1pointsoverthe baseline.6It would be interestingto study howMTL can be usefulwith the\npresenceofthe attention mechanism,whichweleaveforfuturework.\nTaskTranslation Parsing\nValid ppl Test ppl Test BLEU Test F 1\n(Luonget al.,2015a) - 8.1 14.0 -\nOursingle-tasksystems\nTranslation 8.8(0.3) 8.3(0.2) 14.3(0.3) -\nPTBParsing - - - 43.3(1.7)\nOurmulti-tasksystems\nTranslation +PTBParsing(1x) 8.5(0.0) 8.2(0.0) 14.7(0.1) 54.5(0.4)\nTranslation +PTBParsing(0.1x) 8.3(0.1) 7.9(0.0) 15.1(0.0) 55.2(0.0)\nTranslation +PTBParsing(0.01x) 8.2(0.2)7.7(0.2)15.8(0.4) 39.8(2.7)\nTable 2: English\u2192German WMT'14 translation & Penn Tree Bank parsing results \u2013 shown\nare perplexities (ppl), BLEU scores, and parsing F 1for various systems. For muli-task models,\nreference tasks are in italic with the mixing ratio in parentheses. Our results are averaged overtwo\nrunsintheformat mean(stddev) . Best resultsare highlightedinboldface.\n4.3.2 L ARGETASKSWITHMEDIUMTASKS\nWe investigate whether the same pattern carries over to a med ium task such as image caption gen-\neration. Since the image caption generation task maps images to a seq uence of English words\n(Vinyalset al., 2015b; Xu etal., 2015), only the decoder can be shared with a German \u2192English\ntranslationtask. Hence,thissettingfallsunderthe many-to-one MTLsetting( \u00a73.2).\nTheresultsinTable3showthesametrendweobservedbefore, thatis,bytrainingonanothertaskfor\naverysmallfractionoftime,themodelimprovesitsperform anceonitsmaintask.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf26d3da-a8f2-4340-82da-b07e523bed31": {"__data__": {"id_": "cf26d3da-a8f2-4340-82da-b07e523bed31", "embedding": null, "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59f34a73-617d-4d45-90d0-2dfd09f9a620", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8e7e6575521748d985f2c6c8e7cc3b23c8d71d3caa73186760f664470b3cf1e9"}, "2": {"node_id": "435d8253-c2ed-4503-8c24-b453b9b93b64", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a487464b43ed92d7dff2ecf096bdb67e7852fc32c6c48ad69043e78c741a6324"}}, "hash": "b51dda1cc3a8627f6b51d024022c39709e6443ba627e5180f43e64e7a5a4bb2a", "text": "Speci\ufb01cally,with\n5parameterupdatesforimagecaptiongenerationper100upd atesfortranslation(sothemixingratio\nof0.05), we obtain a gain of + 0.7BLEU scores over a strong single-task baseline. Our baselin e is\nalmostaBLEUpointbetterthantheequivalentnon-attentio nmodelreportedinLuongetal.(2015a).\n4.3.3 L ARGETASKS WITH LARGETASKS\nOur \ufb01rst set of experiments is almost the same as the one-to-m any setting in Section 4.3.1 which\ncombines translation ,as thereferencetask, withparsing. Theonlydifferenceis in termsof parsing\n6While perplexities correlate well withBLEUscores as shown in(Luong et al., 2015b), we observe empir-\nically in Section 4.3.3 that parsing perplexities are only r eliable if it is less than 1.3. Hence, we omit parsing\nperplexities in Table 2 for clarity. The parsing test perple xities (averaged over two runs) for the last four rows\ninTable 2are 1.95, 3.05, 2.14, and 1.66. Validperplexities are similar.\n6", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c9e1e885-5520-47fe-8669-54c6025a7277": {"__data__": {"id_": "c9e1e885-5520-47fe-8669-54c6025a7277", "embedding": null, "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31969453-c8d0-4760-a7b4-a32666005be6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6e5230e7a35961d87665496159f766c8adcc35b36535b3df857f0fceb435b69c"}, "3": {"node_id": "7e7b6e0f-c4d1-4f65-99fd-2766d644e910", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "2d91f84d061b578ad8853628ab5523d0828de7f6c5b54379c20a4686e76ae14a"}}, "hash": "d2310e15f44c81de10cd0e9dc717023853c3102d8f5cb2bb4375777830266d62", "text": "Publishedasa conferencepaperat ICLR2016\nTaskTranslation Captioning\nValidppl Test ppl Test BLEU Validppl\n(Luonget al.,2015a) - 14.3 16.9 -\nOursingle-tasksystems\nTranslation 11.0(0.0) 12.5(0.2) 17.8(0.1) -\nCaptioning - - - 30.8(1.3)\nOurmulti-tasksystems\nTranslation + Captioning(1x) 11.9 14.0 16.7 43.3\nTranslation + Captioning(0.1x) 10.5(0.4) 12.1(0.4) 18.0(0.6) 28.4(0.3)\nTranslation + Captioning(0.05x) 10.3(0.1)11.8(0.0)18.5(0.0) 30.1(0.3)\nTranslation + Captioning(0.01x) 10.6(0.0) 12.3(0.1) 18.1(0.4) 35.2(1 .4)\nTable 3:German\u2192English WMT'15 translation & captioning results \u2013 shown are perplexities\n(ppl) and BLEU scores for various tasks with similar format a s in Table 2. Reference tasks are in\nitalic withmixingratiosin parentheses. Theaverageresul tsof2runsarein mean(stddev) format.\ndata. Instead of using the small Penn Tree Bank corpus, we con sider a large parsing resource, the\nhigh-con\ufb01dence(HC)corpus,whichisprovidedbyVinyalset al.(2015a). AshighlightedinTable4,\nthetrendisconsistent;MTLhelpsboosttranslationqualit ybyupto+ 0.9BLEUpoints.\nTaskTranslation\nValidppl Test ppl Test BLEU\n(Luonget al.,2015a) - 8.1 14.0\nOursystems\nTranslation 8.8(0.3) 8.3(0.2) 14.3(0.3)\nTranslation + HCParsing(1x) 8.5(0.0) 8.1(0.1) 15.0(0.6)\nTranslation + HCParsing(0.1x) 8.2(0.3)7.7(0.2)15.2(0.6)\nTranslation + HCParsing(0.05x) 8.4(0.0) 8.0(0.1) 14.8(0.2)\nTable4:English\u2192GermanWMT'14translation \u2013shownareperplexities(ppl)andBLEUscores\nof varioustranslationmodels. Our multi-tasksystems comb inetranslationand parsingon the high-\ncon\ufb01dencecorpustogether. Mixingratiosare in parenthese sandthe averageresultsover2runsare\ninmean(stddev) format. Best resultsarebolded.\nThe second set of experimentsshifts the attention to parsingby having it as the referencetask. We\nshow in Table 5 results that combine parsing with either (a) t he English autoencoder task or (b)\nthe English \u2192German translation task. Our models are compared against th e best attention-based\nsystemsin(Vinyalset al.,2015a),includingthestate-of- the-artresultof92.8F 1.\nBefore discussing the multi-task results, we note a few inte resting observations. First, very small\nparsing perplexities, close to 1.1, can be achieved with lar ge training data.7Second, our baseline\nsystemcanobtainaverycompetitiveF 1scoreof92.2,rivalingVinyalset al.(2015a)\u2019ssystems. Th is\nis rather surprising since our models do not use any attentio n mechanism. A closer look into these\nmodels reveal that there seems to be an architectural differ ence: Vinyalset al. (2015a) use 3-layer\nLSTMwith256cellsand512-dimensionalembeddings;wherea sourmodelsuse4-layerLSTMwith\n1000 cells and 1000-dimensionalembeddings.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e7b6e0f-c4d1-4f65-99fd-2766d644e910": {"__data__": {"id_": "7e7b6e0f-c4d1-4f65-99fd-2766d644e910", "embedding": null, "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31969453-c8d0-4760-a7b4-a32666005be6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6e5230e7a35961d87665496159f766c8adcc35b36535b3df857f0fceb435b69c"}, "2": {"node_id": "c9e1e885-5520-47fe-8669-54c6025a7277", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d2310e15f44c81de10cd0e9dc717023853c3102d8f5cb2bb4375777830266d62"}}, "hash": "2d91f84d061b578ad8853628ab5523d0828de7f6c5b54379c20a4686e76ae14a", "text": "This further s upports \ufb01ndings in (Jozefowiczet al.,\n2016)that largernetworksmatterforsequencemodels.\nFor the multi-task results, while autoencoder does not seem to help parsing, translation does. At\nthe mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F1over the baseline and with\n92.4 F 1, our multi-task system is on par with the best single system r eported in (Vinyalset al.,\n2015a). Furthermore,byensembling6differentmulti-task models(trainedwiththetranslationtask\nat mixing ratios of 0.1, 0.05, and 0.01), we are able to establ ish a new state-of-the-art result in\nEnglishconstituentparsingwith 93.0F1score.\n7Training solely on the small Penn Tree Bank corpus can only re duce the perplexity to at most 1.6, as\nevidencedbypoorparsingresultsinTable2. Atthesametime ,theseparsingperplexitiesaremuchsmallerthan\nwhat can be achieved by a translation task. This is because pa rsing only has 104tags in the target vocabulary\ncompared to 50K words inthe translationcase. Note that 1.0isthe theoretical lowerbound.\n7", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc5aeeb3-beff-4583-862b-a1501e6c7846": {"__data__": {"id_": "dc5aeeb3-beff-4583-862b-a1501e6c7846", "embedding": null, "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b20e5506-61f6-4801-ad12-da7da6debc74", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "aee16b3fbc041923d44a01a3f857f0a161162eb6e220b0277efb836ae2cbf2cf"}, "3": {"node_id": "64e50b3e-18cf-49d8-8f54-b7046d63fae2", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b83fdd5eeabd3cba94ce503fd32b52047ab3c057b7625d77ec7d433afa50650f"}}, "hash": "27b221628b4e7bf288ea45adf90e9a27e7a045f27bd174fd8f33a4d5cfddf92a", "text": "Publishedasa conferencepaperat ICLR2016\nTaskParsing\nValidppl Test F 1\nLSTM+A(Vinyalset al.,2015a) - 92.5\nLSTM+A+E(Vinyalsetal., 2015a) - 92.8\nOursystems\nHCParsing 1.12/1.12 92.2(0.1)\nHCParsing + Autoencoder(1x) 1.12/1.12 92.1(0.1)\nHCParsing + Autoencoder(0.1x) 1.12/1.12 92.1(0.1)\nHCParsing + Autoencoder(0.01x) 1.12/1.13 92.0(0.1)\nHCParsing + Translation(1x) 1.12/1.13 91.5(0.2)\nHCParsing + Translation(0.1x) 1.13/1.13 92.0(0.2)\nHCParsing + Translation(0.05x) 1.11/1.12 92.4(0.1)\nHCParsing + Translation(0.01x) 1.12/1.12 92.2(0.0)\nEnsembleof6multi-tasksystems - 93.0\nTable 5: Large-Corpus parsing results \u2013 shown are perplexities (ppl) and F 1scores for various\nparsing models. Mixing ratios are in parentheses and the ave rage results over 2 runs are in mean\n(stddev)format. We show the individual perplexities for all runs due to small differences among\nthem. For Vinyalset al. (2015a)\u2019s parsing results, LSTM+A r epresents a single LSTM with atten-\ntion,whereasLSTM+A+Eindicatesanensembleof5systems. I mportantresultsarebolded.\n4.3.4 M ULTI-TASKS AND UNSUPERVISED LEARNING\nOur main focus in this section is to determine whether unsupe rvised learning can help improve\ntranslation. Speci\ufb01cally, we follow the many-to-many approach described in Section 3.3 to couple\ntheGerman \u2192Englishtranslationtaskwithtwounsupervisedlearningta sksonmonolingualcorpora,\none per language. The results in Tables 6 show a similar trend as before, a small amount of other\ntasks, in this case the autoencoder objective with mixing coef\ufb01cient 0.05, improvesthe transl ation\nquality by + 0.5BLEU scores. However, as we train more on the autoencodertas k, i.e. with larger\nmixingratios,thetranslationperformancegetsworse.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64e50b3e-18cf-49d8-8f54-b7046d63fae2": {"__data__": {"id_": "64e50b3e-18cf-49d8-8f54-b7046d63fae2", "embedding": null, "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b20e5506-61f6-4801-ad12-da7da6debc74", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "aee16b3fbc041923d44a01a3f857f0a161162eb6e220b0277efb836ae2cbf2cf"}, "2": {"node_id": "dc5aeeb3-beff-4583-862b-a1501e6c7846", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "27b221628b4e7bf288ea45adf90e9a27e7a045f27bd174fd8f33a4d5cfddf92a"}}, "hash": "b83fdd5eeabd3cba94ce503fd32b52047ab3c057b7625d77ec7d433afa50650f", "text": "with larger\nmixingratios,thetranslationperformancegetsworse.\nTaskTranslation German English\nValidppl Test ppl Test BLEU Test ppl Test ppl\n(Luongetal., 2015a) - 14.3 16.9 - -\nOursingle-tasksystems\nTranslation 11.0(0.0) 12.5(0.2) 17.8(0.1) - -\nOurmulti-tasksystemswith Autoencoders\nTranslation + autoencoders(1.0x) 12.3 13.9 16.0 1.01 2.10\nTranslation + autoencoders(0.1x) 11.4 12.7 17.7 1.13 1.44\nTranslation + autoencoders(0.05x) 10.9(0.1)12.0(0.0)18.3(0.4) 1.40(0.01) 2.38(0.39)\nOurmulti-tasksystems with Skip-thoughtVectors\nTranslation + skip-thought(1x) 10.4(0.1)10.8(0.1) 17.3(0.2) 36.9(0.1)31.5(0.4)\nTranslation + skip-thought(0.1x) 10.7(0.0) 11.4(0.2) 17.8(0.4) 52.8( 0.3) 53.7(0.4)\nTranslation + skip-thought(0.01x) 11.0(0.1) 12.2(0.0) 17.8(0.3) 76.3(0.8) 142.4(2.7)\nTable 6:German\u2192English WMT'15 translation & unsupervised learning result s\u2013 shown are\nperplexitiesfortranslationandunsupervisedlearningta sks. We experimentwithboth autoencoders\nandskip-thoughtvectors for the unsupervisedobjectives. Numbersin mean (stddev) format are the\naverageresultsof2 runs;othersarefor1runonly.\nSkip-thought objectives, on the other hand, behave differently. If we mer ely look at the perplexity\nmetric,theresultsareveryencouraging:withmoreskip-th oughtdata,weperformbetterconsistently\nacrossboththetranslationandtheunsupervisedtasks. How ever,whencomputingtheBLEUscores,\nthe translation quality degrades as we increase the mixing c oef\ufb01cients. We anticipate that this is\ndue to the fact that the skip-thoughtobjectivechangesthe n atureof the translationtask when using\none half of a sentence to predict the other half. It is not a pro blem for the autoencoder objectives,\nhowever,sinceonecanthinkofautoencodinga sentenceastr anslatingintothe samelanguage.\n8", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c3e8d662-0db6-4b5f-908e-d038e72c6ad7": {"__data__": {"id_": "c3e8d662-0db6-4b5f-908e-d038e72c6ad7", "embedding": null, "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c61d074-41bb-4a7c-9faf-f35d94f86ca3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fe15a8433c9c7acda8f4ac9d1cadac4321d3d5352403f6ee4fb8c5cd277e3548"}, "3": {"node_id": "3af45a22-dfbd-44c2-b71b-bc58ea17228a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ccd2457ec1c957654527d235d1623d58e0c2d104e49b676b89ff98f59f150bb8"}}, "hash": "c35ab92da6e70c33c477e56a9b61217ba751597e034c5945855326c9084efc22", "text": "Publishedasa conferencepaperat ICLR2016\nWebelievethese\ufb01ndingsposeinterestingchallengesinthe questtowardsbetterunsupervisedobjec-\ntives,whichshouldsatisfythefollowingcriteria: (a)ade sirableobjectiveshouldbecompatiblewith\nthesupervisedtaskinfocus,e.g.,autoencoderscanbeview edasaspecialcaseoftranslation,and(b)\nwith moreunsuperviseddata,bothintrinsic andextrinsicm etricsshouldbe improved;skip-thought\nobjectivessatisfy thiscriterionin termsoftheintrinsic metricbutnottheextrinsicone.\n5 CONCLUSION\nIn this paper, we showed that multi-task learning (MTL) can i mprove the performance of the\nattention-free sequence to sequence model of (Sutskeveret al., 2014). We found it surprising that\ntraining on syntactic parsing and image caption data improv ed our translation performance, given\nthatthesedatasetsareordersofmagnitudesmallerthantyp icaltranslationdatasets. Furthermore,we\nhave established a new state-of-the-art result in constituent parsing with an ensemble of multi-tas k\nmodels. Wealsoshowthatthetwounsupervisedlearningobje ctives,autoencoderandskip-thought,\nbehavedifferentlyintheMTLcontextinvolvingtranslatio n. We hopethattheseinteresting\ufb01ndings\nwill motivate future work in utilizing unsupervised data fo r sequence to sequence learning. A crit-\nicism of our work is that our sequence to sequence modelsdo no t employ the attention mechanism\n(Bahdanauet al.,2015). We leavetheexplorationofMTLwith attentionforfuturework.\nACKNOWLEDGMENTS\nWe thankChrisManningforhelpfulfeedbackonthe paperandm embersofthe GoogleBrain team\nforthoughtfuldiscussionsandinsights.\nREFERENCES\nAndo, Rie Kubota and Zhang, Tong. A frameworkfor learning pr edictive structures from multiple\ntasksandunlabeleddata. JMLR,6:1817\u20131853,2005.\nArgyriou,Andreas,Evgeniou,Theodoros,andPontil,Massi miliano. Multi-taskfeaturelearning. In\nNIPS,2007.\nBahdanau, Dzmitry, Cho, Kyunghyun,and Bengio, Yoshua. Neu ral machine translation by jointly\nlearningtoalignandtranslate. In ICLR, 2015.\nBojar, Ond\u02c7 rej, Chatterjee, Rajen, Federmann, Christian, Haddow, Barry, Huck, Matthias, Hokamp,\nChris, Koehn, Philipp, Logacheva, Varvara, Monz, Christof , Negri, Matteo, Post, Matt, Scarton,\nCarolina,Specia,Lucia,andTurchi,Marco. Findingsofthe 2015workshoponstatisticalmachine\ntranslation. In WMT, 2015.\nCaruana,Rich. Multitasklearning. MachineLearning ,28(1):41\u201375,1997.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, B ougares, Fethi, Schwenk, Holger,\nand Bengio, Yoshua. Learningphrase representationsusing RNN encoder-decoderfor statistical\nmachinetranslation. In EMNLP, 2014.\nDai,AndrewM. andLe, QuocV. Semi-supervisedsequencelear ning. InNIPS,2015.\nDonahue,Jeff,Jia,Yangqing,Vinyals,Oriol,Hoffman,Jud y,Zhang,Ning,Tzeng,Eric,andDarrell,\nTrevor. DeCAF: A deepconvolutionalactivationfeaturefor genericvisualrecognition,2014.\nDong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haife ng. Multi-task learning for\nmultiplelanguagetranslation. In ACL,2015.\nEvgeniou, Theodoros and Pontil, Massimiliano. Regularize d multi\u2013task learning.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3af45a22-dfbd-44c2-b71b-bc58ea17228a": {"__data__": {"id_": "3af45a22-dfbd-44c2-b71b-bc58ea17228a", "embedding": null, "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c61d074-41bb-4a7c-9faf-f35d94f86ca3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fe15a8433c9c7acda8f4ac9d1cadac4321d3d5352403f6ee4fb8c5cd277e3548"}, "2": {"node_id": "c3e8d662-0db6-4b5f-908e-d038e72c6ad7", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c35ab92da6e70c33c477e56a9b61217ba751597e034c5945855326c9084efc22"}}, "hash": "ccd2457ec1c957654527d235d1623d58e0c2d104e49b676b89ff98f59f150bb8", "text": "Regularize d multi\u2013task learning. In SIGKDD,\n2004.\nGulcehre, Caglar, Firat, Orhan, Xu, Kelvin, Cho, Kyunghyun , Barrault, Loic, Lin, Huei-Chi,\nBougares,Fethi,Schwenk,Holger,andBengio,Yoshua. Onus ingmonolingualcorporainneural\nmachinetranslation. arXivpreprintarXiv:1503.03535 ,2015.\n9", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2b6806b5-e7b8-4f53-bbb8-04ce601964d2": {"__data__": {"id_": "2b6806b5-e7b8-4f53-bbb8-04ce601964d2", "embedding": null, "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27edb7b6-3c71-4d30-8b51-f392be1b0b1c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "64c700bea1b4d210892c83e9427a899d432787effd7b19dc4d72d336e994e8fa"}, "3": {"node_id": "ed6b8ae4-7744-4674-870f-6086bfa05304", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6be6a0499f074c462150722ec2a3d993c1e9fa92d69dd3424563aafd63e95f4c"}}, "hash": "7c1fceffe09bab98df4fcaa7ae276d2d64ddc5d4cf8efa91ab528a3033d96d86", "text": "Publishedasa conferencepaperat ICLR2016\nHeigold,Georg,Vanhoucke,Vincent,Senior,Alan,Nguyen, Patrick,Ranzato,Marc\u2019Aurelio,Devin,\nMatthieu,andDean,Jeffrey.Multilingualacousticmodels usingdistributeddeepneuralnetworks.\nInICASSP,2013.\nHochreiter, Sepp and Schmidhuber, J\u00a8 urgen. Long short-ter m memory. Neural Computation , 9(8):\n1735\u20131780,1997.\nHuang, Jui-Ting, Li, Jinyu, Yu, Dong, Deng, Li, and Gong, Yif an. Cross-language knowledge\ntransferusingmultilingualdeepneuralnetworkwith share dhiddenlayers. In ICASSP,2013.\nJean, S\u00b4 ebastien, Cho, Kyunghyun, Memisevic, Roland, and B engio, Yoshua. On using very large\ntargetvocabularyforneuralmachinetranslation. In ACL,2015a.\nJean,S\u00b4 ebastien,Firat,Orhan,Cho,Kyunghyun,Memisevic ,Roland,andBengio,Yoshua. Montreal\nneuralmachinetranslationsystemsforWMT\u201915. In WMT, 2015b.\nJozefowicz,R., Vinyals,O.,Schuster,M.,Shazeer,N.,and Wu,Y. Exploringthelimitsoflanguage\nmodeling. arXiv preprintarXiv:1602.02410 ,2016.\nKalchbrenner,Nal andBlunsom,Phil. Recurrentcontinuous translationmodels. In EMNLP, 2013.\nKiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Ric hard S., Torralba, Antonio, Urtasun,\nRaquel,andFidler,Sanja. Skip-thoughtvectors. In NIPS,2015.\nKumar, Abhishek and III, Hal Daum\u00b4 e. Learning task grouping and overlap in multi-task learning.\nInICML, 2012.\nLiu, Xiaodong, Gao, Jianfeng, He, Xiaodong, Deng, Li, Duh, K evin, and Wang, Ye-Yi. Represen-\ntationlearningusingmulti-taskdeepneuralnetworksfors emanticclassi\ufb01cation andinformation\nretrieval. In NAACL,2015.\nLuong,Minh-ThangandManning,ChristopherD. Stanfordneu ralmachinetranslationsystemsfor\nspokenlanguagedomain. In IWSLT, 2015.\nLuong,Minh-Thang,Pham, Hieu, and Manning,Christopher D. Effectiveapproachesto attention-\nbasedneuralmachinetranslation. In EMNLP, 2015a.\nLuong, Minh-Thang, Sutskever, Ilya, Le, Quoc V., Vinyals, O riol, and Zaremba, Wojciech. Ad-\ndressingthe rarewordproblemin neuralmachinetranslatio n. InACL,2015b.\nMarcus,MitchellP.,Marcinkiewicz,MaryAnn,andSantorin i,Beatrice. Buildingalargeannotated\ncorpusofenglish: Thepenntreebank. ComputationalLinguistics ,19(2):313\u2013330,1993.\nPapineni, Kishore, Roukos, Salim, Ward, Todd, and jing Zhu, Wei. Bleu: a method for automatic\nevaluationofmachinetranslation. In ACL,2002.\nPham,Vu,Bluche,Th\u00b4 eodore,Kermorvant,Christopher,and Louradour,J\u00b4 er\u02c6 ome. Dropoutimproves\nrecurrent neural networks for handwriting recognition. In Frontiers in Handwriting Recognition\n(ICFHR),201414thInternationalConferenceon ,pp.285\u2013290.IEEE,2014.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-\nworks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ed6b8ae4-7744-4674-870f-6086bfa05304": {"__data__": {"id_": "ed6b8ae4-7744-4674-870f-6086bfa05304", "embedding": null, "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27edb7b6-3c71-4d30-8b51-f392be1b0b1c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "64c700bea1b4d210892c83e9427a899d432787effd7b19dc4d72d336e994e8fa"}, "2": {"node_id": "2b6806b5-e7b8-4f53-bbb8-04ce601964d2", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7c1fceffe09bab98df4fcaa7ae276d2d64ddc5d4cf8efa91ab528a3033d96d86"}}, "hash": "6be6a0499f074c462150722ec2a3d993c1e9fa92d69dd3424563aafd63e95f4c", "text": "In NIPS,2014.\nThrun,Sebastian. Islearningthen-ththinganyeasier than learningthe\ufb01rst? In NIPS,1996.\nVinyals, Oriol, Kaiser, Lukasz, Koo, Terry, Petrov, Slav, S utskever, Ilya, and Hinton, Geoffrey.\nGrammarasaforeignlanguage. In NIPS,2015a.\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan , Dumitru. Show and tell: A neural\nimagecaptiongenerator. In CVPR,2015b.\nXu,Kelvin,Ba,Jimmy,Kiros,Ryan,Cho,Kyunghyun,Courvil le,AaronC.,Salakhutdinov,Ruslan,\nZemel, Richard S., and Bengio, Yoshua. Show, attend and tell : Neural image caption generation\nwithvisualattention. In ICML,2015.\n10", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fc892e84-0da4-4245-840a-852069ad8181": {"__data__": {"id_": "fc892e84-0da4-4245-840a-852069ad8181", "embedding": null, "metadata": {"page_label": "1", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6518de0d-dda4-4d06-b56a-ad870a44f66b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d432fe866ecfdb4a9409c341035177b793f1635f11160e9d08ae45dfb7e76731"}}, "hash": "d432fe866ecfdb4a9409c341035177b793f1635f11160e9d08ae45dfb7e76731", "text": "arXiv:1608.05859v3  [cs.CL]  21 Feb 2017Using the Output Embedding to Improve Language Models\nO\ufb01r Press and Lior Wolf\nSchool of Computer Science\nTel-Aviv University, Israel\n{ofir.press,wolf }@cs.tau.ac.il\nAbstract\nWe study the topmost weight matrix of\nneural network language models. We\nshow that this matrix constitutes a valid\nword embedding. When training language\nmodels, we recommend tying the input\nembedding and this output embedding.\nWe analyze the resulting update rules and\nshow that the tied embedding evolves in\na more similar way to the output embed-\nding than to the input embedding in the\nuntied model. We also offer a new method\nof regularizing the output embedding. Our\nmethods lead to a signi\ufb01cant reduction in\nperplexity, as we are able to show on a va-\nriety of neural network language models.\nFinally, we show that weight tying can re-\nduce the size of neural translation models\nto less than half of their original size with-\nout harming their performance.\n1 Introduction\nIn a common family of neural network language\nmodels, the current input word is represented as\nthe vector c\u2208IRCand is projected to a dense\nrepresentation using a word embedding matrix U.\nSome computation is then performed on the word\nembedding U\u22a4c, which results in a vector of ac-\ntivationsh2. A second matrix Vthen projects h2\nto a vector h3containing one score per vocabulary\nword:h3=Vh2. The vector of scores is then con-\nverted to a vector of probability values p, which\nrepresents the models\u2019 prediction of the next word,\nusing the softmax function.\nFor example, in the LSTM-based lan-\nguage models of (Sundermeyer et al., 2012;\nZaremba et al., 2014), for vocabulary of size C,\nthe one-hot encoding is used to represent the input\ncandU\u2208IRC\u00d7H. An LSTM is then employed,which results in an activation vector h2that\nsimilarly to U\u22a4c, is also in IRH. In this case, U\nandVare of exactly the same size.\nWe callUthe input embedding, and Vthe\noutput embedding. In both matrices, we expect\nrows that correspond to similar words to be sim-\nilar: for the input embedding, we would like the\nnetwork to react similarly to synonyms, while in\nthe output embedding, we would like the scores\nof words that are interchangeable to be simi-\nlar (Mnih and Teh, 2012).\nWhileUandVcan both serve as word embed-\ndings, in the literature, only the former serves this\nrole. In this paper, we compare the quality of the\ninput embedding to that of the output embedding,\nand we show that the latter can be used to improve\nneural network language models. Our main results\nare as follows: (i) We show that in the word2vec\nskip-gram model, the output embedding is only\nslightly inferior to the input embedding. This is\nshown using metrics that are commonly used in or-\nder to measure embedding quality. (ii) In recurrent\nneural network based language models, the output\nembedding outperforms the input embedding. (iii)\nBy tying the two embeddings together, i.e., enforc-\ningU=V, the joint embedding evolves in a more\nsimilar way to the output embedding than to the in-\nput embedding of the untied model. (iv) Tying the\ninput and output embeddings leads to an improve-\nment in the perplexity of various language mod-\nels. This is true both when using dropout or when\nnot using it. (v) When not using dropout, we pro-\npose adding an additional projection PbeforeV,\nand apply regularization to P. (vi) Weight tying\nin neural translation models can reduce their size\n(number of parameters) to less than half of their\noriginal size without harming their performance.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "62529978-b833-457c-831b-8094e4a10a93": {"__data__": {"id_": "62529978-b833-457c-831b-8094e4a10a93", "embedding": null, "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4df198f5-f606-4714-a3c2-cffd8c295899", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "32fb84c78494863ea07d56e44137dae8c22c4e8da6aa0df2fd12ca7fd35c3d06"}, "3": {"node_id": "32b02e64-4749-4af8-abde-96b4267d05f4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9852dcad0030a83b053b08dc3fd8593e21f1cbc1f3626dcfacfc9d3f78b65494"}}, "hash": "80a3bfdeab4069d22b3605e868bac1dc942617cfaf6e7c1474b1ae8da927c09e", "text": "2 Related Work\nNeural network language models (NNLMs)\nassign probabilities to word sequences. Their\nresurgence was initiated by (Bengio et al., 2003).\nRecurrent neural networks were \ufb01rst used for\nlanguage modeling in (Mikolov et al., 2010)\nand (Pascanu et al., 2013). The \ufb01rst model\nthat implemented language modeling with\nLSTMs (Hochreiter and Schmidhuber, 1997)\nwas (Sundermeyer et al., 2012). Follow-\ning that, (Zaremba et al., 2014) introduced\na dropout (Srivastava, 2013) augmented\nNNLM. (Gal, 2015; Gal and Ghahramani, 2016)\nproposed a new dropout method, which is referred\nto as Bayesian Dropout below, that improves on\nthe results of (Zaremba et al., 2014).\nThe skip-gram word2vec model introduced\nin (Mikolov et al., 2013a; Mikolov et al., 2013b)\nlearns representations of words. This model learns\na representation for each word in its vocabulary,\nboth in an input embedding matrix and in an\noutput embedding matrix. When training is\ncomplete, the vectors that are returned are the\ninput embeddings. The output embedding is\ntypically ignored, although (Mitra et al., 2016;\nMnih and Kavukcuoglu, 2013) use both the\noutput and input embeddings of words\nin order to compute word similarity. Re-\ncently, (Goldberg and Levy, 2014) argued that\nthe output embedding of the word2vec skip-\ngram model needs to be different than the input\nembedding.\nAs we show, tying the input and the output em-\nbeddings is indeed detrimental in word2vec. How-\never, it improves performance in NNLMs.\nIn neural machine translation (NMT)\nmodels (Kalchbrenner and Blunsom, 2013;\nCho et al., 2014; Sutskever et al., 2014;\nBahdanau et al., 2014), the decoder, which\ngenerates the translation of the input sentence in\nthe target language, is a language model that is\nconditioned on both the previous words of the out-\nput sentence and on the source sentence. State of\nthe art results in NMT have recently been achieved\nby systems that segment the source and target\nwords into subword units (Sennrich et al., 2016a).\nOne such method (Sennrich et al., 2016b) is based\non the byte pair encoding (BPE) compression al-\ngorithm (Gage, 1994). BPE segments rare words\ninto their more commonly appearing subwords.\nWeight tying was previously used in the log-bilinear model of (Mnih and Hinton, 2009), but\nthe decision to use it was not explained, and\nits effect on the model\u2019s performance was not\ntested. Independently and concurrently with\nour work (Inan et al., 2016) presented an ex-\nplanation for weight tying in NNLMs based\non (Hinton et al., 2015).\n3 Weight Tying\nIn this work, we employ three different model cat-\negories: NNLMs, the word2vec skip-gram model,\nand NMT models. Weight tying is applied sim-\nilarly in all models. For translation models, we\nalso present a three-way weight tying method.\nNNLM models contain an input embedding ma-\ntrix, two LSTM layers ( h1andh2), a third hidden\nscores/logits layer h3, and a softmax layer. The\nloss used during training is the cross entropy loss\nwithout any regularization terms.\nFollowing (Zaremba et al., 2014), we employ\ntwo models: large and small. The large model em-\nploys dropout for regularization. The small model\nis not regularized. Therefore, we propose the fol-\nlowing regularization scheme. A projection matrix\nP\u2208IRH\u00d7His inserted before the output embed-\nding, i.e., h3=VPh2. The regularizing term\n\u03bb/bardblP/bardbl2is then added to the small model\u2019s loss\nfunction. In all of our experiments, \u03bb= 0.15.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32b02e64-4749-4af8-abde-96b4267d05f4": {"__data__": {"id_": "32b02e64-4749-4af8-abde-96b4267d05f4", "embedding": null, "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4df198f5-f606-4714-a3c2-cffd8c295899", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "32fb84c78494863ea07d56e44137dae8c22c4e8da6aa0df2fd12ca7fd35c3d06"}, "2": {"node_id": "62529978-b833-457c-831b-8094e4a10a93", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "80a3bfdeab4069d22b3605e868bac1dc942617cfaf6e7c1474b1ae8da927c09e"}}, "hash": "9852dcad0030a83b053b08dc3fd8593e21f1cbc1f3626dcfacfc9d3f78b65494", "text": "In all of our experiments, \u03bb= 0.15.\nProjection regularization allows us to use the\nsame embedding (as both the input/output embed-\nding) with some adaptation that is under regular-\nization. It is, therefore, especially suited for WT.\nWhile training a vanilla untied NNLM , at\ntimestep t, with current input word sequence\ni1:t= [i1,...,it]and current target output word\not, the negative log likelihood loss is given by:\nLt=\u2212logpt(ot|i1:t), where pt(ot|i1:t) =\nexp(V\u22a4\noth(t)\n2)\n/summationtextC\nx=1exp(V\u22a4xh(t)\n2),Uk(Vk) is thekth row of U(V),\nwhich corresponds to word k, andh(t)\n2is the vector\nof activations of the topmost LSTM layer\u2019s output\nat timet. For simplicity, we assume that at each\ntimestept,it/ne}ationslash=ot. Optimization of the model is\nperformed using stochastic gradient descent.\nThe update for row kof the input embedding is:\n\u2202Lt\n\u2202Uk=/braceleftBigg\n(/summationtextC\nx=1pt(x|i1:t)\u00b7V\u22a4\nx\u2212V\u22a4\not)\u2202h(t)\n2\n\u2202Uitk=it\n0 k/negationslash=it\nFor the output embedding, row k\u2019s update is:\n\u2202Lt\n\u2202Vk=/braceleftBigg\n(pt(ot|i1:t)\u22121)h(t)\n2k=ot\npt(k|i1:t)\u00b7h(t)\n2 k/negationslash=ot", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec2e768e-4b09-43d4-bc97-6419d2ebd26f": {"__data__": {"id_": "ec2e768e-4b09-43d4-bc97-6419d2ebd26f", "embedding": null, "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6633cb98-b968-447b-969d-9ff214531f79", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "3db1835b3e7b72675eba1627e04bb536792f0ecd37e2c816d6fb96fd4433593e"}, "3": {"node_id": "8a7d85af-eafb-4b73-a496-754080e1c711", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b6d51ab0500d40ffdf6f602637ef92ff75b91e9f0222eee05bb667d10840e792"}}, "hash": "743bd9bb51ee1422231c1a2455975cdb4d3fe1ecd84c77b00b845763f44e47c1", "text": "Therefore, in the untied model, at every timestep,\nthe only row that is updated in the input embed-\nding is the row Uitrepresenting the current input\nword. This means that vectors representing rare\nwords are updated only a small number of times.\nThe output embedding updates every row at each\ntimestep.\nIntied NNLMs , we setU=V=S. The\nupdate for each row in Sis the sum of the updates\nobtained for the two roles of S as both an input and\noutput embedding.\nThe update for row k/ne}ationslash=itis similar to the up-\ndate of row kin the untied NNLM\u2019s output embed-\nding (the only difference being that U and V are\nboth replaced by a single matrix S). In this case,\nthere is no update from the input embedding role\nofS.\nThe update for row k=it, is made up of a term\nfrom the input embedding (case k=it) and a term\nfrom the output embedding (case k/ne}ationslash=ot). The\nsecond term grows linearly with pt(it|i1:t), which\nis expected to be close to zero, since words sel-\ndom appear twice in a row (the low probability\nin the network was also veri\ufb01ed experimentally).\nThe update that occurs in this case is, therefore,\nmostly impacted by the update from the input em-\nbedding role of S.\nTo conclude, in the tied NNLM, every row of S\nis updated during each iteration, and for all rows\nexcept one, this update is similar to the update of\nthe output embedding of the untied model. This\nimplies a greater degree of similarity of the tied\nembedding to the untied model\u2019s output embed-\nding than to its input embedding.\nThe analysis above focuses on NNLMs for\nbrevity. In word2vec , the update rules are simi-\nlar, just that h(t)\n2is replaced by the identity func-\ntion. As argued by (Goldberg and Levy, 2014), in\nthis case weight tying is not appropriate, because\nifpt(it|i1:t)is close to zero then so is the norm\nof the embedding of it. This argument does not\nhold for NNLMs, since the LSTM layers cause a\ndecoupling of the input and output embedddings.\nFinally, we evaluate the effect of weight ty-\ning in neural translation models . In this model:\npt(ot|i1:t,r) =exp(V\u22a4\notG(t))\n/summationtextCt\nx=1exp(V\u22a4xG(t))wherer=\n(r1,...,rN)is the set of words in the source sen-\ntence,UandVare the input and output embed-\ndings of the decoder and Wis the input embed-\nding of the encoder (in translation models U,V\u2208\nIRCt\u00d7HandW\u2208IRCs\u00d7H, whereCs/Ctis theLanguage Subwords Subwords Subwords\npairs only in source only in target in both\nEN\u2192FR 2K 7K 85K\nEN\u2192DE 3K 11K 80K\nTable 1: Shared BPE subwords between pairs of languages.\nsize of the vocabulary of the source / target). G(t)\nis the decoder, which receives the context vector,\nthe embedding of the input word ( it) inU, and its\nprevious state at each timestep. ctis the context\nvector at timestep t,ct=/summationtext\nj\u2208ratjhj, whereatj\nis the weight given to the jth annotation at time t:\natj=exp(etj)/summationtext\nk\u2208rexp(eik), andetj=at(hj), whereais\nthe alignment model. Fis the encoder which pro-\nduces the sequence of annotations (h1,...,hN).\nThe output of the decoder is then projected to\na vector of scores using the output embedding:\nlt=VG(t). The scores are then converted to prob-\nability values using the softmax function.\nIn our weight tied translation model, we tie the\ninput and output embeddings of the decoder.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a7d85af-eafb-4b73-a496-754080e1c711": {"__data__": {"id_": "8a7d85af-eafb-4b73-a496-754080e1c711", "embedding": null, "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6633cb98-b968-447b-969d-9ff214531f79", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "3db1835b3e7b72675eba1627e04bb536792f0ecd37e2c816d6fb96fd4433593e"}, "2": {"node_id": "ec2e768e-4b09-43d4-bc97-6419d2ebd26f", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "743bd9bb51ee1422231c1a2455975cdb4d3fe1ecd84c77b00b845763f44e47c1"}}, "hash": "b6d51ab0500d40ffdf6f602637ef92ff75b91e9f0222eee05bb667d10840e792", "text": "We observed that when preprocessing the ACL\nWMT 2014 EN \u2192FR1and WMT 2015 EN \u2192DE2\ndatasets using BPE, many of the subwords ap-\npeared in the vocabulary of both the source and\nthe target languages. Tab. 1 shows that up to\n90% (85%) of BPE subwords between English and\nFrench (German) are shared.\nBased on this observation, we propose three-\nway weight tying (TWWT), where the input em-\nbedding of the decoder, the output embedding of\nthe decoder and the input embedding of the en-\ncoder are all tied. The single source/target vocab-\nulary of this model is the union of both the source\nand target vocabularies. In this model, both in the\nencoder and decoder, all subwords are embedded\nin the same duo-lingual space.\n4 Results\nOur experiments study the quality of various em-\nbeddings, the similarity between them, and the\nimpact of tying them on the word2vec skip-gram\nmodel, NNLMs, and NMT models.\n4.1 Quality of Obtained Embeddings\nIn order to compare the various embeddings,\nwe pooled \ufb01ve embedding evaluation meth-\nods from the literature. These evaluation\nmethods involve calculating pairwise (cosine)\n1http://statmt.org/wmt14/translation-task.html\n2http://statmt.org/wmt15/translation-task.html", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0526a46-e196-46d1-ae25-206a3a58006f": {"__data__": {"id_": "c0526a46-e196-46d1-ae25-206a3a58006f", "embedding": null, "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f64ef16c-5d58-40d7-b16e-13a5550bd232", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b917bd34bc3afe083ceea80ad4f17351f20fd1af523539c5e89c384f8e377669"}, "3": {"node_id": "102d1be8-4d2d-4804-aa05-aaaec189c3a8", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9b623a1d844aa989c4bce3c524fcaf29bb81b7b7e86072ed9f53751e6873969d"}}, "hash": "bc93333064f768e467aa4db45834dfae59d1f0607bea4015ac2aacc9b8675b1f", "text": "Input Output Tied\nSimlex999 0.30 0.29 0.17\nVerb-143 0.41 0.34 0.12\nMEN 0.66 0.61 0.50\nRare-Word 0.34 0.34 0.23\nMTurk-771 0.59 0.54 0.37\nTable 2: Comparison of input and output embeddings\nlearned by a word2vec skip-gram model. Results are also\nshown for the tied word2vec model. Spearman\u2019s correlation \u03c1\nis reported for \ufb01ve word embedding evaluation benchmarks.\nPTB text8\nEmbedding In Out Tied In Out Tied\nSimlex999 0.02 0.13 0.14 0.17 0.27 0.28\nVerb143 0.12 0.37 0.32 0.20 0.35 0.42\nMEN 0.11 0.21 0.26 0.26 0.50 0.50\nRare-Word 0.28 0.38 0.36 0.14 0.15 0.17\nMTurk771 0.17 0.28 0.30 0.26 0.48 0.45\nTable 3: Comparison of the input/output embeddings of the\nsmall model from (Zaremba et al., 2014) and the embeddings\nfrom our weight tied variant. Spearman\u2019s correlation \u03c1is pre-\nsented.\ndistances between embeddings and correlat-\ning these distances with human judgments\nof the strength of relationships between con-\ncepts. We use: Simlex999 (Hill et al., 2016),\nVerb-143 (Baker et al., 2014),\nMEN (Bruni et al., 2014), Rare-\nWord (Luong et al., 2013) and MTurk-\n771 (Halawi et al., 2012).\nWe begin by training both the tied and un-\ntied word2vec models on the text83dataset, us-\ning a vocabulary consisting only of words that\nappear at least \ufb01ve times. As can be seen\nin Tab. 2, the output embedding is almost as\ngood as the input embedding. As expected,\nthe embedding of the tied model is not com-\npetitive. The situation is different when train-\ning the small NNLM model on either the Penn\nTreebank (Marcus et al., 1993) or text8 datasets\n(for PTB, we used the same train/validation/test\nset split and vocabulary as (Mikolov et al., 2011),\nwhile on text8 we used the split/vocabulary\nfrom (Mikolov et al., 2014)). These results are\npresented in Tab. 3. In this case, the input embed-\nding is far inferior to the output embedding. The\ntied embedding is comparable to the output em-\nbedding.\nA natural question given these results and the\nanalysis in Sec. 3 is whether the word embedding\nin the weight tied NNLM model is more similar to\nthe input embedding or to the output embedding\n3http://mattmahoney.net/dc/textdataA B \u03c1(A,B)\u03c1(A,B)\u03c1(A,B)\nword2vec NNLM(S) NNLM(L)\nIn Out 0.77 0.13 0.16\nIn Tied 0.19 0.31 0.45\nOut Tied 0.39 0.65 0.77\nTable 4: Spearman\u2019s rank correlation \u03c1of similarity values\nbetween all pairs of words evaluated for the different embed -\ndings: input/output embeddings (of the untied model) and th e\nembeddings of our tied model. We show the results for both\nthe word2vec models and the small and large NNLM models\nfrom (Zaremba et al., 2014).\nModel Size Train Val.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "102d1be8-4d2d-4804-aa05-aaaec189c3a8": {"__data__": {"id_": "102d1be8-4d2d-4804-aa05-aaaec189c3a8", "embedding": null, "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f64ef16c-5d58-40d7-b16e-13a5550bd232", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b917bd34bc3afe083ceea80ad4f17351f20fd1af523539c5e89c384f8e377669"}, "2": {"node_id": "c0526a46-e196-46d1-ae25-206a3a58006f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "bc93333064f768e467aa4db45834dfae59d1f0607bea4015ac2aacc9b8675b1f"}}, "hash": "9b623a1d844aa989c4bce3c524fcaf29bb81b7b7e86072ed9f53751e6873969d", "text": "Model Size Train Val. Test\nLarge (Zaremba et al., 2014) 66M 37.8 82.2 78.4\nLarge + Weight Tying 51M 48.5 77.7 74.3\nLarge + BD (Gal, 2015) + WD 66M 24.3 78.1 75.2\nLarge + BD + WT 51M 28.2 75.8 73.2\nRHN (Zilly et al., 2016) + BD 32M 67.4 71.2 68.5\nRHN + BD + WT 24M 74.1 68.1 66.0\nTable 5: Word level perplexity (lower is better) on PTB\nand size (number of parameters) of models that use either\ndropout (baseline model) or Bayesian dropout (BD). WD \u2013\nweight decay.\nof the original model. We, therefore, run the fol-\nlowing experiment: First, for each embedding, we\ncompute the cosine distances between each pair of\nwords. We then compute Spearman\u2019s rank corre-\nlation between these vectors of distances. As can\nbe seen in Tab. 4, the results are consistent with\nour analysis and the results of Tab. 2 and Tab. 3:\nfor word2vec the input and output embeddings are\nsimilar to each other and differ from the tied em-\nbedding; for the NNLM models, the output em-\nbedding and the tied embeddings are similar, the\ninput embedding is somewhat similar to the tied\nembedding, and differs considerably from the out-\nput embedding.\n4.2 Neural Network Language Models\nWe next study the effect of tying the embeddings\non the perplexity obtained by the NNLM mod-\nels. Following (Zaremba et al., 2014), we study\ntwo NNLMs. The two models differ mostly in the\nsize of the LSTM layers. In the small model, both\nLSTM layers contain 200 units and in the large\nmodel, both contain 1500 units. In addition, the\nlarge model uses three dropout layers, one placed\nright before the \ufb01rst LSTM layer, one between h1\nandh2and one right after h2. The dropout proba-\nbility is0.65. For both the small and large models,\nwe use the same hyperparameters (i.e. weight ini-\ntialization, learning rate schedule, batch size) as\nin (Zaremba et al., 2014).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bd5681e-547b-4ffe-98b9-c5f840045017": {"__data__": {"id_": "7bd5681e-547b-4ffe-98b9-c5f840045017", "embedding": null, "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dedb899c-393a-4e00-88ee-3bd1e5d2dfdb", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c8a8c8ecae1b4f99b06372e146169f9e5e673f1208724a223c70ef56ce743295"}, "3": {"node_id": "1a3fb2ef-0a42-4c8b-8212-5882c8e59ff3", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "011c520fd17886122a5b59ca762173699887f85c08d0646205cd53d605f6afe6"}}, "hash": "21f03d2c76b5f9c545edeccfb9d4633c6763249b9af91a672449c3cb93a6ef32", "text": "Model Size Train Val. Test\nKN 5-gram 141\nRNN 123\nLSTM 117\nStack RNN 8.48M 110\nFOFE-FNN 108\nNoisy LSTM 4.65M 111.7 108.0\nDeep RNN 6.16M 107.5\nSmall model 4.65M 38.0 120.7 114.5\nSmall + WT 2.65M 36.4 117.5 112.4\nSmall + PR 4.69M 50.8 116.0 111.7\nSmall + WT + PR 2.69M 53.5 104.9 100.9\nTable 6: Word level perplexity on PTB and size\nfor models that do not use dropout. The com-\npared models are: KN 5-gram (Mikolov et al., 2011),\nRNN (Mikolov et al., 2011), LSTM (Graves, 2013),\nStack / Deep RNN (Pascanu et al., 2013),\nFOFE-FNN (Zhang et al., 2015), Noisy\nLSTM (G\u00a8 ulc \u00b8ehre et al., 2016), and the small model\nfrom (Zaremba et al., 2014). The last three models are our\nmodels, which extend the small model. PR \u2013 projection\nregularization.\nModel Small S + WT S + PR S + WT + PRtext8Train 90.4 95.6 92.6 95.3\nVal. - - - -\nTest 195.3 187.1 199.0 183.2IMDBTrain 71.3 75.4 72.0 72.9\nVal. 94.1 94.6 94.0 91.2\nTest 94.3 94.8 94.4 91.5BBCTrain 28.6 30.1 42.5 45.7\nVal. 103.6 99.4 104.9 96.4\nTest 110.8 106.8 108.7 98.9\nTable 7: Word level perplexity on the text8, IMDB and\nBBC datasets. The last three models are our models, which\nextend the small model (S) of (Zaremba et al., 2014).\nIn addition to training our models on PTB and\ntext8, following (Miyamoto and Cho, 2016), we\nalso compare the performance of the NNLMs\non the BBC (Greene and Cunningham, 2006)\nand IMDB (Maas et al., 2011) datasets,\neach of which we process and split into a\ntrain/validation/test split (we use the same\nvocabularies as (Miyamoto and Cho, 2016)).\nIn the \ufb01rst experiment, which was conducted on\nthe PTB dataset, we compare the perplexity ob-\ntained by the large NNLM model and our ver-\nsion in which the input and output embeddings\nare tied. As can be seen in Tab. 5, weight tying\nsigni\ufb01cantly reduces perplexity on both the val-\nidation set and the test set, but not on the train-\ning set. This indicates less over\ufb01tting, as expected\ndue to the reduction in the number of parameters.\nRecently, (Gal and Ghahramani, 2016), proposed\na modi\ufb01ed model that uses Bayesian dropout and\nweight decay. They obtained improved perfor-\nmance. When the embeddings of this model areSize Validation Test\nEN\u2192FR Baseline 168M 29.49 33.13\nDecoder WT 122M 29.47 33.26\nTWWT 80M 29.43 33.46\nEN\u2192DE Baseline 165M 20.96 16.79\nDecoder WT 119M 21.09 16.54\nTWWT 79M 21.02 17.15\nTable 8: Size (number of parameters) and BLEU score of\nvarious translation models. TWWT \u2013 three-way weight tying.\ntied, a similar amount of improvement is gained.\nWe tried this with and without weight decay and\ngot similar results in both cases, with slight im-\nprovement in the latter model. Finally, by re-\nplacing the LSTM with a recurrent highway net-\nwork (Zilly et al., 2016), state of the art results are\nachieved when applying weight tying. The contri-\nbution of WT is also signi\ufb01cant in this model.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1a3fb2ef-0a42-4c8b-8212-5882c8e59ff3": {"__data__": {"id_": "1a3fb2ef-0a42-4c8b-8212-5882c8e59ff3", "embedding": null, "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dedb899c-393a-4e00-88ee-3bd1e5d2dfdb", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c8a8c8ecae1b4f99b06372e146169f9e5e673f1208724a223c70ef56ce743295"}, "2": {"node_id": "7bd5681e-547b-4ffe-98b9-c5f840045017", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "21f03d2c76b5f9c545edeccfb9d4633c6763249b9af91a672449c3cb93a6ef32"}}, "hash": "011c520fd17886122a5b59ca762173699887f85c08d0646205cd53d605f6afe6", "text": "Perplexity results are often reported separately\nfor models with and without dropout. In Tab. 6, we\nreport the results of the small NNLM model, that\ndoes not utilize dropout, on PTB. As can be seen,\nboth WT and projection regularization (PR) im-\nprove the results. When combining both methods\ntogether, state of the art results are obtained. An\nanalog table for text8, IMDB and BBC is Tab. 7,\nwhich shows a signi\ufb01cant reduction in perplexity\nacross these datasets when both PR and WT are\nused. PR does not help the large models, which\nemploy dropout for regularization.\n4.3 Neural Machine Translation\nFinally, we study the impact of weight tying in at-\ntention based NMT models, using the DL4MT4\nimplementation. We train our EN \u2192FR mod-\nels on the parallel corpora provided by ACL\nWMT 2014. We use the data as processed\nby (Cho et al., 2014) using the data selection\nmethod of (Axelrod et al., 2011). For EN \u2192DE\nwe train on data from the translation task of\nWMT 2015, validate on newstest2013 and test\non newstest2014 and newstest2015. Follow-\ning (Sennrich et al., 2016b) we learn the BPE seg-\nmentation on the union of the vocabularies that\nwe are translating from and to (we use BPE with\n89500 merge operations). All models were trained\nusing Adadelta (Zeiler, 2012) for 300K updates,\nhave a hidden layer size of 1000 and all embed-\nding layers are of size 500.\nTab. 8 shows that even though the weight tied\nmodels have about 28% fewer parameters than the\n4https://github.com/nyu-dl/dl4mt-tutorial", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f260e5a4-ca53-4575-8201-9ae078411479": {"__data__": {"id_": "f260e5a4-ca53-4575-8201-9ae078411479", "embedding": null, "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "040f261a-e914-4818-9948-18bbb65aca82", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c8bf2a712584bb83a370925425d63564a99c1d03d01aa6e5b1a8e0f524a303d3"}, "3": {"node_id": "9bf950e2-ccbd-403e-90fd-d858d97a4e2d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "404102fbf864aec910c10a397930da2072efbc63d194501c63436392f61212ef"}}, "hash": "1db626dc8932eeeb5e70b6aae3c9b0db40141657562a7aa231e9f272f6ecb10c", "text": "baseline models, their performance is similar. This\nis also the case for the three-way weight tied mod-\nels, even though they have about 52% fewer pa-\nrameters than their untied counterparts.\nReferences\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 355\u2013362, Edinburgh, Scotland, UK., July.\nAssociation for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473 .\nSimon Baker, Roi Reichart, and Anna Korhonen. 2014.\nAn unsupervised model for instance level subcate-\ngorization acquisition. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 278\u2013289. Asso-\nciation for Computational Linguistics.\nYoshua Bengio, R\u00b4 ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. J. Mach. Learn. Res. , 3:1137\u20131155,\nMarch.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni.\n2014. Multimodal distributional semantics. J. Ar-\ntif. Intell. Res.(JAIR) , 49(1-47).\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder\u2013decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724\u2013\n1734, Doha, Qatar, October. Association for Com-\nputational Linguistics.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. The C Users Journal , 12(2):23\u201338.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout\nas a Bayesian approximation: Representing model\nuncertainty in deep learning. In Proceedings of the\n33rd International Conference on Machine Learning\n(ICML-16) .\nYarin Gal. 2015. A Theoretically Grounded Appli-\ncation of Dropout in Recurrent Neural Networks.\narXiv preprint arXiv:1512.05287 .\nYoav Goldberg and Omer Levy. 2014. word2vec\nexplained: deriving mikolov et al.\u2019s negative-\nsampling word-embedding method. arXiv preprint\narXiv:1402.3722 .Alex Graves. 2013. Generating sequences\nwith recurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nDerek Greene and P\u00b4 adraig Cunningham. 2006. Prac-\ntical solutions to the problem of diagonal dom-\ninance in kernel document clustering. In Proc.\n23rd International Conference on Machine learning\n(ICML\u201906) , pages 377\u2013384. ACM Press.\nC \u00b8 aglar G\u00a8 ulc \u00b8ehre, Marcin Moczulski, Misha Denil, and\nYoshua Bengio. 2016. Noisy activation functions.\narXiv preprint arXiv:1603.00391 .\nGuy Halawi, Gideon Dror, Evgeniy Gabrilovich, and\nYehuda Koren. 2012. Large-scale learning of\nword relatedness with constraints. In Proceedings of\nthe 18th ACM SIGKDD international conference on\nKnowledge discovery and data mining , pages 1406\u2013\n1414.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2016.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9bf950e2-ccbd-403e-90fd-d858d97a4e2d": {"__data__": {"id_": "9bf950e2-ccbd-403e-90fd-d858d97a4e2d", "embedding": null, "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "040f261a-e914-4818-9948-18bbb65aca82", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c8bf2a712584bb83a370925425d63564a99c1d03d01aa6e5b1a8e0f524a303d3"}, "2": {"node_id": "f260e5a4-ca53-4575-8201-9ae078411479", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "1db626dc8932eeeb5e70b6aae3c9b0db40141657562a7aa231e9f272f6ecb10c"}}, "hash": "404102fbf864aec910c10a397930da2072efbc63d194501c63436392f61212ef", "text": "Felix Hill, Roi Reichart, and Anna Korhonen. 2016.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531 .\nSepp Hochreiter and J\u00a8 urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput. , 9(8):1735\u2013\n1780, November.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classi\ufb01ers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462 .\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1700\u20131709, Seattle,\nWashington, USA, October. Association for Compu-\ntational Linguistics.\nThang Luong, Richard Socher, and Christopher Man-\nning, 2013. Proceedings of the Seventeenth Confer-\nence on Computational Natural Language Learning ,\nchapter Better Word Representations with Recursive\nNeural Networks for Morphology, pages 104\u2013113.\nAssociation for Computational Linguistics.\nL. Andrew Maas, E. Raymond Daly, T. Peter Pham,\nDan Huang, Y . Andrew Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 142\u2013150. Asso-\nciation for Computational Linguistics.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large anno-\ntated corpus of english: The penn treebank. Com-\nput. Linguist. , 19(2):313\u2013330, June.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fb0df862-8a0c-4926-9d86-45e245183d49": {"__data__": {"id_": "fb0df862-8a0c-4926-9d86-45e245183d49", "embedding": null, "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad590b0f-21fe-41a9-a41b-fedf22806134", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "db4e28e7e982c62c43bece656dfcc47cc97c41a7db36d1a70b3aa08f1bba7416"}, "3": {"node_id": "5420fbac-7c32-4b10-a748-aa149a857223", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "32143edc9f46ab85643b88d3fb0497e6a39f69d81c2130e29a887b0188ff581b"}}, "hash": "d9f607c264f76a1ae5f3950ee1bdf3c05990302e35a8ebd534aa16ba89b3f3e5", "text": "Tomas Mikolov, Martin Kara\ufb01\u00b4 at, Luk\u00b4 as Burget, Jan\nCernock\u00b4 y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010 ,\npages 1045\u20131048.\nTom\u00b4 a\u02c7 s Mikolov, Stefan Kombrink, Luk\u00b4 a\u02c7 s Burget, Jan\n\u02c7Cernock` y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn2011 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n5528\u20135531. IEEE.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Ef\ufb01cient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781 .\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26 , pages 3111\u20133119.\nTomas Mikolov, Armand Joulin, Sumit Chopra,\nMicha\u00a8 el Mathieu, and Marc\u2019Aurelio Ranzato. 2014.\nLearning longer memory in recurrent neural net-\nworks. arXiv preprint arXiv:1412.7753 .\nBhaskar Mitra, Eric Nalisnick, Nick Craswell, and\nRich Caruana. 2016. A dual embedding space\nmodel for document ranking. arXiv preprint\narXiv:1602.01137 .\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated word-character recurrent language model. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pages\n1992\u20131997. Association for Computational Linguis-\ntics.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model. In\nAdvances in neural information processing systems ,\npages 1081\u20131088.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings ef\ufb01ciently with noise-contrastive\nestimation. In Advances in Neural Information Pro-\ncessing Systems , pages 2265\u20132273.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic\nlanguage models. arXiv preprint arXiv:1206.6426 .\nRazvan Pascanu, C \u00b8 aglar G\u00a8 ulc \u00b8ehre, Kyunghyun Cho,\nand Yoshua Bengio. 2013. How to construct\ndeep recurrent neural networks. arXiv preprint\narXiv:1312.6026 .\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Edinburgh neural machine translation sys-\ntems for wmt 16. arXiv preprint arXiv:1606.02891 .Rico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of ACL .\nNitish Srivastava. 2013. Improving Neural Net-\nworks with Dropout. Master\u2019s thesis, University of\nToronto, Toronto, Canada, January.\nMartin Sundermeyer, Ralf Schl\u00a8 uter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nInInterspeech , pages 194\u2013197, Portland, OR, USA,\nSeptember.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems , pages 3104\u20133112.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5420fbac-7c32-4b10-a748-aa149a857223": {"__data__": {"id_": "5420fbac-7c32-4b10-a748-aa149a857223", "embedding": null, "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad590b0f-21fe-41a9-a41b-fedf22806134", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "db4e28e7e982c62c43bece656dfcc47cc97c41a7db36d1a70b3aa08f1bba7416"}, "2": {"node_id": "fb0df862-8a0c-4926-9d86-45e245183d49", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d9f607c264f76a1ae5f3950ee1bdf3c05990302e35a8ebd534aa16ba89b3f3e5"}}, "hash": "32143edc9f46ab85643b88d3fb0497e6a39f69d81c2130e29a887b0188ff581b", "text": "In Advances in neural information process-\ning systems , pages 3104\u20133112.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329 .\nMatthew D Zeiler. 2012. Adadelta: an adaptive learn-\ning rate method. arXiv preprint arXiv:1212.5701 .\nShiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou,\nand Li-Rong Dai. 2015. A \ufb01xed-size encoding\nmethod for variable-length sequences with its ap-\nplication to neural network language models. arXiv\npreprint arXiv:1505.01504 .\nJulian G. Zilly, Rupesh Kumar Srivastava, Jan Koutn\u00b4 \u0131k,\nand J\u00a8 urgen Schmidhuber. 2016. Recurrent highway\nnetworks. arXiv preprint arXiv:1607.03474 .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e975a0d2-165f-4f97-8855-53d42cd0638f": {"__data__": {"id_": "e975a0d2-165f-4f97-8855-53d42cd0638f", "embedding": null, "metadata": {"page_label": "1", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ec8c8b1-2c08-4d68-9769-aafcb27956e7", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c88819cf6ece2bd3fdd77dd64c13bbdf1186e0abc69f6fb8b16b537bcf9d9f51"}}, "hash": "c88819cf6ece2bd3fdd77dd64c13bbdf1186e0abc69f6fb8b16b537bcf9d9f51", "text": "Published as a conference paper at ICLR 2017\nA S TRUCTURED SELF-ATTENTIVE\nSENTENCE EMBEDDING\nZhouhan Linz\u0005\u0003, Minwei Feng\u0005, Cicero Nogueira dos Santos\u0005, Mo Yu\u0005,\nBing Xiang\u0005, Bowen Zhou\u0005& Yoshua Bengiozy\n\u0005IBM Watson\nzMontreal Institute for Learning Algorithms (MILA), Universit \u00b4e de Montr \u00b4eal\nyCIFAR Senior Fellow\nlin.zhouhan@gmail.com\nfmfeng, cicerons, yum, bingxia, zhou g@us.ibm.com\nABSTRACT\nThis paper proposes a new model for extracting an interpretable sentence embed-\nding by introducing self-attention. Instead of using a vector, we use a 2-D matrix\nto represent the embedding, with each row of the matrix attending on a different\npart of the sentence. We also propose a self-attention mechanism and a special\nregularization term for the model. As a side effect, the embedding comes with an\neasy way of visualizing what speci\ufb01c parts of the sentence are encoded into the\nembedding. We evaluate our model on 3 different tasks: author pro\ufb01ling, senti-\nment classi\ufb01cation and textual entailment. Results show that our model yields a\nsigni\ufb01cant performance gain compared to other sentence embedding methods in\nall of the 3 tasks.\n1 I NTRODUCTION\nMuch progress has been made in learning semantically meaningful distributed representations of\nindividual words, also known as word embeddings (Bengio et al., 2001; Mikolov et al., 2013).\nOn the other hand, much remains to be done to obtain satisfying representations of phrases and\nsentences. Those methods generally fall into two categories. The \ufb01rst consists of universal sentence\nembeddings usually trained by unsupervised learning (Hill et al., 2016). This includes SkipThought\nvectors (Kiros et al., 2015), ParagraphVector (Le & Mikolov, 2014), recursive auto-encoders (Socher\net al., 2011; 2013), Sequential Denoising Autoencoders (SDAE), FastSent (Hill et al., 2016), etc.\nThe other category consists of models trained speci\ufb01cally for a certain task. They are usually\ncombined with downstream applications and trained by supervised learning. One generally \ufb01nds\nthat speci\ufb01cally trained sentence embeddings perform better than generic ones, although generic\nones can be used in a semi-supervised setting, exploiting large unlabeled corpora. Several models\nhave been proposed along this line, by using recurrent networks (Hochreiter & Schmidhuber, 1997;\nChung et al., 2014), recursive networks (Socher et al., 2013) and convolutional networks (Kalchbren-\nner et al., 2014; dos Santos & Gatti, 2014; Kim, 2014) as an intermediate step in creating sentence\nrepresentations to solve a wide variety of tasks including classi\ufb01cation and ranking (Yin & Sch \u00a8utze,\n2015; Palangi et al., 2016; Tan et al., 2016; Feng et al., 2015). A common approach in previous\nmethods consists in creating a simple vector representation by using the \ufb01nal hidden state of the\nRNN or the max (or average) pooling from either RNNs hidden states or convolved n-grams. Ad-\nditional works have also been done in exploiting linguistic structures such as parse and dependence\ntrees to improve sentence representations (Ma et al., 2015; Mou et al., 2015b; Tai et al., 2015).\nFor some tasks people propose to use attention mechanism on top of the CNN or LSTM model to\nintroduce extra source of information to guide the extraction of sentence embedding (dos Santos\net al., 2016). However, for some other tasks like sentiment classi\ufb01cation, this is not directly appli-\ncable since there is no such extra information: the model is only given one single sentence as input.\nIn those cases, the most common way is to add a max pooling or averaging step across all time steps\n\u0003This work has been done during the 1st author\u2019s internship with IBM Watson.\n1arXiv:1703.03130v1  [cs.CL]  9 Mar 2017", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57953232-be82-44ce-97d3-910966bdd746": {"__data__": {"id_": "57953232-be82-44ce-97d3-910966bdd746", "embedding": null, "metadata": {"page_label": "2", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "598dad45-b79a-4f0f-b791-626974833e17", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "5f40ca128d69dbe47f34d3156dde8b15f4c0108db5156baa107bd7f950bb8800"}}, "hash": "5f40ca128d69dbe47f34d3156dde8b15f4c0108db5156baa107bd7f950bb8800", "text": "Published as a conference paper at ICLR 2017\nwe.\nhada\nnice\nexperiencethatin\nrestaurant.........\nw1w2w3 wn w4h1h2h3h4... hnM\nAi1Ai2Ai3Ai4... Ainm1m2 mr mi\n.........\n(a)\nh1h2hn...\nWs1n\nntanh\nsoftmaxWs2\nAda\nr\nr2u(b)\nFigure 1: A sample model structure showing the sentence embedding model combined with a fully\nconnected and softmax layer for sentiment analysis (a). The sentence embedding Mis computed as\nmultiple weighted sums of hidden states from a bidirectional LSTM ( h1; :::;hn), where the summa-\ntion weights ( Ai1; :::; A in) are computed in a way illustrated in (b). Blue colored shapes stand for\nhidden representations, and red colored shapes stand for weights, annotations, or input/output.\n(Lee & Dernoncourt, 2016), or just pick up the hidden representation at the last time step as the\nencoded embedding (Margarit & Subramaniam, 2016).\nA common approach in many of the aforementioned methods consists of creating a simple vector\nrepresentation by using the \ufb01nal hidden state of the RNN or the max (or average) pooling from\neither RNNs hidden states or convolved n-grams. We hypothesize that carrying the semantics along\nall time steps of a recurrent model is relatively hard and not necessary. We propose a self-attention\nmechanism for these sequential models to replace the max pooling or averaging step. Different from\nprevious approaches, the proposed self-attention mechanism allows extracting different aspects of\nthe sentence into multiple vector representations. It is performed on top of an LSTM in our sentence\nembedding model. This enables attention to be used in those cases when there are no extra inputs. In\naddition, due to its direct access to hidden representations from previous time steps, it relieves some\nlong-term memorization burden from LSTM. As a side effect coming together with our proposed\nself-attentive sentence embedding, interpreting the extracted embedding becomes very easy and\nexplicit.\nSection 2 details on our proposed self-attentive sentence embedding model, as well as a regular-\nization term we proposed for this model, which is described in Section 2.2. We also provide a\nvisualization method for this sentence embedding in section 2.3. We then evaluate our model in\nauthor pro\ufb01ling, sentiment classi\ufb01cation and textual entailment tasks in Section 4.\n2 A PPROACH\n2.1 M ODEL\nThe proposed sentence embedding model consists of two parts. The \ufb01rst part is a bidirectional\nLSTM, and the second part is the self-attention mechanism, which provides a set of summation\nweight vectors for the LSTM hidden states. These set of summation weight vectors are dotted\nwith the LSTM hidden states, and the resulting weighted LSTM hidden states are considered as\nan embedding for the sentence. It can be combined with, for example, a multilayer perceptron to\n2", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0d270c5-dbca-42ea-9e99-c24e5b966712": {"__data__": {"id_": "c0d270c5-dbca-42ea-9e99-c24e5b966712", "embedding": null, "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5f4a401-1e88-462f-8b8b-405dfd8ac7e4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "1fd94d43c9a6603bae26b34fc0a87bdc54bf619c0a3cdf56a4b84840175b5a39"}, "3": {"node_id": "a8f3c25a-b9f4-4c20-aba3-1e29d643033c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "76f71b34409583d1f86716b28ae382f8e40131da06904cff29782de21b1b3ba1"}}, "hash": "cc94b13d953d2c2ad6c9ae063f9271fe04a39e61735783d41cb05d3ddeb9770f", "text": "Published as a conference paper at ICLR 2017\nbe applied on a downstream application. Figure 1 shows an example when the proposed sentence\nembedding model is applied to sentiment analysis, combined with a fully connected layer and a\nsoftmax layer. Besides using a fully connected layer, we also proposes an approach that prunes\nweight connections by utilizing the 2-D structure of matrix sentence embedding, which is detailed\nin Appendix A. For this section, we will use Figure 1 to describe our model.\nSuppose we have a sentence, which has ntokens, represented in a sequence of word embeddings.\nS= (w1;w2;\u0001\u0001\u0001wn) (1)\nHerewiis a vector standing for a ddimentional word embedding for the i-th word in the sentence.\nSis thus a sequence represented as a 2-D matrix, which concatenates all the word embeddings\ntogether. Sshould have the shape n-by-d.\nNow each entry in the sequence Sare independent with each other. To gain some dependency be-\ntween adjacent words within a single sentence, we use a bidirectional LSTM to process the sentence:\n\u0000 !ht=\u0000\u0000\u0000\u0000!LSTM (wt;\u0000\u0000!ht\u00001) (2)\n \u0000ht= \u0000\u0000\u0000\u0000LSTM (wt; \u0000\u0000ht+1) (3)\nAnd we concatenate each\u0000 !htwith \u0000htto obtain a hidden state ht. Let the hidden unit number for each\nunidirectional LSTM be u. For simplicity, we note all the n hts asH, who have the size n-by-2u.\nH= (h1;h2;\u0001\u0001\u0001hn) (4)\nOur aim is to encode a variable length sentence into a \ufb01xed size embedding. We achieve that by\nchoosing a linear combination of the nLSTM hidden vectors in H. Computing the linear combina-\ntion requires the self-attention mechanism. The attention mechanism takes the whole LSTM hidden\nstates Has input, and outputs a vector of weights a:\na=softmax\u0000\nws2tanh\u0000\nWs1HT\u0001\u0001\n(5)\nHere Ws1is a weight matrix with a shape of da-by-2u. and ws2is a vector of parameters with\nsizeda, where dais a hyperparameter we can set arbitrarily. Since His sized n-by-2u, the anno-\ntation vector awill have a size n. the softmax (_)ensures all the computed weights sum up to 1.\nThen we sum up the LSTM hidden states Haccording to the weight provided by ato get a vector\nrepresentation mof the input sentence.\nThis vector representation usually focuses on a speci\ufb01c component of the sentence, like a special set\nof related words or phrases. So it is expected to re\ufb02ect an aspect, or component of the semantics in\na sentence. However, there can be multiple components in a sentence that together forms the overall\nsemantics of the whole sentence, especially for long sentences. (For example, two clauses linked\ntogether by an \u201dand.\u201d) Thus, to represent the overall semantics of the sentence, we need multiple m\u2019s\nthat focus on different parts of the sentence. Thus we need to perform multiple hops of attention.\nSay we want rdifferent parts to be extracted from the sentence, with regard to this, we extend the\nws2into a r-by-damatrix, note it as Ws2, and the resulting annotation vector abecomes annotation\nmatrix A. Formally,\nA=softmax\u0000\nWs2tanh\u0000\nWs1HT\u0001\u0001\n(6)\nHere the softmax (_)is performed along the second dimension of its input. We can deem Equation\n6 as a 2-layer MLP without bias, whose hidden unit numbers is da, and parameters are fWs2; Ws1g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a8f3c25a-b9f4-4c20-aba3-1e29d643033c": {"__data__": {"id_": "a8f3c25a-b9f4-4c20-aba3-1e29d643033c", "embedding": null, "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5f4a401-1e88-462f-8b8b-405dfd8ac7e4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "1fd94d43c9a6603bae26b34fc0a87bdc54bf619c0a3cdf56a4b84840175b5a39"}, "2": {"node_id": "c0d270c5-dbca-42ea-9e99-c24e5b966712", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "cc94b13d953d2c2ad6c9ae063f9271fe04a39e61735783d41cb05d3ddeb9770f"}}, "hash": "76f71b34409583d1f86716b28ae382f8e40131da06904cff29782de21b1b3ba1", "text": "The embedding vector mthen becomes an r-by-2uembedding matrix M. We compute the r\nweighted sums by multiplying the annotation matrix Aand LSTM hidden states H, the resulting\nmatrix is the sentence embedding:\nM=AH (7)\n2.2 P ENALIZATION TERM\nThe embedding matrix Mcan suffer from redundancy problems if the attention mechanism always\nprovides similar summation weights for all the rhops. Thus we need a penalization term to encour-\nage the diversity of summation weight vectors across different hops of attention.\n3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "28744c52-322a-4899-a6e0-fedd5b134458": {"__data__": {"id_": "28744c52-322a-4899-a6e0-fedd5b134458", "embedding": null, "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6742c4da-7e0a-4a21-9fe4-e182dd0e221c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a2666ca0798f5c9a7d525cd988baf989fe6ba6197cb7428b572e12b7f77c332c"}, "3": {"node_id": "2e0985ac-c3f2-42e1-a1fe-aade395804e8", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4ae285d902f9a15a42623756265b34dc968ee3638aa1680064c57d980a17ada4"}}, "hash": "37a2c5057a2d9b4ca6ba7c740fe389c55f6d74d23240c0e02fe3cd3592ec65cb", "text": "Published as a conference paper at ICLR 2017\nThe best way to evaluate the diversity is de\ufb01nitely the Kullback Leibler divergence between any 2\nof the summation weight vectors. However, we found that not very stable in our case. We conjecture\nit is because we are maximizing a set of KL divergence (instead of minimizing only one, which is\nthe usual case), we are optimizing the annotation matrix A to have a lot of suf\ufb01ciently small or\neven zero values at different softmax output units, and these vast amount of zeros is making the\ntraining unstable. There is another feature that KL doesn\u2019t provide but we want, which is, we want\neach individual row to focus on a single aspect of semantics, so we want the probability mass in the\nannotation softmax output to be more focused. but with KL penalty we cant encourage that.\nWe hereby introduce a new penalization term which overcomes the aforementioned shortcomings.\nCompared to the KL divergence penalization, this term consumes only one third of the computation.\nWe use the dot product of Aand its transpose, subtracted by an identity matrix, as a measure of\nredundancy.\nP=\r\r\u0000\nAAT\u0000I\u0001\r\r\nF2(8)\nHerek\u000fkFstands for the Frobenius norm of a matrix. Similar to adding an L2 regularization term,\nthis penalization term Pwill be multiplied by a coef\ufb01cient, and we minimize it together with the\noriginal loss, which is dependent on the downstream application.\nLet\u2019s consider two different summation vectors aiandajinA. Because of the softmax, all entries\nwithin any summation vector in Ashould sum up to 1. Thus they can be deemed as probability\nmasses in a discrete probability distribution. For any non-diagonal elements aij(i6=j)in the AAT\nmatrix, it corresponds to a summation over elementwise product of two distributions:\n0< aij=nX\nk=1ai\nkaj\nk<1 (9)\nwhere ai\nkandaj\nkare the k-th element in the aiandajvectors, respectively. In the most extreme case,\nwhere there is no overlap between the two probability distributions aiandaj, the correspond aijwill\nbe 0. Otherwise, it will have a positive value. On the other extreme end, if the two distributions are\nidentical and all concentrates on one single word, it will have a maximum value of 1. We subtract\nan identity matrix from AATso that forces the elements on the diagonal of AATto approximate\n1, which encourages each summation vector aito focus on as few number of words as possible,\nforcing each vector to be focused on a single aspect, and all other elements to 0, which punishes\nredundancy between different summation vectors.\n2.3 V ISUALIZATION\nThe interpretation of the sentence embedding is quite straight forward because of the existence of\nannotation matrix A. For each row in the sentence embedding matrix M, we have its corresponding\nannotation vector ai. Each element in this vector corresponds to how much contribution the LSTM\nhidden state of a token on that position contributes to. We can thus draw a heat map for each row of\nthe embedding matrix MThis way of visualization gives hints on what is encoded in each part of\nthe embedding, adding an extra layer of interpretation. (See Figure 3a and 3b).\nThe second way of visualization can be achieved by summing up over all the annotation vectors,\nand then normalizing the resulting weight vector to sum up to 1. Since it sums up all aspects of\nsemantics of a sentence, it yields a general view of what the embedding mostly focuses on. We can\n\ufb01gure out which words the embedding takes into account a lot, and which ones are skipped by the\nembedding. See Figure 3c and 3d.\n3 R ELATED WORK\nVarious supervised and unsupervised sentence embedding models have been mentioned in Section\n1. Different from those models, our proposed method uses a new self-attention mechanism that\nallows it to extract different aspects of the sentence into multiple vector-representations. The matrix\nstructure together with the penalization term gives our model a greater capacity to disentangle the\nlatent information from the input sentence. We also do not use linguistic structures to guide our\nsentence representation model.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2e0985ac-c3f2-42e1-a1fe-aade395804e8": {"__data__": {"id_": "2e0985ac-c3f2-42e1-a1fe-aade395804e8", "embedding": null, "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6742c4da-7e0a-4a21-9fe4-e182dd0e221c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a2666ca0798f5c9a7d525cd988baf989fe6ba6197cb7428b572e12b7f77c332c"}, "2": {"node_id": "28744c52-322a-4899-a6e0-fedd5b134458", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "37a2c5057a2d9b4ca6ba7c740fe389c55f6d74d23240c0e02fe3cd3592ec65cb"}}, "hash": "4ae285d902f9a15a42623756265b34dc968ee3638aa1680064c57d980a17ada4", "text": "We also do not use linguistic structures to guide our\nsentence representation model. Additionally, using our method we can easily create visualizations\nthat can help in the interpretation of the learned representations.\n4", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f8e35205-bd27-4b38-a90b-1c72ba35fe8f": {"__data__": {"id_": "f8e35205-bd27-4b38-a90b-1c72ba35fe8f", "embedding": null, "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e9f0a56-7e29-4909-88bd-73e467a5eaeb", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "537efb6661d06d58276b55cd61947d7acd1b1c4089bae7198e6e3241c2584e4d"}, "3": {"node_id": "1c972d34-d3b8-4ec5-9acb-5d1fc239bf9b", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "b958947e62d869ba0dcbb8d7923c6e76618dd5287851bd74ef64b992b52a4d82"}}, "hash": "0adf93c852820412cec71cee8fe3a0d50aa1e61f4a439eb62e020542f9425a2a", "text": "Published as a conference paper at ICLR 2017\nSome recent work have also proposed supervised methods that use intra/self-sentence attention. Ling\net al. (2015) proposed an attention based model for word embedding, which calculates an attention\nweight for each word at each possible position in the context window. However this method cannot\nbe extended to sentence level embeddings since one cannot exhaustively enumerate all possible\nsentences. Liu et al. (2016a) proposes a sentence level attention which has a similar motivation but\ndone differently. They utilize the mean pooling over LSTM states as the attention source, and use\nthat to re-weight the pooled vector representation of the sentence.\nApart from the previous 2 variants, we want to note that Li et al. (2016) proposed a same self\nattention mechanism for question encoding in their factoid QA model, which is concurrent to our\nwork. The difference lies in that their encoding is still presented as a vector, but our attention\nproduces a matrix representation instead, with a specially designed penalty term. We applied the\nmodel for sentiment anaysis and entailment, and their model is for factoid QA.\nThe LSTMN model (Cheng et al., 2016) also proposed a very successful intra-sentence level atten-\ntion mechanism, which is later used by Parikh et al. (2016). We see our attention and theirs as having\ndifferent granularities. LSTMN produces an attention vector for each of its hidden states during the\nrecurrent iteration, which is sort of an \u201donline updating\u201d attention. It\u2019s more \ufb01ne-grained, targeting\nat discovering lexical correlations between a certain word and its previous words. On the contrary,\nour attention mechanism is only performed once, focuses directly on the semantics that makes sense\nfor discriminating the targets. It is less focused on relations between words, but more on the seman-\ntics of the whole sentence that each word contributes to. Computationally, our method also scales up\nwith the sentence length better, since it doesn\u2019t require the LSTM to compute an annotation vector\nover all of its previous words each time when the LSTMN computes its next step.\n4 E XPERIMENTAL RESULTS\nWe \ufb01rst evaluate our sentence embedding model by applying it to 3 different datasets: the Age\ndataset, the Yelp dataset, and the Stanford Natural Language Inference (SNLI) Corpus. These 3\ndatasets fall into 3 different tasks, corresponding to author pro\ufb01ling, sentiment analysis, and tex-\ntual entailment, respectively. Then we also perform a set of exploratory experiments to validate\nproperties of various aspects for our sentence embedding model.\n4.1 A UTHOR PROFILING\nThe Author Pro\ufb01ling dataset1consists of Twitter tweets in English, Spanish, and Dutch. For some of\nthe tweets, it also provides an age and gender of the user when writing the tweet. The age range are\nsplit into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+. We use English tweets as input, and use those\ntweets to predict the age range of the user. Since we are predicting the age of users, we refer to it\nas Age dataset in the rest of our paper. We randomly selected 68485 tweets as training set, 4000 for\ndevelopment set, and 4000 for test set. Performances are also chosen to be classi\ufb01cation accuracy.\nTable 1: Performance Comparision of Different Models on Yelp and Age Dataset\nModels Yelp Age\nBiLSTM + Max Pooling + MLP 61.99% 77.40%\nCNN + Max Pooling + MLP 62.05% 78.15%\nOur Model 64.21% 80.45%\nWe compare our model with two baseline models: biLSTM and CNN. For the two baseline models.\nThe biLSTM model uses a bidirectional LSTM with 300 dimensions in each direction, and use max\npooling across all LSTM hidden states to get the sentence embedding vector, then use a 2-layer\nReLU output MLP with 3000 hidden states to output the classi\ufb01cation result. The CNN model\nuses the same scheme, but substituting biLSTM with 1 layer of 1-D convolutional network.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c972d34-d3b8-4ec5-9acb-5d1fc239bf9b": {"__data__": {"id_": "1c972d34-d3b8-4ec5-9acb-5d1fc239bf9b", "embedding": null, "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e9f0a56-7e29-4909-88bd-73e467a5eaeb", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "537efb6661d06d58276b55cd61947d7acd1b1c4089bae7198e6e3241c2584e4d"}, "2": {"node_id": "f8e35205-bd27-4b38-a90b-1c72ba35fe8f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0adf93c852820412cec71cee8fe3a0d50aa1e61f4a439eb62e020542f9425a2a"}}, "hash": "b958947e62d869ba0dcbb8d7923c6e76618dd5287851bd74ef64b992b52a4d82", "text": "During\ntraining we use 0.5 dropout on the MLP and 0.0001 L2 regularization. We use stochastic gradient\ndescent as the optimizer, with a learning rate of 0.06, batch size 16. For biLSTM, we also clip the\n1http://pan.webis.de/clef16/pan16-web/author-profiling.html\n5", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d1a2ad8d-d439-4461-af79-9b425b652b1c": {"__data__": {"id_": "d1a2ad8d-d439-4461-af79-9b425b652b1c", "embedding": null, "metadata": {"page_label": "6", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7456b3b-d65d-4c0a-a434-f699011c3d80", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "aee2a92c67ee3cbb1e3c79cb23644d671bff93252f41dce83d0965a276ef54a2"}}, "hash": "aee2a92c67ee3cbb1e3c79cb23644d671bff93252f41dce83d0965a276ef54a2", "text": "Published as a conference paper at ICLR 2017\n(a) 1 star reviews\n(b) 5 star reviews\nFigure 2: Heatmap of Yelp reviews with the two extreme score.\nnorm of gradients to be between -0.5 and 0.5. We searched hyperparameters in a wide range and\n\ufb01nd the aforementioned set of hyperparameters yields the highest accuracy.\nFor our model, we use the same settings as what we did in biLSTM. We also use a 2-layer ReLU\noutput MLP, but with 2000 hidden units. In addition, our self-attention MLP has a hidden layer with\n350 units (the dain Section 2), we choose the matrix embedding to have 30 rows (the r), and a\ncoef\ufb01cient of 1 for the penalization term.\nWe train all the three models until convergence and select the corresponding test set performance\naccording to the best development set performance. Our results show that the model outperforms\nboth of the biLSTM and CNN baselines by a signi\ufb01cant margin.\n4.2 S ENTIMENT ANALYSIS\nWe choose the Yelp dataset2for sentiment analysis task. It consists of 2.7M yelp reviews, we take\nthe review as input and predict the number of stars the user who wrote that review assigned to the\ncorresponding business store. We randomly select 500K review-star pairs as training set, and 2000\nfor development set, 2000 for test set. We tokenize the review texts by Stanford tokenizer. We use\n2https://www.yelp.com/dataset challenge\n6", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43b1053b-0a4a-46ae-a0ee-9fc41955ad9e": {"__data__": {"id_": "43b1053b-0a4a-46ae-a0ee-9fc41955ad9e", "embedding": null, "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84e8d558-a595-4bf4-9587-f7ede9273991", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4367e7b7f299fc13ec4688810fc3313aa79f0665c12da47203e526c43d807647"}, "3": {"node_id": "42ec8213-57f8-4ef5-b272-27e16f7caaa7", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e7606dee7ac83b53756e8113a3d74d6eeea36ee9da2386e08f2695d37fb3d172"}}, "hash": "ed797efdb4fd56e7eadeb98335596da74e3aee2725301b244f942666ac61cd05", "text": "Published as a conference paper at ICLR 2017\n100 dimensional word2vec as initialization for word embeddings, and tune the embedding during\ntraining across all of our experiments. The target number of stars is an integer number in the range\nof[1;5], inclusive. We are treating the task as a classi\ufb01cation task, i.e., classify a review text into\none of the 5 classes. We use classi\ufb01cation accuracy as a measurement.\nFor the two baseline models, we use the same setting as what we used for Author Pro\ufb01ling dataset,\nexcept that we are using a batch size of 32 instead. For our model, we are also using the same\nsetting, except that we choose the hidden unit numbers in the output MLP to be 3000 instead. We\nalso observe a signi\ufb01cant performance gain comparining to the two baselines. (Table 1)\nAs an interpretation of the learned sentence embedding, we use the second way of visualization\ndescribed in Section 2.3 to plot heat maps for some of the reviews in the dataset. We randomly\nselect 5 examples of negative (1 star) and positive (5 stars) reviews from the test set, when the model\nhas a high con\ufb01dence ( >0:8) in predicting the label. As shown in Figure 2, we \ufb01nd that the model\nmajorly learns to capture some key factors in the review that indicate strongly on the sentiment\nbehind the sentence. For most of the short reviews, the model manages to capture all the key factors\nthat contribute to an extreme score, but for longer reviews, the model is still not able to capture all\nrelated factors. For example, in the 3rd review in Figure 2b), it seems that a lot of focus is spent on\none single factor, i.e., the \u201dso much fun\u201d, and the model puts a little amount of attention on other\nkey points like \u201dhighly recommend\u201d, \u201damazing food\u201d, etc.\n4.3 T EXTUAL ENTAILMENT\nWe use the biggest dataset in textual entailment, the SNLI corpus (Bowman et al., 2015) for our\nevaluation on this task. SNLI is a collection of 570k human-written English sentence pairs manually\nlabeled for balanced classi\ufb01cation with the labels entailment, contradiction, and neutral. The model\nwill be given a pair of sentences, called hypothesis and premise respectively, and asked to tell if the\nsemantics in the two sentences are contradicting with each other or not. It is also a classi\ufb01cation\ntask, so we measure the performance by accuracy.\nWe process the hypothesis and premise independently, and then extract the relation between the two\nsentence embeddings by using multiplicative interactions proposed in Memisevic (2013) (see Ap-\npendix B for details), and use a 2-layer ReLU output MLP with 4000 hidden units to map the hidden\nrepresentation into classi\ufb01cation results. Parameters of biLSTM and attention MLP are shared across\nhypothesis and premise. The biLSTM is 300 dimension in each direction, the attention MLP has\n150 hidden units instead, and both sentence embeddings for hypothesis and premise have 30 rows\n(ther). The penalization term coef\ufb01cient is set to 0.3. We use 300 dimensional GloVe (Pennington\net al., 2014) word embedding to initialize word embeddings. We use AdaGrad as the optimizer,\nwith a learning rate of 0.01. We don\u2019t use any extra regularization methods, like dropout or L2\nnormalization. Training converges after 4 epochs, which is relatively fast.\nThis task is a bit different from previous two tasks, in that it has 2 sentences as input. There are\na bunch of ways to add inter-sentence level attention, and those attentions bring a lot of bene\ufb01ts.\nTo make the comparison focused and fair, we only compare methods that fall into the sentence\nencoding-based models. i.e., there is no information exchanged between the hypothesis and premise\nbefore they are encoded into some distributed encoding.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "42ec8213-57f8-4ef5-b272-27e16f7caaa7": {"__data__": {"id_": "42ec8213-57f8-4ef5-b272-27e16f7caaa7", "embedding": null, "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84e8d558-a595-4bf4-9587-f7ede9273991", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4367e7b7f299fc13ec4688810fc3313aa79f0665c12da47203e526c43d807647"}, "2": {"node_id": "43b1053b-0a4a-46ae-a0ee-9fc41955ad9e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ed797efdb4fd56e7eadeb98335596da74e3aee2725301b244f942666ac61cd05"}}, "hash": "e7606dee7ac83b53756e8113a3d74d6eeea36ee9da2386e08f2695d37fb3d172", "text": "Table 2: Test Set Performance Compared to other Sentence Encoding Based Methods in SNLI Datset\nModel Test Accuracy\n300D LSTM encoders (Bowman et al., 2016) 80.6%\n600D (300+300) BiLSTM encoders (Liu et al., 2016b) 83.3%\n300D Tree-based CNN encoders (Mou et al., 2015a) 82.1%\n300D SPINN-PI encoders (Bowman et al., 2016) 83.2%\n300D NTI-SLSTM-LSTM encoders (Munkhdalai & Yu, 2016a) 83.4%\n1024D GRU encoders with SkipThoughts pre-training (Vendrov et al., 2015) 81.4%\n300D NSE encoders (Munkhdalai & Yu, 2016b) 84.6%\nOur method 84.4%\n7", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cdd72c20-924f-444d-8f21-a8fe217f38c5": {"__data__": {"id_": "cdd72c20-924f-444d-8f21-a8fe217f38c5", "embedding": null, "metadata": {"page_label": "8", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd074a23-19f2-4efd-bd4a-ff87a76d76d0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "312ae5b8707c296847fbb4135d430dab40e2b0a429580997a12a31a78dff7f3a"}}, "hash": "312ae5b8707c296847fbb4135d430dab40e2b0a429580997a12a31a78dff7f3a", "text": "Published as a conference paper at ICLR 2017\nWe \ufb01nd that compared to other published approaches, our method shows a signi\ufb01cant gain ( \u00151%)\nto them, except for the 300D NSE encoders, which is the state-of-the-art in this category. However,\nthe0:2%different is relatively small compared to the differences between other methods.\n4.4 E XPLORATORY EXPERIMENTS\nIn this subsection we are going to do a set of exploratory experiments to study the relative effect of\neach component in our model.\n4.4.1 E FFECT OF PENALIZATION TERM\nSince the purpose of introducing the penalization term Pis majorly to discourage the redundancy\nin the embedding, we \ufb01rst directly visualize the heat maps of each row when the model is presented\nwith a sentence. We compare two identical models with the same size as detailed in Section 4.1\ntrained separately on Age dataset, one with this penalization term (where the penalization coef\ufb01cient\nis set to 1.0) and the other with no penalty. We randomly select one tweet from the test set and\ncompare the two models by plotting a heat map for each hop of attention on that single tweet. Since\nthere are 30 hops of attention for each model, which makes plotting all of them quite redundant, we\nonly plot 6 of them. These 6 hops already re\ufb02ect the situation in all of the 30 hops.\n(a)\n (b)\n(c) without penalization\n (d) with 1.0 penalization\nFigure 3: Heat maps for 2 models trained on Age dataset. The left column is trained without the\npenalization term, and the right column is trained with 1.0 penalization. (a) and (b) shows detailed\nattentions taken by 6 out of 30 rows of the matrix embedding, while (c) and (d) shows the overall\nattention by summing up all 30 attention weight vectors.\n(a) Yelp without penalization\n (b) Yelp with penalization\nFigure 4: Attention of sentence embedding on 3 different Yelp reviews. The left one is trained\nwithout penalization, and the right one is trained with 1.0 penalization.\n8", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6bf0c098-2f15-4fa6-8ba2-2765e2d4f404": {"__data__": {"id_": "6bf0c098-2f15-4fa6-8ba2-2765e2d4f404", "embedding": null, "metadata": {"page_label": "9", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11f1db50-1d2a-41be-8e60-894c2a28ce8b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0630f3faaccd461c07695a182ea9e82314f8f500cbe9899e995c4357a8fabaab"}}, "hash": "0630f3faaccd461c07695a182ea9e82314f8f500cbe9899e995c4357a8fabaab", "text": "Published as a conference paper at ICLR 2017\nTable 3: Performance comparision regarding the penalization term\nPenalization coef\ufb01cient Yelp Age\n1.0 64.21% 80.45%\n0.0 61.74% 79.27%\nFrom the \ufb01gure we can tell that the model trained without the penalization term have lots of redun-\ndancies between different hops of attention (Figure 3a), resulting in putting lot of focus on the word\n\u201dit\u201d (Figure 3c), which is not so relevant to the age of the author. However in the right column, the\nmodel shows more variations between different hops, and as a result, the overall embedding focuses\non \u201dmail-replies spam\u201d instead. (Figure 3d)\nFor the Yelp dataset, we also observe a similar phenomenon. To make the experiments more ex-\nplorative, we choose to plot heat maps of overall attention heat maps for more samples, instead of\nplotting detailed heat maps for a single sample again. Figure 4 shows overall focus of the sentence\nembedding on three different reviews. We observe that with the penalization term, the model tends\nto be more focused on important parts of the review. We think it is because that we are encouraging\nit to be focused, in the diagonals of matrix AAT(Equation 8).\nTo validate if these differences result in performance difference, we evaluate four models trained\non Yelp and Age datasets, both with and without the penalization term. Results are shown in Table\n3. Consistent with what expected, models trained with the penalization term outperforms their\ncounterpart trained without.\nIn SNLI dataset, although we observe that introducing the penalization term still contributes to en-\ncouraging the diversity of different rows in the matrix sentence embedding, and forcing the network\nto be more focused on the sentences, the quantitative effect of this penalization term is not so obvious\non SNLI dataset. Both models yield similar test set accuracies.\n4.4.2 E FFECT OF MULTIPLE VECTORS\nHaving multiple rows in the sentence embedding is expected to provide more abundant information\nabout the encoded content. It makes sence to evaluate how signi\ufb01cant the improvement can be\nbrought by r. Taking the models we used for Age and SNLI dataset as an example, we vary rfrom\n1to30for each task, and train the resulting 10models independently (Figure 5). Note that when\nr= 1, the sentence embedding reduces to a normal vector form.\nFrom this \ufb01gure we can \ufb01nd that, without having multiple rows, the model performs on-par with\nits competitiors which use other forms of vector sentence embeddings. But there is signi\ufb01cant\n(a)\n (b)\nFigure 5: Effect of the number of rows ( r) in matrix sentence embedding. The vertical axes indicates\ntest set accuracy and the horizontal axes indicates training epoches. Numbers in the legends stand\nfor the corresponding values of r. (a) is conducted in Age dataset and (b) is conducted in SNLI\ndataset.\n9", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3171923f-0c02-4444-a14b-c2814f035ea3": {"__data__": {"id_": "3171923f-0c02-4444-a14b-c2814f035ea3", "embedding": null, "metadata": {"page_label": "10", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba107626-2d72-4316-b3a1-fa3799a192fe", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "856d3c1b2659a460c3d0c1eba3c146f16a1bb585464d530bafddb84f24c04809"}}, "hash": "856d3c1b2659a460c3d0c1eba3c146f16a1bb585464d530bafddb84f24c04809", "text": "Published as a conference paper at ICLR 2017\ndifference between having only one vector for the sentence embedding and multiple vectors. The\nmodels are also quite invariant with respect to r, since in the two \ufb01gures a wide range of values\nbetween 10to30are all generating comparable curves.\n5 C ONCLUSION AND DISCUSSION\nIn this paper, we introduced a \ufb01xed size, matrix sentence embedding with a self-attention mecha-\nnism. Because of this attention mechanism, there is a way to interpret the sentence embedding in\ndepth in our model. Experimental results over 3 different tasks show that the model outperforms\nother sentence embedding models by a signi\ufb01cant margin.\nIntroducing attention mechanism allows the \ufb01nal sentence embedding to directly access previous\nLSTM hidden states via the attention summation. Thus the LSTM doesn\u2019t need to carry every piece\nof information towards its last hidden state. Instead, each LSTM hidden state is only expected to\nprovide shorter term context information around each word, while the higher level semantics, which\nrequires longer term dependency, can be picked up directly by the attention mechanism. This setting\nreliefs the burden of LSTM to carry on long term dependencies. Our experiments also support that,\nas we observed that our model has a bigger advantage when the contents are longer. Further more,\nthe notion of summing up elements in the attention mechanism is very primitive, it can be something\nmore complex than that, which will allow more operations on the hidden states of LSTM.\nThe model is able to encode any sequence with variable length into a \ufb01xed size representation,\nwithout suffering from long-term dependency problems. This brings a lot of scalability to the model:\nwithout any modi\ufb01cation, it can be applied directly to longer contents like paragraphs, articles, etc.\nThough this is beyond the focus of this paper, it remains an interesting direction to explore as a\nfuture work.\nAs a downside of our proposed model, the current training method heavily relies on downstream\napplications, thus we are not able to train it in an unsupervised way. The major obstacle towards\nenabling unsupervised learning in this model is that during decoding, we don\u2019t know as prior how\nthe different rows in the embedding should be divided and reorganized. Exploring all those possible\ndivisions by using a neural network could easily end up with over\ufb01tting. Although we can still do\nunsupervised learning on the proposed model by using a sequential decoder on top of the sentence\nembedding, it merits more to \ufb01nd some other structures as a decoder.\nACKNOWLEDGMENTS\nThe authors would like to acknowledge the developers of Theano (Theano Development Team,\n2016) and Lasagne. The \ufb01rst author would also like to thank IBM Watson for providing resources,\nfundings and valuable discussions to make this project possible, and Caglar Gulcehre for helpful\ndiscussions.\nREFERENCES\nYoshua Bengio, R \u00b4ejean Ducharme, and Pascal Vincent. A neural probabilistic language model. In\nAdvances in Neural Information Processing Systems , pp. 932\u2013938, 2001.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\ntated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326 , 2015.\nSamuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and\nChristopher Potts. A fast uni\ufb01ed model for parsing and sentence understanding. arXiv preprint\narXiv:1603.06021 , 2016.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . Asso-\nciation for Computational Linguistics, 2016.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.\n10", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a65709ff-3b33-4f8e-9cdc-44bb8a0be449": {"__data__": {"id_": "a65709ff-3b33-4f8e-9cdc-44bb8a0be449", "embedding": null, "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f00fb90-1605-4f75-9509-e5dda2c7db19", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "3a1cb78de522ffb503a1f88c121f93b5dee7bbb1bb1d64f110c0c0efce448668"}, "3": {"node_id": "a541e458-9efa-4027-9f76-d4123e9752d6", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "04a3e1f2ee14e57640dfcba6ab09eccce9398290f8f261c96539233aa94abf6b"}}, "hash": "d2d244f4d0eda72d96c6d5efc6d589a91b9802e13a0dfb73d6eb280b7f0380ea", "text": "Published as a conference paper at ICLR 2017\nCicero dos Santos and Maira Gatti. Deep convolutional neural networks for sentiment analysis of\nshort texts. In Proceedings of COLING 2014, the 25th International Conference on Computa-\ntional Linguistics: Technical Papers , pp. 69\u201378, 2014.\nCicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv\npreprint arXiv:1602.03609 , 2016.\nMinwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou. Applying deep learn-\ning to answer selection: a study and an open task. In 2015 IEEE Workshop on Automatic Speech\nRecognition and Understanding, ASRU 2015, Scottsdale, AZ, USA, December 13-17, 2015 , pp.\n813\u2013820, 2015.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies , pp. 1367\u2013\n1377, San Diego, California, June 2016. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/N16-1162 .\nSepp Hochreiter and J \u00a8urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n1735\u20131780, 1997.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for\nmodelling sentences. arXiv preprint arXiv:1404.2188 , 2014.\nYoon Kim. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint\narXiv:1408.5882 , 2014.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing\nsystems , pp. 3294\u20133302, 2015.\nQuoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML ,\nvolume 14, pp. 1188\u20131196, 2014.\nJi Young Lee and Franck Dernoncourt. Sequential short-text classi\ufb01cation with recurrent and con-\nvolutional neural networks. arXiv preprint arXiv:1603.03827 , 2016.\nPeng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. Dataset and neural\nrecurrent sequence labeling model for open-domain factoid question answering. arXiv preprint\narXiv:1607.06275 , 2016.\nWang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Silvio Amir. Not all contexts are created equal:\nBetter word representations with variable attention. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing , pp. 1367\u20131372, Lisbon, Portugal, Septem-\nber 2015. Association for Computational Linguistics.\nYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using\nbidirectional LSTM model and inner-attention. CoRR , abs/1605.09090, 2016a.\nYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using\nbidirectional lstm model and inner-attention. arXiv preprint arXiv:1605.09090 , 2016b.\nMingbo Ma, Liang Huang, Bing Xiang, and Bowen Zhou. Dependency-based convolutional neural\nnetworks for sentence embedding. In Proceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing , volume 2, pp. 174\u2013179, 2015.\nHoria Margarit and Raghav Subramaniam.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a541e458-9efa-4027-9f76-d4123e9752d6": {"__data__": {"id_": "a541e458-9efa-4027-9f76-d4123e9752d6", "embedding": null, "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f00fb90-1605-4f75-9509-e5dda2c7db19", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "3a1cb78de522ffb503a1f88c121f93b5dee7bbb1bb1d64f110c0c0efce448668"}, "2": {"node_id": "a65709ff-3b33-4f8e-9cdc-44bb8a0be449", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d2d244f4d0eda72d96c6d5efc6d589a91b9802e13a0dfb73d6eb280b7f0380ea"}}, "hash": "04a3e1f2ee14e57640dfcba6ab09eccce9398290f8f261c96539233aa94abf6b", "text": "Horia Margarit and Raghav Subramaniam. A batch-normalized recurrent network for sentiment\nclassi\ufb01cation. In Advances in Neural Information Processing Systems , 2016.\nRoland Memisevic. Learning to relate images. IEEE transactions on pattern analysis and machine\nintelligence , 35(8):1829\u20131846, 2013.\n11", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bc492931-4fe0-4d5c-87d4-a9ab193f12be": {"__data__": {"id_": "bc492931-4fe0-4d5c-87d4-a9ab193f12be", "embedding": null, "metadata": {"page_label": "12", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "154ed6bc-79e8-4bc9-8426-2480a3c0238c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fa896d8a12e1943930c965879201f5e57648325d502abbdf6496790ba76be64d"}}, "hash": "fa896d8a12e1943930c965879201f5e57648325d502abbdf6496790ba76be64d", "text": "Published as a conference paper at ICLR 2017\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word represen-\ntations in vector space. arXiv preprint arXiv:1301.3781 , 2013.\nLili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. Natural language inference by\ntree-based convolution and heuristic matching. arXiv preprint arXiv:1512.08422 , 2015a.\nLili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. Discriminative neural sentence model-\ning by tree-based convolution. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing , pp. 2315\u20132325, Lisbon, Portugal, September 2015b. Association\nfor Computational Linguistics. URL http://aclweb.org/anthology/D15-1279 .\nTsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding. arXiv preprint\narXiv:1607.04492 , 2016a.\nTsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. arXiv preprint arXiv:1607.04315 ,\n2016b.\nHamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song,\nand Rabab Ward. Deep sentence embedding using long short-term memory networks: Analysis\nand application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(4):694\u2013707, 2016.\nAnkur P. Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. In Proceedings of EMNLP , 2016.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In EMNLP , volume 14, pp. 1532\u201343, 2014.\nRichard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y . Ng, and Christopher D. Manning.\nSemi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings\nof the 2011 Conference on Empirical Methods in Natural Language Processing , pp. 151\u2013161,\nEdinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/D11-1014 .\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the conference on empirical methods in natural language processing\n(EMNLP) , volume 1631, pp. 1642. Citeseer, 2013.\nKai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations\nfrom tree-structured long short-term memory networks. In Proceedings of ACL , pp. 1556\u20131566,\n2015.\nMing Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. Improved representation learning for\nquestion answer matching. In Proceedings of ACL , pp. 464\u2013473, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\nP16-1044 .\nTheano Development Team. Theano: A fPythongframework for fast computation of mathemati-\ncal expressions. arXiv e-prints , abs/1605.0, 2016. URL http://arxiv.org/abs/1605.\n02688 .\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and\nlanguage. arXiv preprint arXiv:1511.06361 , 2015.\nWenpeng Yin and Hinrich Sch \u00a8utze. Convolutional neural network for paraphrase identi\ufb01cation.\nInProceedings of the 2015 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies , pp. 901\u2013911, 2015.\n12", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1cd43094-43e0-4ba5-817f-d4145d585e6b": {"__data__": {"id_": "1cd43094-43e0-4ba5-817f-d4145d585e6b", "embedding": null, "metadata": {"page_label": "13", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e9a8331-517d-4453-97f5-037b8d82d3d7", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "94ad38df10a01b87f94123b22f5f0ff958f612e76b3fe5aeae357f1ee20cf667"}}, "hash": "94ad38df10a01b87f94123b22f5f0ff958f612e76b3fe5aeae357f1ee20cf667", "text": "Published as a conference paper at ICLR 2017\nAPPENDIX\nA P RUNED MLP FOR STRUCTURED MATRIX SENTENCE EMBEDDING\nAs a side effect of having multiple vectors to represent a sentence, the matrix sentence embedding\nis usually several times larger than vector sentence embeddings. This results in needing more pa-\nrameters in the subsequent fully connected layer, which connects every hidden units to every units\nin the matrix sentence embedding. Actually in the example shown in Figure 1, this fully connected\nlayer takes around 90% percent of the parameters. See Table 4. In this appendix we are going to\nintroduce a weight pruning method which, by utilizing the 2D structure of matrix embedding, is able\nto drastically reduce the number of parameters in the fully connected hidden layer.\nInheriting the notation used in the main paper, let the matrix embedding Mhas a shape of rbyu,\nand let the fully connected hidden layer has bunits. The normal fully connected hidden layer will\nrequire each hidden unit to be connected to every unit in the matrix embedding, as shown in Figure\n1. This ends up with r\u0002u\u0002bparameters in total.\nHowever there are 2-D structures in the matrix embedding, which we should make use of. Each\nrow (miin Figure 1) in the matrix is computed from a weighted sum of LSTM hidden states, which\nmeans they share some similarities\nTo re\ufb02ect these similarity in the fully connected layer, we split the hidden states into requally sized\ngroups, with each group having punits. The i-th group is only fully connected to the i-th row in\nthe matrix representation. All connections that connects the i-th group hidden units to other rows\nof the matrix are pruned away. In this way, Simillarity between different rows of matrix embedding\nare re\ufb02ected as symmetry of connecting type in the hidden layer. As a result, the hidden layer can\nbe interperated as also having a 2-D structute, with the number ( r) and size ( p) of groups as its\ntwo dimensions (The Mvin Figure 6). When the total number of hidden units are the same (i.e.,\nMm1m2miMh Mv\nuq\np\nrru\nFigure 6: Hidden layer with pruned weight connections. Mis the matrix sentence embedding, Mv\nandMhare the structured hidden representation computed by pruned weights.\nTable 4: Model Size Comparison Before and After Pruning\nHidden layer Softmax Other Parts Total Accuracy\nYelp, Original, b=3000 54M 15K 1.3M 55.3M 64.21%\nYelp, Pruned, p=150, q=10 2.7M 52.5K 1.3M 4.1M 63.86%\nAge, Original, b=4000 72M 20K 1.3M 73.2M 80.45%\nAge, Pruned, p=25,q=20 822K 63.75K 1.3M 2.1M 77.32%\nSNLI, Original, b=4000 72M 12K 22.9M 95.0M 84.43%\nSNLI, Pruned, p=300, q=10 5.6M 45K 22.9M 28.6M 83.16%\n13", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b3c9600a-369d-456c-b8b8-679f00fecd55": {"__data__": {"id_": "b3c9600a-369d-456c-b8b8-679f00fecd55", "embedding": null, "metadata": {"page_label": "14", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e1eb5a1-f9db-4fb3-bb17-613d176c030b", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "f49b20544135b08a7da459581c6bedfd71c96d600aefc60dc0d70d89798013aa"}}, "hash": "f49b20544135b08a7da459581c6bedfd71c96d600aefc60dc0d70d89798013aa", "text": "Published as a conference paper at ICLR 2017\nr\u0002p=b), this process prunes away (r\u00001)=rof weight values, which is a fairly large portion when\nris large.\nOn the other dimension, another form of similarity exists too. For each vector representation miin\nM, thej-th element mijis a weighted sum of an LSTM hidden unit at different time steps. And for\na certain j-th element in all vector representations, they are summed up from a same LSTM hidden\nunit. We can also re\ufb02ect this similarity into the symmetry of weight connections by using the same\npruning method we did above. Thus we will have another 2-D structured hidden states sized u-by-q,\nnoted as Mhin Figure 6.\nTable 4 takes the model we use for yelp dataset as a concrete example, and compared the number of\nparameters in each part of the model, both before and after pruning. We can see the above pruning\nmethod drastically reduces the model size. Note that the pandqin this structure can be adjusted\nfreely as hyperparameters. Also, we can continue the corresponding pruning process on top of Mv\nandMhover and over again, and end up with having a stack of structured hidden layers, just like\nstacking fully connected layers.\nThe subsequent softmax layer will be fully connected to both MvandMh, i.e., each unit in the\nsoftmax layer is connected to all units in MvandMh. This is not a problem since the speed of\nsoftmax is largely dependent of the number of softmax units, which is not changed.In addition, for\napplications like sentiment analysis and textural entailment, the softmax layer is so tiny that only\ncontains several units.\nExperimental results in the three datasets has shown that, this pruning mechanism lowers perfor-\nmances a bit, but still allows all three models to perform comparable or better than other models\ncompared in the paper.\nB D ETAILED STRUCTURE OF THE MODEL FOR SNLI D ATASET\nIn Section 4.3 we tested our matrix sentence embedding model for the textual entailment task on\nthe SNLI dataset. Different from the former two tasks, the textual entailment task consists of a pair\nof sentences as input. We propose to use a set of multiplicative interactions to combine the two\n.........\nw1 w2 w3 wn w4Mh\n... ... w1 w2 w3 wn w4Mp\n... ...Fh FpGated Encoder\nHypothesis Premise Fr\nFigure 7: Model structure used for textual entailment task.\n14", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3fbb6995-dc85-435f-8fe5-2f790713c197": {"__data__": {"id_": "3fbb6995-dc85-435f-8fe5-2f790713c197", "embedding": null, "metadata": {"page_label": "15", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44ee76b9-bf15-4cc8-95fd-aa2265dbf3ce", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "2e18f2b632a681f880ae0a76bb490ab7cada3ffc7e6c649904a2af91e3049450"}}, "hash": "2e18f2b632a681f880ae0a76bb490ab7cada3ffc7e6c649904a2af91e3049450", "text": "Published as a conference paper at ICLR 2017\nmatrix embeddings extracted for each sentence. The form of multiplicative interaction is inspired\nbyFactored Gated Autoencoder (Memisevic, 2013).\nThe overall structure of our model for SNLI is dipicted in Figure 7. For both hypothesis and premise,\nwe extract their embeddings ( MhandMpin the \ufb01gure) independently, with a same LSTM and\nattention mechanism. The parameters of this part of model are shared (rectangles with dashed orange\nline in the \ufb01gure).\nComparing the two matrix embeddings corresponds to the green dashed rectangle part in the \ufb01gure,\nwhich computes a single matrix embedding ( Fr) as the factor of semantic relation between the two\nsentences. To represent the relation between MhandMp,Frcan be connected to MhandMp\nthrough a three-way multiplicative interaction . In a three-way multiplicative interaction, the value\nof anyone of Fr,MhandMpis a function of the product of the others. This type of connection is\noriginally introduced to extract relation between images (Memisevic, 2013). Since here we are just\ncomputing the factor of relations ( Fr) from MhandMp, it corresponds to the encoder part in the\nFactored Gated Autoencoder in Memisevic (2013). We call it Gated Encoder in Figure 7.\nFirst we multiply each row in the matrix embedding by a different weight matrix. Repeating it\nover all rows, corresponds to a batched dot product between a 2-D matrix and a 3-D weight tensor.\nInheriting the name in (Memisevic, 2013), we call the resulting matrix as factor . Doing the batched\ndot for both hypothesis embedding and premise embedding, we have FhandFp, respectively.\nFh=batcheddot (Mh; Wfh) (10)\nFp=batcheddot (Mp; Wfp) (11)\nHereWfhandWfpare the two weight tensors for hypothesis embedding and premise embedding.\nThe factor of the relation ( Fr) is just an element-wise product of FhandFp(the triangle in the\nmiddle of Figure 7):\nFr=Fh\fFp (12)\nHere\fstands for element-wise product. After the Frlayer, we then use an MLP with softmax\noutput to classify the relation into different categlories.\n15", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0cf5db57-0271-4cf3-b67d-7f6eff1d9cf2": {"__data__": {"id_": "0cf5db57-0271-4cf3-b67d-7f6eff1d9cf2", "embedding": null, "metadata": {"page_label": "1", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e371c83-9b2f-4a26-89e0-c6697ab820d8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ebab343c445163f12ef3bb159f51b0d2977d82a1c6a6ddcf20ada77f25364835"}}, "hash": "ebab343c445163f12ef3bb159f51b0d2977d82a1c6a6ddcf20ada77f25364835", "text": "Workshop track - ICLR 2017\nFACTORIZATION TRICKS FOR LSTM NETWORKS\nOleksii Kuchaiev\nNVIDIA\nokuchaiev@nvidia.comBoris Ginsburg\nNVIDIA\nbginsburg@nvidia.com\nABSTRACT\nWe present two simple ways of reducing the number of parameters and acceler-\nating the training of large Long Short-Term Memory (LSTM) networks: the \ufb01rst\none is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two\nsmaller matrices, and the second one is partitioning of LSTM matrix, its inputs\nand states into the independent groups. Both approaches allow us to train large\nLSTM networks signi\ufb01cantly faster to the near state-of the art perplexity while\nusing signi\ufb01cantly less RNN parameters.\n1 I NTRODUCTION\nLSTM networks (Hochreiter & Schmidhuber, 1997) have been successfully used in language model-\ning (Jozefowicz et al., 2016; Shazeer et al., 2017), speech recognition (Xiong et al., 2016), machine\ntranslation (Wu et al., 2016), and many other tasks. However, these networks have millions of\nparameters, and require weeks of training on multi-GPU systems.\nWe introduce two modi\ufb01cations of LSTM cell with projection, LSTMP (Sak et al., 2014), to reduce\nthe number of parameters and speed-up training. The \ufb01rst method, factorized LSTM (F-LSTM)\napproximates big LSTM matrix with a product of two smaller matrices. The second method, group\nLSTM (G-LSTM) partitions LSTM cell into the independent groups. We test F-LSTM and G-LSTM\narchitectures on the task of language modeling using One Billion Word Benchmark (Chelba et al.,\n2013). As a baseline, we used BIGLSTM model without CNN inputs described by Jozefowicz et al.\n(2016). We train all networks for 1 week on a DGX Station system with 4 Tesla V100 GPUs, after\nwhich BIGLSTM\u2019s evaluation perplexity was 35.1. Our G-LSTM based model got 36 and F-LSTM\nbased model got 36.3 while using two to three times less RNN parameters.\n1.1 L ONG SHORT -TERM MEMORY OVERVIEW\nLearning long-range dependencies with Recurrent Neural Networks (RNN) is challenging due to\nthe vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013). To ad-\ndress this issue, the LSTM cell has been introduced by Hochreiter & Schmidhuber (1997), with the\nfollowing recurrent computations:\nLSTM :ht\u00001; ct\u00001; xt!ht; ct: (1)\nwhere xtis input, htis cell\u2019s state, and ctis cell\u2019s memory. We consider LSTM cell with projection\nof size p, LSTMP, where Equation 1 is computed as follows (Sak et al., 2014; Zaremba et al., 2014).\nFirst, cell gates (i; f; o; g )are computed:\n0\nB@i\nf\no\ng1\nCA=0\nB@sigm\nsigm\nsigm\ntanh1\nCAT\u0012\nxt\nht\u00001\u0013\n(2)\nwhere xt2Rp,ht2Rp, andT:R2p!R4nis an af\ufb01ne transform T=W\u0003[xt; ht\u00001] +b.\nNext state ht2Rpand memory ct2Rnare computed using following equations:\nct=f\fct\u00001+i\fg;ht=P(o\ftanh(ct))\nwhere P:Rn!Rpis a linear projection. The major part of LSTMP cell computation is in\ncomputing af\ufb01ne transform Tbecause it involves multiplication with 4n\u00022pmatrix W. Thus we\nfocus on reducing the number of parameters in W.\n1arXiv:1703.10722v3  [cs.CL]  24 Feb 2018", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d8bb20d-b87b-4ad1-9b35-af47cff055dd": {"__data__": {"id_": "2d8bb20d-b87b-4ad1-9b35-af47cff055dd", "embedding": null, "metadata": {"page_label": "2", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "203af889-47e6-413d-b785-fcfee2a17832", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "f7133e919e4af4cd108b2736856890bef924871ec16e1b51631bdd205caa6e0a"}}, "hash": "f7133e919e4af4cd108b2736856890bef924871ec16e1b51631bdd205caa6e0a", "text": "Workshop track - ICLR 2017\n1.2 R ELATED WORK\nThe partition of layer into parallel groups have been introduced by Krizhevsky et al. (2012) in\nAlexNet, where some convolutional layers have been divided into two groups to split the model\nbetween two GPUs. Multi-group convnets have been widely used to reduce network weights and\nrequired compute, for example by Esser et al. (2016). This multi-group approach was extended to the\nextreme in Xception architecture by Chollet (2016). The idea of factorization of large convolutinal\nlayer into the stack of layers with smaller \ufb01lters was used, for example, in VGG networks (Simonyan\n& Zisserman, 2014), and in ResNet \u201cbottleneck design\u201d (He et al., 2016). Denil et al. (2013) have\nshown that it is possible to train several different deep architectures by learning only a small number\nof weights and predicting the rest. In case of LSTM networks, ConvLSTM (Shi et al., 2015), has\nbeen introduced to better exploit possible spatiotemporal correlations, which is conceptually similar\nto grouping.\n2 M ODELS\n2.1 F ACTORIZED LSTM CELL\nFactorized LSTM (F-LSTM) replaces matrix Wby the product of two smaller matrices that essen-\ntially try to approximate WasW\u0019W2\u0003W1, where W1is of size 2p\u0002r,W2isr\u00024n, and\nr < p < =n(\u201dfactorization by design\u201d). The key assumption here is that Wcan be well approxi-\nmated by the matrix of rank r. Such approximation contains less LSTMP parameters than original\nmodel - (r\u00032p+r\u00034n)versus (2p\u00034n)and, therefore, can be computed faster and synchronized\nfaster in the case of distributed training.\nFigure 1: Language model using: (a) 2 regular LSTM layers, (b) 2 F-LSTM layers, and (c) 2 G-\nLSTM layers with 2 group in each layer. Equations inside cells show what kind of af\ufb01ne transforms\nare computed by those cells at each time step. Here d= (x; h)for models without groups and\nd1 = (x1; h1),d2 = (x2; h2)for model with two groups; and time index dropped for clarity.\n2.2 G ROUP LSTM CELL\nThis approach is inspired by groups in Alexnet (Krizhevsky et al., 2012). We postulate that some\nparts of the input xtand hidden state htcan be thought of as independent feature groups. For\nexample, if we use two groups, then both xtandhtare effectively split into two vectors concatenated\ntogether xt= (x1\nt; x2\nt)andht= (h1\nt; h2\nt), with hi\ntonly dependent on xi\nt,hi\nt\u00001and cell\u2019s memory\nstate. Therefore, for kgroups Equation 2 changes to:\n0\nB@i\nf\no\ng1\nCA=0\nB@0\nB@sigm\nsigm\nsigm\ntanh1\nCAT1\u0012\nx1\nt\nh1\nt\u00001\u0013\n; :::;0\nB@sigm\nsigm\nsigm\ntanh1\nCATk\u0012\nxk\nt\nhk\nt\u00001\u00131\nCA (3)\nwhere, Tjis a group j\u2019s af\ufb01ne transform from R2p=ktoR4n=k. The partitioned Twill now have\nk\u00034n\u00032p\nk\u0003kparameters. This cell architecture is well suited for model parallelism since every group\ncomputation is independent. An alternative interpretation of G-LSTM layers is demonstrated in\n2", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e30d4ba2-7e62-4193-ab40-0ca19977cae5": {"__data__": {"id_": "e30d4ba2-7e62-4193-ab40-0ca19977cae5", "embedding": null, "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "074d81c9-eb70-40c1-9099-2d5eaa51fc93", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "eeff765b9ff755b84034d05db37da59eef591f45f2f216f999bce9f5700288bc"}, "3": {"node_id": "b8ead17b-639b-4db2-b46e-d74e26b71cb3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "1a89a93b1374b06cc50721f11a9cc0c9412cde18bb4abea32748b68d4ebda790"}}, "hash": "0cebd1f9335aef0ed3d14a8a3965d2bce794547d862160d4b2275f5b060aaf07", "text": "Workshop track - ICLR 2017\nTable 1: One Billion Words benchmark evaluation results after 1 week of training using one DGX\nStation with 4 Tesla V100 GPUs.\nModel Perplexity Step Num of RNN parameters Words/sec\nBIGLSTM baseline 35.1 0.99M 151,060,480 33.8K\nBIG F-LSTM F512 36.3 1.67 M 52,494,336 56.5K\nBIG G-LSTM G-2 36 1.37M 83,951,616 41.7K\nBIG G-LSTM G-4 40.6 1.128M 50,397,184 56K\nBIG G-LSTM G-8 39.4 850.4K 33,619,968 58.5K\nthe Figure 1 (c). While this might look similar to ensemble (Shazeer et al., 2017) or multi-tower\n(Ciregan et al., 2012) models, the key differences are: (1) input to different groups is different\nand assumed independent, and (2) instead of computing ensemble output, it is concatenated into\nindependent pieces.\n3 E XPERIMENTS AND RESULTS\nFor testing we used the task of learning the joint probabilities over word sequences of arbitrary\nlengths n:P(w1; :::; w n) =Qn\ni=1P(wijw1; :::; w i\u00001), such that \u201creal\u201d sentences have high prob-\nabilities compared to the random sequences of words. Figure 1 (a) shows the typical LSTM-based\nmodel, where \ufb01rst the words are embedded into the low dimensional dense input for RNN, then the\n\u201ccontext\u201d is learned using RNNs via number of steps and, \ufb01nally, the softmax layer converts RNN\noutput into the probability distribution P(w1; :::; w n). We test the following models:\n\u000fBIGLSTM - model with projections but without CNN inputs from Jozefowicz et al. (2016)\n\u000fBIG F-LSTM F512 - with intermediate rank of 512 for LSTM matrix W,\n\u000fBIG G-LSTM G-4, with 4 groups in both layers\n\u000fBIG G-LSTM G-16, with 16 groups in both layers.\nWe train all models on DGX Station with 4 GV100 GPUs for one ween using Adagrad optimizer,\nprojection size of 1024, cell size of 8192, mini-batch of 256 per GPU, sampled softmax with 8192\nsamples and 0.2 learning rate. Note that the use of projection is crucial as it helps to keep down\nembedding and softmax layer sizes. Table 1 summarizes our experiments.\nJudging from the training loss Plots 2 in Appendix , it is clearly visible that at the same step count,\nmodel with more parameters wins. However, given the same amount of time, factorized models\ntrain faster. While the difference between BIGLSTM and BIG G-LSTM-G2 is clearly visible, BIG\nG-LSTM-G2 contains almost 2 times less RNN parameters than BIGLSTM, trains faster and, as a\nresults, achieves similar evaluation perplexity within the same training time budget (1 week).\nOur code is available at https://github.com/okuchaiev/f-lm\n3.1 F UTURE RESEARCH\nWhile one might go further and try to approximate transform Tusing arbitrary feed forward neural\nnetwork with 2pinputs and 4noutputs, during our initial experiments we did not see immediate\nbene\ufb01ts of doing so. Hence, it remains a topic of future research.\nIt might be possible to reduce the number of RNN parameters even further by stacking G-LSTM\nlayers with increasing group counts on top of each other. In our second, smaller experiment, we\nreplace the second layer of BIG G-LSTM-G4 network by the layer with 8 groups instead of 4, and\ncall it BIG G-LSTM-G4-G8. We let both BIG G-LSTM-G4 and BIG G-LSTM-G4-G8 ran for 1\nweek on 4 GPUs each and achieved very similar perplexities. Hence, the model with \u201chierarchical\u201d\ngroups did not lose much accuracy, ran faster and got better perplexity.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8ead17b-639b-4db2-b46e-d74e26b71cb3": {"__data__": {"id_": "b8ead17b-639b-4db2-b46e-d74e26b71cb3", "embedding": null, "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "074d81c9-eb70-40c1-9099-2d5eaa51fc93", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "eeff765b9ff755b84034d05db37da59eef591f45f2f216f999bce9f5700288bc"}, "2": {"node_id": "e30d4ba2-7e62-4193-ab40-0ca19977cae5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0cebd1f9335aef0ed3d14a8a3965d2bce794547d862160d4b2275f5b060aaf07"}}, "hash": "1a89a93b1374b06cc50721f11a9cc0c9412cde18bb4abea32748b68d4ebda790", "text": "Such \u201chierarchical\u201d group\nlayers look intriguing as they might provide a way for learning different levels of abstractions but\nthis remains a topic of future research.\n3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "273bc17e-2ea4-49da-af39-40cfc92f1752": {"__data__": {"id_": "273bc17e-2ea4-49da-af39-40cfc92f1752", "embedding": null, "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "963338af-f8c1-4291-a9b0-4686a6eb7f94", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "575ba4cefcf0b71a6ef2bfbd076c55977fdff82d67e50fddbe0b70d7eebfa589"}, "3": {"node_id": "8081d5a9-f1a3-453a-bb77-1a7a6343f332", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4de27ff195ba831dd10bcf94c6dac3216ef9a58f5c880410321819afc12cecde"}}, "hash": "02bcee9c2c341459ec80f9787cec19c5dc2d86bd892d48b436e4cab85a6d2a33", "text": "Workshop track - ICLR 2017\nAcknowledgements We are grateful to Scott Gray and Ciprian Chelba for helping us identify and\ncorrect issues with earlier versions of this work.\nREFERENCES\nYoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient\ndescent is dif\ufb01cult. IEEE transactions on neural networks , 5(2):157\u2013166, 1994.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005 , 2013.\nFranc \u00b8ois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint\narXiv:1610.02357 , 2016.\nDan Ciregan, Ueli Meier, and J \u00a8urgen Schmidhuber. Multi-column deep neural networks for image\nclassi\ufb01cation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on ,\npp. 3642\u20133649. IEEE, 2012.\nMisha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al. Predicting parameters in deep\nlearning. In Advances in Neural Information Processing Systems , pp. 2148\u20132156, 2013.\nSteven K Esser, Paul A Merolla, John V Arthur, Andrew S Cassidy, Rathinakumar Appuswamy,\nAlexander Andreopoulos, David J Berg, Jeffrey L McKinstry, Timothy Melano, Davis R Barch,\net al. Convolutional networks for fast, energy-ef\ufb01cient neuromorphic computing. Proceedings of\nthe National Academy of Sciences , pp. 201604850, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016.\nSepp Hochreiter and J \u00a8urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n1735\u20131780, 1997.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In Advances in neural information processing systems , pp. 1097\u20131105,\n2012.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the dif\ufb01culty of training recurrent neural\nnetworks. ICML (3) , 28:1310\u20131318, 2013.\nHasim Sak, Andrew W Senior, and Franc \u00b8oise Beaufays. Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic modeling. In Interspeech , pp. 338\u2013342, 2014.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538 , 2017.\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo.\nConvolutional lstm network: A machine learning approach for precipitation nowcasting. In\nProceedings of the 28th International Conference on Neural Information Processing Systems ,\nNIPS\u201915, pp. 802\u2013810, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.\norg/citation.cfm?id=2969239.2969329 .\nKaren Simonyan and Andrew Zisserman.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8081d5a9-f1a3-453a-bb77-1a7a6343f332": {"__data__": {"id_": "8081d5a9-f1a3-453a-bb77-1a7a6343f332", "embedding": null, "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "963338af-f8c1-4291-a9b0-4686a6eb7f94", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "575ba4cefcf0b71a6ef2bfbd076c55977fdff82d67e50fddbe0b70d7eebfa589"}, "2": {"node_id": "273bc17e-2ea4-49da-af39-40cfc92f1752", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "02bcee9c2c341459ec80f9787cec19c5dc2d86bd892d48b436e4cab85a6d2a33"}}, "hash": "4de27ff195ba831dd10bcf94c6dac3216ef9a58f5c880410321819afc12cecde", "text": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n4", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4dac1fdc-d778-4658-b30c-4f48a7b3eb4c": {"__data__": {"id_": "4dac1fdc-d778-4658-b30c-4f48a7b3eb4c", "embedding": null, "metadata": {"page_label": "5", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab286212-c22d-499d-8d31-60d0a2af8264", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ad9996d3ecd4fd4de5e7b580df5fb680bc5114fef0725c439958482c6a8fbac4"}}, "hash": "ad9996d3ecd4fd4de5e7b580df5fb680bc5114fef0725c439958482c6a8fbac4", "text": "Workshop track - ICLR 2017\nWayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong\nYu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv\npreprint arXiv:1610.05256 , 2016.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329 , 2014.\n5", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "09ad95d7-15a5-4ea4-b423-fbdecc0a8585": {"__data__": {"id_": "09ad95d7-15a5-4ea4-b423-fbdecc0a8585", "embedding": null, "metadata": {"page_label": "6", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ceb5ddd-d825-40cb-b26d-cdbe59a738f6", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "820024c671770c3efa2a98755b921abb22fdf7a88922ac424916837c5283b2e1"}}, "hash": "820024c671770c3efa2a98755b921abb22fdf7a88922ac424916837c5283b2e1", "text": "Workshop track - ICLR 2017\nAPPENDIX : TRAINING LOSS FOR 4 LSTM- LIKE MODELS\nFigure 2: Y-axis: same for (A) and (B) - training loss log-scale, X-axis: for (A) - step, or mini-batch\ncount, for (B) - hours (w.g. wall time) of training. BIGLSTM baseline, BIG G-LSTM-G4, BIG\nG-LSTM-G16, and BIG F-LSTM-F512 all trained for exactly one week. It is clearly visible, that at\nthe same step count, the model with more parameters wins. On the other hand, factorized models\ncan do signi\ufb01cantly more iterations in the given amount of time and therefore get to the better results\ngiven same amount of time. (full extent of X-axis for both (A) and (B) is 1 week).\n6", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9497ee67-f2e1-406e-a923-8b6e55fc8775": {"__data__": {"id_": "9497ee67-f2e1-406e-a923-8b6e55fc8775", "embedding": null, "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4e6f2f1-1603-4c5c-96fd-71b098f988db", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "88722359a816f8540ab1a4f78f21c9576465c73ea01e8bd64b0bc16ab4cd5f3b"}, "3": {"node_id": "c628c85a-5f1c-495b-b97e-22608f93d7a1", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "69e8776e4a4f9eb09918a3c2019da63ada92f123a577b7fbb93a48fd5b3c4a7d"}}, "hash": "2456de6b25e1ec6be0fdb6c61ea7b6ea3ad76aed4ae37869a066993b3d03ad0c", "text": "Convolutional Sequence to Sequence Learning\nJonas Gehring\nMichael Auli\nDavid Grangier\nDenis Yarats\nYann N. Dauphin\nFacebook AI Research\nAbstract\nThe prevalent approach to sequence to sequence\nlearning maps an input sequence to a variable\nlength output sequence via recurrent neural net-\nworks. We introduce an architecture based en-\ntirely on convolutional neural networks.1Com-\npared to recurrent models, computations over all\nelements can be fully parallelized during training\nto better exploit the GPU hardware and optimiza-\ntion is easier since the number of non-linearities\nis \ufb01xed and independent of the input length. Our\nuse of gated linear units eases gradient propaga-\ntion and we equip each decoder layer with a sep-\narate attention module. We outperform the accu-\nracy of the deep LSTM setup of Wu et al. (2016)\non both WMT\u201914 English-German and WMT\u201914\nEnglish-French translation at an order of magni-\ntude faster speed, both on GPU and CPU.\n1. Introduction\nSequence to sequence learning has been successful in\nmany tasks such as machine translation, speech recogni-\ntion (Sutskever et al., 2014; Chorowski et al., 2015) and\ntext summarization (Rush et al., 2015; Nallapati et al.,\n2016; Shen et al., 2016) amongst others. The dominant\napproach to date encodes the input sequence with a se-\nries of bi-directional recurrent neural networks (RNN) and\ngenerates a variable length output with another set of de-\ncoder RNNs, both of which interface via a soft-attention\nmechanism (Bahdanau et al., 2014; Luong et al., 2015).\nIn machine translation, this architecture has been demon-\nstrated to outperform traditional phrase-based models by\nlarge margins (Sennrich et al., 2016b; Zhou et al., 2016;\nWu et al., 2016;\u00a72).\n1The source code and models are available at https://\ngithub.com/facebookresearch/fairseq .Convolutional neural networks are less common for se-\nquence modeling, despite several advantages (Waibel et al.,\n1989; LeCun & Bengio, 1995). Compared to recurrent lay-\ners, convolutions create representations for \ufb01xed size con-\ntexts, however, the effective context size of the network can\neasily be made larger by stacking several layers on top of\neach other. This allows to precisely control the maximum\nlength of dependencies to be modeled. Convolutional net-\nworks do not depend on the computations of the previous\ntime step and therefore allow parallelization over every ele-\nment in a sequence. This contrasts with RNNs which main-\ntain a hidden state of the entire past that prevents parallel\ncomputation within a sequence.\nMulti-layer convolutional neural networks create hierarchi-\ncal representations over the input sequence in which nearby\ninput elements interact at lower layers while distant ele-\nments interact at higher layers. Hierarchical structure pro-\nvides a shorter path to capture long-range dependencies\ncompared to the chain structure modeled by recurrent net-\nworks, e.g. we can obtain a feature representation captur-\ning relationships within a window of nwords by applying\nonlyO(n\nk)convolutional operations for kernels of width\nk, compared to a linear number O(n)for recurrent neu-\nral networks. Inputs to a convolutional network are fed\nthrough a constant number of kernels and non-linearities,\nwhereas recurrent networks apply up to noperations and\nnon-linearities to the \ufb01rst word and only a single set of\noperations to the last word. Fixing the number of non-\nlinearities applied to the inputs also eases learning.\nRecent work has applied convolutional neural networks to\nsequence modeling such as Bradbury et al. (2016) who in-\ntroduce recurrent pooling between a succession of convo-\nlutional layers or Kalchbrenner et al. (2016) who tackle\nneural translation without attention. However, none of\nthese approaches has been demonstrated improvements\nover state of the art results on large benchmark datasets.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c628c85a-5f1c-495b-b97e-22608f93d7a1": {"__data__": {"id_": "c628c85a-5f1c-495b-b97e-22608f93d7a1", "embedding": null, "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4e6f2f1-1603-4c5c-96fd-71b098f988db", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "88722359a816f8540ab1a4f78f21c9576465c73ea01e8bd64b0bc16ab4cd5f3b"}, "2": {"node_id": "9497ee67-f2e1-406e-a923-8b6e55fc8775", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "2456de6b25e1ec6be0fdb6c61ea7b6ea3ad76aed4ae37869a066993b3d03ad0c"}}, "hash": "69e8776e4a4f9eb09918a3c2019da63ada92f123a577b7fbb93a48fd5b3c4a7d", "text": "Gated convolutions have been previously explored for ma-\nchine translation by Meng et al. (2015) but their evaluation\nwas restricted to a small dataset and the model was used\nin tandem with a traditional count-based model. Architec-\n1arXiv:1705.03122v3  [cs.CL]  25 Jul 2017", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e32034af-9926-49f8-8023-e4a452ec43c6": {"__data__": {"id_": "e32034af-9926-49f8-8023-e4a452ec43c6", "embedding": null, "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "076a8af5-e36e-4070-9ae0-3d014e547de1", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9c03fed300ffe9555d06afef33e9387cb1e35a79e333134146058504056bde2e"}, "3": {"node_id": "297dbeac-874a-4aa1-a9de-1ed25528bea1", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e60f08f9483056075c0d2c3d652c97c8c909fbbf4e04f964b6975e633fa17682"}}, "hash": "7daee48782547bc567d09c786d4e0cfab4c4592f974eb1d4aabe108a78a4ece9", "text": "Convolutional Sequence to Sequence Learning\ntures which are partially convolutional have shown strong\nperformance on larger tasks but their decoder is still recur-\nrent (Gehring et al., 2016).\nIn this paper we propose an architecture for sequence to se-\nquence modeling that is entirely convolutional. Our model\nis equipped with gated linear units (Dauphin et al., 2016)\nand residual connections (He et al., 2015a). We also use\nattention in every decoder layer and demonstrate that each\nattention layer only adds a negligible amount of overhead.\nThe combination of these choices enables us to tackle large\nscale problems (\u00a73).\nWe evaluate our approach on several large datasets for ma-\nchine translation as well as summarization and compare to\nthe current best architectures reported in the literature. On\nWMT\u201916 English-Romanian translation we achieve a new\nstate of the art, outperforming the previous best result by\n1.9 BLEU. On WMT\u201914 English-German we outperform\nthe strong LSTM setup of Wu et al. (2016) by 0.5 BLEU\nand on WMT\u201914 English-French we outperform the like-\nlihood trained system of Wu et al. (2016) by 1.6 BLEU.\nFurthermore, our model can translate unseen sentences at\nan order of magnitude faster speed than Wu et al. (2016)\non GPU and CPU hardware ( \u00a74,\u00a75).\n2. Recurrent Sequence to Sequence Learning\nSequence to sequence modeling has been synonymous\nwith recurrent neural network based encoder-decoder ar-\nchitectures (Sutskever et al., 2014; Bahdanau et al., 2014).\nThe encoder RNN processes an input sequence x=\n(x1,...,xm)ofmelements and returns state representa-\ntions z= (z1....,zm). The decoder RNN takes zand\ngenerates the output sequence y= (y1,...,yn)left to\nright, one element at a time. To generate output yi+1, the\ndecoder computes a new hidden state hi+1based on the\nprevious state hi, an embedding giof the previous target\nlanguage word yi, as well as a conditional input ciderived\nfrom the encoder output z. Based on this generic formula-\ntion, various encoder-decoder architectures have been pro-\nposed, which differ mainly in the conditional input and the\ntype of RNN.\nModels without attention consider only the \ufb01nal encoder\nstatezmby settingci=zmfor alli(Cho et al., 2014), or\nsimply initialize the \ufb01rst decoder state with zm(Sutskever\net al., 2014), in which case ciis not used. Architectures\nwith attention (Bahdanau et al., 2014; Luong et al., 2015)\ncomputecias a weighted sum of (z1....,zm)at each time\nstep. The weights of the sum are referred to as attention\nscores and allow the network to focus on different parts of\nthe input sequence as it generates the output sequences. At-\ntention scores are computed by essentially comparing each\nencoder state zjto a combination of the previous decoderstatehiand the last prediction yi; the result is normalized\nto be a distribution over input elements.\nPopular choices for recurrent networks in encoder-decoder\nmodels are long short term memory networks (LSTM;\nHochreiter & Schmidhuber, 1997) and gated recurrent units\n(GRU; Cho et al., 2014). Both extend Elman RNNs (El-\nman, 1990) with a gating mechanism that allows the mem-\norization of information from previous time steps in order\nto model long-term dependencies. Most recent approaches\nalso rely on bi-directional encoders to build representations\nof both past and future contexts (Bahdanau et al., 2014;\nZhou et al., 2016; Wu et al., 2016). Models with many lay-\ners often rely on shortcut or residual connections (He et al.,\n2015a; Zhou et al., 2016; Wu et al., 2016).\n3. A Convolutional Architecture\nNext we introduce a fully convolutional architecture for se-\nquence to sequence modeling.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "297dbeac-874a-4aa1-a9de-1ed25528bea1": {"__data__": {"id_": "297dbeac-874a-4aa1-a9de-1ed25528bea1", "embedding": null, "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "076a8af5-e36e-4070-9ae0-3d014e547de1", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "9c03fed300ffe9555d06afef33e9387cb1e35a79e333134146058504056bde2e"}, "2": {"node_id": "e32034af-9926-49f8-8023-e4a452ec43c6", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7daee48782547bc567d09c786d4e0cfab4c4592f974eb1d4aabe108a78a4ece9"}}, "hash": "e60f08f9483056075c0d2c3d652c97c8c909fbbf4e04f964b6975e633fa17682", "text": "Instead of relying on RNNs\nto compute intermediate encoder states zand decoder states\nhwe use convolutional neural networks (CNN).\n3.1. Position Embeddings\nFirst, we embed input elements x= (x1,...,xm)in dis-\ntributional space as w= (w1,...,wm), wherewj\u2208Rf\nis a column in an embedding matrix D\u2208RV\u00d7f. We also\nequip our model with a sense of order by embedding the ab-\nsolute position of input elements p= (p1,...,pm)where\npj\u2208Rf. Both are combined to obtain input element rep-\nresentations e= (w1+p1,...,wm+pm). We proceed\nsimilarly for output elements that were already generated\nby the decoder network to yield output element represen-\ntations that are being fed back into the decoder network\ng= (g1,...,gn). Position embeddings are useful in our\narchitecture since they give our model a sense of which\nportion of the sequence in the input or output it is currently\ndealing with (\u00a75.4).\n3.2. Convolutional Block Structure\nBoth encoder and decoder networks share a simple block\nstructure that computes intermediate states based on a \ufb01xed\nnumber of input elements. We denote the output of the l-\nth block as hl= (hl\n1,...,hl\nn)for the decoder network,\nandzl= (zl\n1,...,zl\nm)for the encoder network; we refer\nto blocks and layers interchangeably. Each block contains\na one dimensional convolution followed by a non-linearity.\nFor a decoder network with a single block and kernel width\nk, each resulting state h1\nicontains information over kinput\nelements. Stacking several blocks on top of each other in-\ncreases the number of input elements represented in a state.\nFor instance, stacking 6blocks with k= 5results in an in-\nput \ufb01eld of 25elements, i.e. each output depends on 25\n2", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8aaae5eb-f328-4746-b399-07994d2b0acb": {"__data__": {"id_": "8aaae5eb-f328-4746-b399-07994d2b0acb", "embedding": null, "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e13b5f59-ed8a-42a8-8569-db7f6f108ff0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "527fc5e5f6dcd31578e17ea8914510009ea9eb314eef3decf0a297c25f7ade1b"}, "3": {"node_id": "13be01f6-0b0c-4a82-bdad-1b2a0a99128c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7ebd38598f4d172aec72ff3e5f81704b40b9852a729ea8d0ce4fede09076864c"}}, "hash": "a2994ef71c78814b935c054a37b6b0eaab118063e5166a753e96975906bc8eb9", "text": "Convolutional Sequence to Sequence Learning\ninputs. Non-linearities allow the networks to exploit the\nfull input \ufb01eld, or to focus on fewer elements if needed.\nEach convolution kernel is parameterized as W\u2208R2d\u00d7kd,\nbw\u2208R2dand takes as input X\u2208Rk\u00d7dwhich is a\nconcatenation of kinput elements embedded in ddimen-\nsions and maps them to a single output element Y\u2208R2d\nthat has twice the dimensionality of the input elements;\nsubsequent layers operate over the koutput elements of\nthe previous layer. We choose gated linear units (GLU;\nDauphin et al., 2016) as non-linearity which implement a\nsimple gating mechanism over the output of the convolu-\ntionY= [AB]\u2208R2d:\nv([AB]) =A\u2297\u03c3(B)\nwhereA,B\u2208Rdare the inputs to the non-linearity, \u2297is\nthe point-wise multiplication and the output v([A B])\u2208\nRdis half the size of Y. The gates \u03c3(B)control which\ninputsAof the current context are relevant. A similar non-\nlinearity has been introduced in Oord et al. (2016b) who\napply tanh toAbut Dauphin et al. (2016) shows that GLUs\nperform better in the context of language modelling.\nTo enable deep convolutional networks, we add residual\nconnections from the input of each convolution to the out-\nput of the block (He et al., 2015a).\nhl\ni=v(Wl[hl\u22121\ni\u2212k/2,...,hl\u22121\ni+k/2] +bl\nw) +hl\u22121\ni\nFor encoder networks we ensure that the output of the con-\nvolutional layers matches the input length by padding the\ninput at each layer. However, for decoder networks we have\nto take care that no future information is available to the de-\ncoder (Oord et al., 2016a). Speci\ufb01cally, we pad the input\nbyk\u22121elements on both the left and right side by zero\nvectors, and then remove kelements from the end of the\nconvolution output.\nWe also add linear mappings to project between the embed-\nding sizefand the convolution outputs that are of size 2d.\nWe apply such a transform to wwhen feeding embeddings\nto the encoder network, to the encoder output zu\nj, to the \ufb01-\nnal layer of the decoder just before the softmax hL, and to\nall decoder layers hlbefore computing attention scores (1).\nFinally, we compute a distribution over the Tpossible next\ntarget elements yi+1by transforming the top decoder out-\nputhL\nivia a linear layer with weights Woand biasbo:\np(yi+1|y1,...,yi,x) = softmax( WohL\ni+bo)\u2208RT\n3.3. Multi-step Attention\nWe introduce a separate attention mechanism for each de-\ncoder layer. To compute the attention, we combine the cur-\nrent decoder state hl\niwith an embedding of the previous\nFigure 1. Illustration of batching during training. The English\nsource sentence is encoded (top) and we compute all attention\nvalues for the four German target words (center) simultaneously.\nOur attentions are just dot products between decoder context rep-\nresentations (bottom left) and encoder representations. We add\nthe conditional inputs computed by the attention (center right) to\nthe decoder states which then predict the target words (bottom\nright). The sigmoid and multiplicative boxes illustrate Gated Lin-\near Units.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "13be01f6-0b0c-4a82-bdad-1b2a0a99128c": {"__data__": {"id_": "13be01f6-0b0c-4a82-bdad-1b2a0a99128c", "embedding": null, "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e13b5f59-ed8a-42a8-8569-db7f6f108ff0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "527fc5e5f6dcd31578e17ea8914510009ea9eb314eef3decf0a297c25f7ade1b"}, "2": {"node_id": "8aaae5eb-f328-4746-b399-07994d2b0acb", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a2994ef71c78814b935c054a37b6b0eaab118063e5166a753e96975906bc8eb9"}}, "hash": "7ebd38598f4d172aec72ff3e5f81704b40b9852a729ea8d0ce4fede09076864c", "text": "The sigmoid and multiplicative boxes illustrate Gated Lin-\near Units.\ntarget element gi:\ndl\ni=Wl\ndhl\ni+bl\nd+gi (1)\nFor decoder layer lthe attention al\nijof stateiand source el-\nementjis computed as a dot-product between the decoder\nstate summary dl\niand each output zu\njof the last encoder\nblocku:\nal\nij=exp/parenleftbig\ndl\ni\u00b7zu\nj/parenrightbig\n/summationtextm\nt=1exp/parenleftbig\ndl\ni\u00b7zu\nt/parenrightbig\nThe conditional input cl\nito the current decoder layer is a\nweighted sum of the encoder outputs as well as the input\nelement embeddings ej(Figure 1, center right):\ncl\ni=m/summationdisplay\nj=1al\nij(zu\nj+ej) (2)\nThis is slightly different to recurrent approaches which\ncompute both the attention and the weighted sum over zu\nj\n3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c1a6d24-8acc-4c3f-94c9-4e876126c15c": {"__data__": {"id_": "6c1a6d24-8acc-4c3f-94c9-4e876126c15c", "embedding": null, "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf41e409-ceaa-4311-8982-adc573e48688", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "41905b623b8889ee46f0545097fe766997d8c0058df4db48ac8e491a40e00857"}, "3": {"node_id": "0a363440-8cc2-458e-a73e-310ec1288106", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "be3d844b46c57e22e2a5e7ddfdaf7e70db55c455f0f4012a8b7cc752ab4aaf05"}}, "hash": "e202b912773371d66102df7f1c9b35ca5eb8a1e37ee294809bebd801e4bb5754", "text": "Convolutional Sequence to Sequence Learning\nonly. We found adding ejto be bene\ufb01cial and it resem-\nbles key-value memory networks where the keys are the zu\nj\nand the values are the zu\nj+ej(Miller et al., 2016). En-\ncoder outputs zu\njrepresent potentially large input contexts\nandejprovides point information about a speci\ufb01c input el-\nement that is useful when making a prediction. Once cl\ni\nhas been computed, it is simply added to the output of the\ncorresponding decoder layer hl\ni.\nThis can be seen as attention with multiple \u2019hops\u2019\n(Sukhbaatar et al., 2015) compared to single step attention\n(Bahdanau et al., 2014; Luong et al., 2015; Zhou et al.,\n2016; Wu et al., 2016). In particular, the attention of\nthe \ufb01rst layer determines a useful source context which\nis then fed to the second layer that takes this information\ninto account when computing attention etc. The decoder\nalso has immediate access to the attention history of the\nk\u22121previous time steps because the conditional inputs\ncl\u22121\ni\u2212k,...,cl\u22121\niare part ofhl\u22121\ni\u2212k,...,hl\u22121\niwhich are input\ntohl\ni. This makes it easier for the model to take into ac-\ncount which previous inputs have been attended to already\ncompared to recurrent nets where this information is in the\nrecurrent state and needs to survive several non-linearities.\nOverall, our attention mechanism considers which words\nwe previously attended to (Yang et al., 2016) andperforms\nmultiple attention \u2019hops\u2019 per time step. In Appendix \u00a7C,\nwe plot attention scores for a deep decoder and show that\nat different layers, different portions of the source are at-\ntended to.\nOur convolutional architecture also allows to batch the at-\ntention computation across all elements of a sequence com-\npared to RNNs (Figure 1, middle). We batch the computa-\ntions of each decoder layer individually.\n3.4. Normalization Strategy\nWe stabilize learning through careful weight initialization\n(\u00a73.5) and by scaling parts of the network to ensure that the\nvariance throughout the network does not change dramati-\ncally. In particular, we scale the output of residual blocks\nas well as the attention to preserve the variance of activa-\ntions. We multiply the sum of the input and output of a\nresidual block by\u221a\n0.5to halve the variance of the sum.\nThis assumes that both summands have the same variance\nwhich is not always true but effective in practice.\nThe conditional input cl\nigenerated by the attention is a\nweighted sum of mvectors (2) and we counteract a change\nin variance through scaling by m/radicalbig\n1/m; we multiply by\nmto scale up the inputs to their original size, assuming the\nattention scores are uniformly distributed. This is generally\nnot the case but we found it to work well in practice.\nFor convolutional decoders with multiple attention, we\nscale the gradients for the encoder layers by the numberof attention mechanisms we use; we exclude source word\nembeddings. We found this to stabilize learning since the\nencoder received too much gradient otherwise.\n3.5. Initialization\nNormalizing activations when adding the output of dif-\nferent layers, e.g. residual connections, requires careful\nweight initialization. The motivation for our initialization\nis the same as for the normalization: maintain the variance\nof activations throughout the forward and backward passes.\nAll embeddings are initialized from a normal distribution\nwith mean 0and standard deviation 0.1. For layers whose\noutput is not directly fed to a gated linear unit, we initial-\nize weights fromN(0,/radicalbig\n1/nl)wherenlis the number of\ninput connections to each neuron. This ensures that the\nvariance of a normally distributed input is retained.\nFor layers which are followed by a GLU activation, we pro-\npose a weight initialization scheme by adapting the deriva-\ntions in (He et al., 2015b; Glorot & Bengio, 2010; Ap-\npendix A).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a363440-8cc2-458e-a73e-310ec1288106": {"__data__": {"id_": "0a363440-8cc2-458e-a73e-310ec1288106", "embedding": null, "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf41e409-ceaa-4311-8982-adc573e48688", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "41905b623b8889ee46f0545097fe766997d8c0058df4db48ac8e491a40e00857"}, "2": {"node_id": "6c1a6d24-8acc-4c3f-94c9-4e876126c15c", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e202b912773371d66102df7f1c9b35ca5eb8a1e37ee294809bebd801e4bb5754"}}, "hash": "be3d844b46c57e22e2a5e7ddfdaf7e70db55c455f0f4012a8b7cc752ab4aaf05", "text": "If the GLU inputs are distributed with mean 0\nand have suf\ufb01ciently small variance, then we can approx-\nimate the output variance with 1/4of the input variance\n(Appendix A.1). Hence, we initialize the weights so that\nthe input to the GLU activations have 4times the variance\nof the layer input. This is achieved by drawing their initial\nvalues fromN(0,/radicalbig\n4/nl). Biases are uniformly set to zero\nwhen the network is constructed.\nWe apply dropout to the input of some layers so that in-\nputs are retained with a probability of p. This can be seen\nas multiplication with a Bernoulli random variable taking\nvalue 1/pwith probability pand0otherwise (Srivastava\net al., 2014). The application of dropout will then cause\nthe variance to be scaled by 1/p. We aim to restore the\nincoming variance by initializing the respective layers with\nlarger weights. Speci\ufb01cally, we use N(0,/radicalbig\n4p/nl)for lay-\ners whose output is subject to a GLU and N(0,/radicalbig\np/nl)\notherwise (Appendix A.3).\n4. Experimental Setup\n4.1. Datasets\nWe consider three major WMT translation tasks as well as\na text summarization task.\nWMT\u201916 English-Romanian. We use the same data and\npre-processing as Sennrich et al. (2016b) but remove sen-\ntences with more than 175words. This results in 2.8M sen-\ntence pairs for training and we evaluate on newstest2016.2\n2We followed the pre-processing of https://github.\ncom/rsennrich/wmt16-scripts/blob/80e21e5/\nsample/preprocess.sh and added the back-translated data\nfrom http://data.statmt.org/rsennrich/wmt16_\n4", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9bbe9300-82ab-48f9-924e-25c28da0c22c": {"__data__": {"id_": "9bbe9300-82ab-48f9-924e-25c28da0c22c", "embedding": null, "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b1dcd45-cc0b-40da-a634-3825d04c51b7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "40a359674b6140fa88c077a3bddef0dcae3ba0c256ef7953e7c8556f1ea94b2a"}, "3": {"node_id": "a58397a2-e7ac-42c7-bd5a-06a0c0a7dc12", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ce346e200cf0efdb622eb84c5732f93867b14092495e3266a9b0709b964aa0dc"}}, "hash": "16b40d6ae8535d52bafb9332506b529f52aed7b1324bb8ce77de76464a0e2ae8", "text": "Convolutional Sequence to Sequence Learning\nWe experiment with word-based models using a source vo-\ncabulary of 200K types and a target vocabulary of 80K\ntypes. We also consider a joint source and target byte-pair\nencoding (BPE) with 40K types (Sennrich et al., 2016a;b).\nWMT\u201914 English-German. We use the same setup as Lu-\nong et al. (2015) which comprises 4.5M sentence pairs for\ntraining and we test on newstest2014.3As vocabulary we\nuse 40K sub-word types based on BPE.\nWMT\u201914 English-French. We use the full training set of\n36M sentence pairs, and remove sentences longer than 175\nwords as well as pairs with a source/target length ratio ex-\nceeding 1.5. This results in 35.5M sentence-pairs for train-\ning. Results are reported on newstest2014 . We use a source\nand target vocabulary with 40K BPE types.\nIn all setups a small subset of the training data serves as val-\nidation set (about 0.5-1% for each dataset) for early stop-\nping and learning rate annealing.\nAbstractive summarization. We train on the Gigaword\ncorpus (Graff et al., 2003) and pre-process it identically\nto Rush et al. (2015) resulting in 3.8M training examples\nand 190K for validation. We evaluate on the DUC-2004\ntest data comprising 500 article-title pairs (Over et al.,\n2007) and report three variants of recall-based ROUGE\n(Lin, 2004), namely, ROUGE-1 (unigrams), ROUGE-2 (bi-\ngrams), and ROUGE-L (longest-common substring). We\nalso evaluate on a Gigaword test set of 2000 pairs which\nis identical to the one used by Rush et al. (2015) and we\nreport F1 ROUGE similar to prior work. Similar to Shen\net al. (2016) we use a source and target vocabulary of 30K\nwords and require outputs to be at least 14 words long.\n4.2. Model Parameters and Optimization\nWe use 512hidden units for both encoders and decoders,\nunless otherwise stated. All embeddings, including the out-\nput produced by the decoder before the \ufb01nal linear layer,\nhave dimensionality 512; we use the same dimensionalities\nfor linear layers mapping between the hidden and embed-\nding sizes (\u00a73.2).\nWe train our convolutional models with Nesterov\u2019s accel-\nerated gradient method (Sutskever et al., 2013) using a mo-\nmentum value of 0.99and renormalize gradients if their\nnorm exceeds 0.1(Pascanu et al., 2013). We use a learn-\ning rate of 0.25and once the validation perplexity stops\nimproving, we reduce the learning rate by an order of mag-\nnitude after each epoch until it falls below 10\u22124.\nUnless otherwise stated, we use mini-batches of 64sen-\ntences. We restrict the maximum number of words in a\nmini-batch to make sure that batches with long sentences\nbacktranslations/en-ro .\n3http://nlp.stanford.edu/projects/nmtstill \ufb01t in GPU memory. If the threshold is exceeded, we\nsimply split the batch until the threshold is met and pro-\ncess the parts separatedly. Gradients are normalized by the\nnumber of non-padding tokens per mini-batch. We also use\nweight normalization for all layers except for lookup tables\n(Salimans & Kingma, 2016).\nBesides dropout on the embeddings and the decoder out-\nput, we also apply dropout to the input of the convolu-\ntional blocks (Srivastava et al., 2014). All models are im-\nplemented in Torch (Collobert et al., 2011) and trained on\na single Nvidia M40 GPU except for WMT\u201914 English-\nFrench for which we use a multi-GPU setup on a single\nmachine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a58397a2-e7ac-42c7-bd5a-06a0c0a7dc12": {"__data__": {"id_": "a58397a2-e7ac-42c7-bd5a-06a0c0a7dc12", "embedding": null, "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b1dcd45-cc0b-40da-a634-3825d04c51b7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "40a359674b6140fa88c077a3bddef0dcae3ba0c256ef7953e7c8556f1ea94b2a"}, "2": {"node_id": "9bbe9300-82ab-48f9-924e-25c28da0c22c", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "16b40d6ae8535d52bafb9332506b529f52aed7b1324bb8ce77de76464a0e2ae8"}}, "hash": "ce346e200cf0efdb622eb84c5732f93867b14092495e3266a9b0709b964aa0dc", "text": "We train on up to eight GPUs synchronously by\nmaintaining copies of the model on each card and split the\nbatch so that each worker computes 1/8-th of the gradients;\nat the end we sum the gradients via Nvidia NCCL.\n4.3. Evaluation\nWe report average results over three runs of each model,\nwhere each differs only in the initial random seed. Trans-\nlations are generated by a beam search and we normalize\nlog-likelihood scores by sentence length. We use a beam\nof width 5. We divide the log-likelihoods of the \ufb01nal hy-\npothesis in beam search by their length |y|. For WMT\u201914\nEnglish-German we tune a length normalization constant\non a separate development set ( newstest2015 ) and we nor-\nmalize log-likelihoods by |y|\u03b1(Wu et al., 2016). On other\ndatasets we did not \ufb01nd any bene\ufb01t with length normaliza-\ntion.\nFor word-based models, we perform unknown word re-\nplacement based on attention scores after generation (Jean\net al., 2015). Unknown words are replaced by looking up\nthe source word with the maximum attention score in a pre-\ncomputed dictionary. If the dictionary contains no trans-\nlation, then we simply copy the source word. Dictionar-\nies were extracted from the word aligned training data that\nwe obtained with fast align (Dyer et al., 2013). Each\nsource word is mapped to the target word it is most fre-\nquently aligned to. In our multi-step attention ( \u00a73.3) we\nsimply average the attention scores over all layers. Fi-\nnally, we compute case-sensitive tokenized BLEU, except\nfor WMT\u201916 English-Romanian where we use detokenized\nBLEU to be comparable with Sennrich et al. (2016b).4\n4https://github.com/moses-smt/\nmosesdecoder/blob/617e8c8/scripts/generic/\n{multi-bleu.perl ,mteval-v13a.pl}\n5", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46074122-c9e3-4232-a7d4-2e98354962ee": {"__data__": {"id_": "46074122-c9e3-4232-a7d4-2e98354962ee", "embedding": null, "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e067fee-1de3-4a11-bb31-b19b4c9e379f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d3d428b355178dc51ed48d07151ffdb9091c6fb8079e13e629aa7e9330ae9860"}, "3": {"node_id": "61a50473-f583-42dd-91be-5973c3121201", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "47be80372113ac829a8ecf47422c38ac02742966cf5a3f8a3b9c3cf6bdb87f4c"}}, "hash": "4f10a42f785395ee40f0ec9fc06f75d5a3f518558c0fe0f923784e4f84248966", "text": "Convolutional Sequence to Sequence Learning\n5. Results\n5.1. Recurrent vs. Convolutional Models\nWe \ufb01rst evaluate our convolutional model on three transla-\ntion tasks. On WMT\u201916 English-Romanian translation we\ncompare to Sennrich et al. (2016b) which is the winning\nentry on this language pair at WMT\u201916 (Bojar et al., 2016).\nTheir model implements the attention-based sequence to\nsequence architecture of Bahdanau et al. (2014) and uses\nGRU cells both in the encoder and decoder. We test both\nword-based and BPE vocabularies ( \u00a74).\nTable 1 shows that our fully convolutional sequence to se-\nquence model (ConvS2S) outperforms the WMT\u201916 win-\nning entry for English-Romanian by 1.9 BLEU with a BPE\nencoding and by 1.3 BLEU with a word factored vocabu-\nlary. This instance of our architecture has 20 layes in the\nencoder and 20 layers in the decoder, both using kernels\nof width 3 and hidden size 512 throughout. Training took\nbetween 6 and 7.5 days on a single GPU.\nOn WMT\u201914 English to German translation we compare to\nthe following prior work: Luong et al. (2015) is based on a\nfour layer LSTM attention model, ByteNet (Kalchbrenner\net al., 2016) propose a convolutional model based on char-\nacters without attention, with 30 layers in the encoder and\n30 layers in the decoder, GNMT (Wu et al., 2016) repre-\nsents the state of the art on this dataset and they use eight\nencoder LSTMs as well as eight decoder LSTMs, we quote\ntheir result for a word-based model, such as ours, as well\nas a word-piece model (Schuster & Nakajima, 2012).5\nThe results (Table 1) show that our convolutional model\noutpeforms GNMT by 0.5 BLEU. Our encoder has 15 lay-\ners and the decoder has 15 layers, both with 512 hidden\nunits in the \ufb01rst ten layers and 768 units in the subsequent\nthree layers, all using kernel width 3. The \ufb01nal two layers\nhave 2048 units which are just linear mappings with a sin-\ngle input. We trained this model on a single GPU over a\nperiod of 18.5 days with a batch size of 48. LSTM sparse\nmixtures have shown strong accuracy at 26.03 BLEU for a\nsingle run (Shazeer et al., 2016) which compares to 25.39\nBLEU for our best run. This mixture sums the output of\nfour experts, not unlike an ensemble which sums the output\nof multiple networks. ConvS2S also bene\ufb01ts from ensem-\nbling (\u00a75.2), therefore mixtures are a promising direction.\nFinally, we train on the much larger WMT\u201914 English-\nFrench task where we compare to the state of the art re-\nsult of GNMT (Wu et al., 2016). Our model is trained with\na simple token-level likelihood objective and we improve\nover GNMT in the same setting by 1.6 BLEU on average.\nWe also outperform their reinforcement (RL) models by 0.5\n5We did not use the exact same vocabulary size because word\npieces and BPE estimate the vocabulary differently.WMT\u201916 English-Romanian BLEU\nSennrich et al. (2016b) GRU (BPE 90K) 28.1\nConvS2S (Word 80K) 29.45\nConvS2S (BPE 40K) 30.02\nWMT\u201914 English-German BLEU\nLuong et al. (2015) LSTM (Word 50K) 20.9\nKalchbrenner et al. (2016) ByteNet (Char) 23.75\nWu et al. (2016) GNMT (Word 80K) 23.12\nWu et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61a50473-f583-42dd-91be-5973c3121201": {"__data__": {"id_": "61a50473-f583-42dd-91be-5973c3121201", "embedding": null, "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e067fee-1de3-4a11-bb31-b19b4c9e379f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d3d428b355178dc51ed48d07151ffdb9091c6fb8079e13e629aa7e9330ae9860"}, "2": {"node_id": "46074122-c9e3-4232-a7d4-2e98354962ee", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "4f10a42f785395ee40f0ec9fc06f75d5a3f518558c0fe0f923784e4f84248966"}}, "hash": "47be80372113ac829a8ecf47422c38ac02742966cf5a3f8a3b9c3cf6bdb87f4c", "text": "(2016) GNMT (Word 80K) 23.12\nWu et al. (2016) GNMT (Word pieces) 24.61\nConvS2S (BPE 40K) 25.16\nWMT\u201914 English-French BLEU\nWu et al. (2016) GNMT (Word 80K) 37.90\nWu et al. (2016) GNMT (Word pieces) 38.95\nWu et al. (2016) GNMT (Word pieces) + RL 39.92\nConvS2S (BPE 40K) 40.51\nTable 1. Accuracy on WMT tasks comapred to previous work.\nConvS2S and GNMT results are averaged over several runs.\nBLEU. Reinforcement learning is equally applicable to our\narchitecture and we believe that it would further improve\nour results.\nThe ConvS2S model for this experiment uses 15 layers in\nthe encoder and 15 layers in the decoder, both with 512\nhidden units in the \ufb01rst \ufb01ve layers, 768 units in the subse-\nquent four layers, 1024 units in the next 3 layers, all using\nkernel width 3; the \ufb01nal two layers have 2048 units and\n4096 units each but the they are linear mappings with ker-\nnel width 1. This model has an effective context size of\nonly 25 words, beyond which it cannot access any infor-\nmation on the target size. Our results are based on training\nwith 8 GPUs for about 37 days and batch size 32 on each\nworker.6The same con\ufb01guration as for WMT\u201914 English-\nGerman achieves 39.41 BLEU in two weeks on this dataset\nin an eight GPU setup.\nZhou et al. (2016) report a non-averaged result of 39.2\nBLEU. More recently, Ha et al. (2016) showed that one\ncan generate weights with one LSTM for another LSTM.\nThis approach achieves 40.03 BLEU but the result is not\naveraged. Shazeer et al. (2016) compares at 40.56 BLEU\nto our best single run of 40.70 BLEU.\n6This is half of the GPU time consumed by a basic model of\nWu et al. (2016) who use 96 GPUs for 6 days. We expect the time\nto train our model to decrease substantially in a multi-machine\nsetup.\n6", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e1448d1a-02c9-4f78-897b-a75f5e49518d": {"__data__": {"id_": "e1448d1a-02c9-4f78-897b-a75f5e49518d", "embedding": null, "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b371e23-4721-4cb3-a324-15e5c2cdcbdd", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7d18622bf72894e80bb85a3919d82af0f3592bd90f4cb133471ca09fc61cb2eb"}, "3": {"node_id": "27fc9f8d-9779-4761-aefc-081a9cf1604e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7d15deec3bec2178446e4b933f144f970a21a7927ba07f70d30c347750836839"}}, "hash": "161889d9f69f7b4b649a6f324b304da5caae01f37eeefe72f96afc789a0b4111", "text": "Convolutional Sequence to Sequence Learning\nWMT\u201914 English-German BLEU\nWu et al. (2016) GNMT 26.20\nWu et al. (2016) GNMT + RL 26.30\nConvS2S 26.43\nWMT\u201914 English-French BLEU\nZhou et al. (2016) 40.4\nWu et al. (2016) GNMT 40.35\nWu et al. (2016) GNMT + RL 41.16\nConvS2S 41.44\nConvS2S (10 models) 41.62\nTable 2. Accuracy of ensembles with eight models. We show\nboth likelihood and Reinforce (RL) results for GNMT; Zhou et al.\n(2016) and ConvS2S use simple likelihood training.\nThe translations produced by our models often match the\nlength of the references, particularly for the large WMT\u201914\nEnglish-French task, or are very close for small to medium\ndata sets such as WMT\u201914 English-German or WMT\u201916\nEnglish-Romanian.\n5.2. Ensemble Results\nNext, we ensemble eight likelihood-trained models for both\nWMT\u201914 English-German and WMT\u201914 English-French\nand compare to previous work which also reported ensem-\nble results. For the former, we also show the result when\nensembling 10 models. Table 2 shows that we outperform\nthe best current ensembles on both datasets.\n5.3. Generation Speed\nNext, we evaluate the inference speed of our architecture\non the development set of the WMT\u201914 English-French\ntask which is the concatenation of newstest2012 and new-\nstest2013; it comprises 6003 sentences. We measure gener-\nation speed both on GPU and CPU hardware. Speci\ufb01cally,\nwe measure GPU speed on three generations of Nvidia\ncards: a GTX-1080ti, an M40 as well as an older K40\ncard. CPU timings are measured on one host with 48 hyper-\nthreaded cores (Intel Xeon E5-2680 @ 2.50GHz) with 40\nworkers. In all settings, we batch up to 128 sentences, com-\nposing batches with sentences of equal length. Note that\nthe majority of batches is smaller because of the small size\nof the development set. We experiment with beams of size\n5 as well as greedy search, i.e beam of size 1. To make gen-\neration fast, we do not recompute convolution states that\nhave not changed compared to the previous time step but\nrather copy (shift) these activations.\nWe compare to results reported in Wu et al. (2016) whoBLEU Time (s)\nGNMT GPU (K80) 31.20 3,028\nGNMT CPU 88 cores 31.20 1,322\nGNMT TPU 31.21 384\nConvS2S GPU (K40) b= 1 33.45 327\nConvS2S GPU (M40) b= 1 33.45 221\nConvS2S GPU (GTX-1080ti) b= 1 33.45 142\nConvS2S CPU 48 cores b= 1 33.45 142\nConvS2S GPU (K40) b= 5 34.10 587\nConvS2S CPU 48 cores b= 5 34.10 482\nConvS2S GPU (M40) b= 5 34.10 406\nConvS2S GPU (GTX-1080ti) b= 5 34.10 256\nTable 3. CPU and GPU generation speed in seconds on the de-\nvelopment set of WMT\u201914 English-French. We show results for\ndifferent beam sizes b. GNMT \ufb01gures are taken from Wu et al.\n(2016). CPU speeds are not directly comparable because Wu et al.\n(2016) use a 88 core machine versus our 48 core setup.\nuse Nvidia K80 GPUs which are essentially two K40s. We\ndid not have such a GPU available and therefore run ex-\nperiments on an older K40 card which is inferior to a K80,\nin addition to the newer M40 and GTX-1080ti cards.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "27fc9f8d-9779-4761-aefc-081a9cf1604e": {"__data__": {"id_": "27fc9f8d-9779-4761-aefc-081a9cf1604e", "embedding": null, "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b371e23-4721-4cb3-a324-15e5c2cdcbdd", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7d18622bf72894e80bb85a3919d82af0f3592bd90f4cb133471ca09fc61cb2eb"}, "2": {"node_id": "e1448d1a-02c9-4f78-897b-a75f5e49518d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "161889d9f69f7b4b649a6f324b304da5caae01f37eeefe72f96afc789a0b4111"}}, "hash": "7d15deec3bec2178446e4b933f144f970a21a7927ba07f70d30c347750836839", "text": "The\nresults (Table 3) show that our model can generate transla-\ntions on a K40 GPU at 9.3 times the speed and 2.25 higher\nBLEU; on an M40 the speed-up is up to 13.7 times and on\na GTX-1080ti card the speed is 21.3 times faster. A larger\nbeam of size 5 decreases speed but gives better BLEU.\nOn CPU, our model is up to 9.3 times faster, however, the\nGNMT CPU results were obtained with an 88 core machine\nwhereas our results were obtained with just over half the\nnumber of cores. On a per CPU core basis, our model is\n17 times faster at a better BLEU. Finally, our CPU speed is\n2.7 times higher than GNMT on a custom TPU chip which\nshows that high speed can be achieved on commodity hard-\nware. We do no report TPU \ufb01gures as we do not have ac-\ncess to this hardware.\n5.4. Position Embeddings\nIn the following sections, we analyze the design choices in\nour architecture. The remaining results in this paper are\nbased on the WMT\u201914 English-German task with 13 en-\ncoder layers at kernel size 3 and 5 decoder layers at kernel\nsize 5. We use a target vocabulary of 160K words as well as\nvocabulary selection (Mi et al., 2016; L\u2019Hostis et al., 2016)\nto decrease the size of the output layer which speeds up\ntraining and testing. The average vocabulary size for each\ntraining batch is about 20K target words. All \ufb01gures are av-\neraged over three runs ( \u00a74) and BLEU is reported on new-\nstest2014 before unknown word replacement.\nWe start with an experiment that removes the position em-\n7", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fc5fa2b7-8871-41b4-ac91-b27d57ca55df": {"__data__": {"id_": "fc5fa2b7-8871-41b4-ac91-b27d57ca55df", "embedding": null, "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44e46b2f-9826-4bf3-bc87-7dac192228b9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8a667b60d73b58f9fc9f85653d637e30203f46e97f459498eb70dab72bd58422"}, "3": {"node_id": "bddd3a50-6b0f-465e-92a0-95c8fa716e8b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e911abccebe9900f802dc35366ffa3351554105e7c14c47399bad2e24cf2518f"}}, "hash": "f6ccc621d40088792056012b1e3d05464fd65fbdb56cdca839082bae9bc88625", "text": "Convolutional Sequence to Sequence Learning\nPPL BLEU\nConvS2S 6.64 21.7\n-source position 6.69 21.3\n-target position 6.63 21.5\n-source & target position 6.68 21.2\nTable 4. Effect of removing position embeddings from our model\nin terms of validation perplexity (valid PPL) and BLEU.\nbeddings from the encoder and decoder ( \u00a73.1). These em-\nbeddings allow our model to identify which portion of the\nsource and target sequence it is dealing with but also im-\npose a restriction on the maximum sentence length. Ta-\nble 4 shows that position embeddings are helpful but that\nour model still performs well without them. Removing\nthe source position embeddings results in a larger accuracy\ndecrease than target position embeddings. However, re-\nmoving both source and target positions decreases accuracy\nonly by 0.5BLEU. We had assumed that the model would\nnot be able to calibrate the length of the output sequences\nvery well without explicit position information, however,\nthe output lengths of models without position embeddings\nclosely matches models with position information. This in-\ndicates that the models can learn relative position informa-\ntion within the contexts visible to the encoder and decoder\nnetworks which can observe up to 27 and 25 words respec-\ntively.\nRecurrent models typically do not use explicit position em-\nbeddings since they can learn where they are in the se-\nquence through the recurrent hidden state computation. In\nour setting, the use of position embeddings requires only a\nsimple addition to the input word embeddings which is a\nnegligible overhead.\n5.5. Multi-step Attention\nThe multiple attention mechanism ( \u00a73.3) computes a sep-\narate source context vector for each decoder layer. The\ncomputation also takes into account contexts computed for\npreceding decoder layers of the current time step as well\nas previous time steps that are within the receptive \ufb01eld of\nthe decoder. How does multiple attention compare to at-\ntention in fewer layers or even only in a single layer as is\nusual? Table 5 shows that attention in all decoder layers\nachieves the best validation perplexity (PPL). Furthermore,\nremoving more and more attention layers decreases accu-\nracy, both in terms of BLEU as well as PPL.\nThe computational overhead for attention is very small\ncompared to the rest of the network. Training with atten-\ntion in all \ufb01ve decoder layers processes 3624 target words\nper second on average on a single GPU, compared to 3772\nwords per second for attention in a single layer. This is onlyAttn Layers PPL BLEU\n1,2,3,4,5 6.65 21.63\n1,2,3,4 6.70 21.54\n1,2,3 6.95 21.36\n1,2 6.92 21.47\n1,3,5 6.97 21.10\n1 7.15 21.26\n2 7.09 21.30\n3 7.11 21.19\n4 7.19 21.31\n5 7.66 20.24\nTable 5. Multi-step attention in all \ufb01ve decoder layers or fewer\nlayers in terms of validation perplexity (PPL) and test BLEU.\n 19 19.5 20 20.5 21 21.5 22\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25BLEU\nLayersEncoder\nDecoder\nFigure 2. Encoder and decoder with different number of layers.\na 4% slow down when adding 4 attention modules. Most\nneural machine translation systems only use a single mod-\nule. This demonstrates that attention is not the bottleneck\nin neural machine translation, even though it is quadratic in\nthe sequence length (cf. Kalchbrenner et al., 2016). Part of\nthe reason for the low impact on speed is that we batch the\ncomputation of an attention module over all target words,\nsimilar to Kalchbrenner et al. (2016). However, for RNNs\nbatching of the attention may be less effective because of\nthe dependence on the previous time step.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bddd3a50-6b0f-465e-92a0-95c8fa716e8b": {"__data__": {"id_": "bddd3a50-6b0f-465e-92a0-95c8fa716e8b", "embedding": null, "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44e46b2f-9826-4bf3-bc87-7dac192228b9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8a667b60d73b58f9fc9f85653d637e30203f46e97f459498eb70dab72bd58422"}, "2": {"node_id": "fc5fa2b7-8871-41b4-ac91-b27d57ca55df", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "f6ccc621d40088792056012b1e3d05464fd65fbdb56cdca839082bae9bc88625"}}, "hash": "e911abccebe9900f802dc35366ffa3351554105e7c14c47399bad2e24cf2518f", "text": "5.6. Kernel size and Depth\nFigure 2 shows accuracy when we change the number of\nlayers in the encoder or decoder. The kernel width for lay-\ners in the encoder is 3and for the decoder it is 5. Deeper\narchitectures are particularly bene\ufb01cial for the encoder but\nless so for the decoder. Decoder setups with two layers al-\nready perform well whereas for the encoder accuracy keeps\nincreasing steadily with more layers until up to 9 layers\nwhen accuracy starts to plateau.\n8", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "802a0e3c-1c99-4f2f-bb01-1cd426dcc64d": {"__data__": {"id_": "802a0e3c-1c99-4f2f-bb01-1cd426dcc64d", "embedding": null, "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03e1dd28-c2b5-4961-bff2-1de7c3789fd3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "52677b484a0ab4293dba6b4be2808992a0cc5ac5cfe365411315d79290921c73"}, "3": {"node_id": "56bde1d1-72a9-42bd-b1cb-e76711c9c745", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0385018c96a16a7245af0c2eca21874e0e8da0ba47231ad1ec6f30aa1d4de22d"}}, "hash": "7e1d211af2d12c8512c1a99ecfd4fd4b60a615630a801fbfa59dcc9d1a875aad", "text": "Convolutional Sequence to Sequence Learning\nDUC-2004 Gigaword\nRG-1 (R) RG-2 (R) RG-L (R) RG-1 (F) RG-2 (F) RG-L (F)\nRNN MLE (Shen et al., 2016) 24.92 8.60 22.25 32.67 15.23 30.56\nRNN MRT (Shen et al., 2016) 30.41 10.87 26.79 36.54 16.59 33.44\nWFE (Suzuki & Nagata, 2017) 32.28 10.54 27.80 36.30 17.31 33.88\nConvS2S 30.44 10.84 26.90 35.88 17.48 33.29\nTable 6. Accuracy on two summarization tasks in terms of Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L).\nKernel width Encoder layers\n5 9 13\n3 20.61 21.17 21.63\n5 20.80 21.02 21.42\n7 20.81 21.30 21.09\nTable 7. Encoder with different kernel width in terms of BLEU.\nKernel width Decoder layers\n3 5 7\n3 21.10 21.71 21.62\n5 21.09 21.63 21.24\n7 21.40 21.31 21.33\nTable 8. Decoder with different kernel width in terms of BLEU.\nAside from increasing the depth of the networks, we can\nalso change the kernel width. Table 7 shows that encoders\nwith narrow kernels and many layers perform better than\nwider kernels. These networks can also be faster since the\namount of work to compute a kernel operating over 3 input\nelements is less than half compared to kernels over 7 ele-\nments. We see a similar picture for decoder networks with\nlarge kernel sizes (Table 8). Dauphin et al. (2016) shows\nthat context sizes of 20 words are often suf\ufb01cient to achieve\nvery good accuracy on language modeling for English.\n5.7. Summarization\nFinally, we evaluate our model on abstractive sentence\nsummarization which takes a long sentence as input and\noutputs a shortened version. The current best models on\nthis task are recurrent neural networks which either opti-\nmize the evaluation metric (Shen et al., 2016) or address\nspeci\ufb01c problems of summarization such as avoiding re-\npeated generations (Suzuki & Nagata, 2017). We use stan-\ndard likelhood training for our model and a simple model\nwith six layers in the encoder and decoder each, hidden\nsize256, batch size 128, and we trained on a single GPU in\none night. Table 6 shows that our likelhood trained model\noutperforms the likelihood trained model (RNN MLE) of\nShen et al. (2016) and is not far behind the best models on\nthis task which bene\ufb01t from task-speci\ufb01c optimization andmodel structure. We expect our model to bene\ufb01t from these\nimprovements as well.\n6. Conclusion and Future Work\nWe introduce the \ufb01rst fully convolutional model for se-\nquence to sequence learning that outperforms strong re-\ncurrent models on very large benchmark datasets at an or-\nder of magnitude faster speed. Compared to recurrent net-\nworks, our convolutional approach allows to discover com-\npositional structure in the sequences more easily since rep-\nresentations are built hierarchically. Our model relies on\ngating and performs multiple attention steps.\nWe achieve a new state of the art on several public trans-\nlation benchmark data sets. On the WMT\u201916 English-\nRomanian task we outperform the previous best result by\n1.9 BLEU, on WMT\u201914 English-French translation we im-\nprove over the LSTM model of Wu et al. (2016) by 1.6\nBLEU in a comparable setting, and on WMT\u201914 English-\nGerman translation we ouperform the same model by 0.5\nBLEU. In future work, we would like to apply convolu-\ntional architectures to other sequence to sequence learn-\ning problems which may bene\ufb01t from learning hierarchical\nrepresentations as well.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "56bde1d1-72a9-42bd-b1cb-e76711c9c745": {"__data__": {"id_": "56bde1d1-72a9-42bd-b1cb-e76711c9c745", "embedding": null, "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03e1dd28-c2b5-4961-bff2-1de7c3789fd3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "52677b484a0ab4293dba6b4be2808992a0cc5ac5cfe365411315d79290921c73"}, "2": {"node_id": "802a0e3c-1c99-4f2f-bb01-1cd426dcc64d", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "7e1d211af2d12c8512c1a99ecfd4fd4b60a615630a801fbfa59dcc9d1a875aad"}}, "hash": "0385018c96a16a7245af0c2eca21874e0e8da0ba47231ad1ec6f30aa1d4de22d", "text": "Acknowledgements\nWe thank Benjamin Graham for providing a fast 1-D con-\nvolution, and Ronan Collobert as well as Yann LeCun for\nhelpful discussions related to this work.\nReferences\nBa, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Ge-\noffrey E. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,\nYoshua. Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 ,\n2014.\nBojar, Ondej, Chatterjee, Rajen, Federmann, Christian,\nGraham, Yvette, Haddow, Barry, Huck, Matthias,\n9", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b78e3b9d-35dd-49b8-a721-e3b6d14f0300": {"__data__": {"id_": "b78e3b9d-35dd-49b8-a721-e3b6d14f0300", "embedding": null, "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d2bce7f-3073-4a67-9e1b-15cfb44cc4a8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6c96ae8d1b654cd484ed277ec78c814e941c78781a19f0909063e666756c74ee"}, "3": {"node_id": "5063748f-bc7b-4a58-ae66-8a06e22d00a9", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8710fc93f4eea2f4c3887aa7b6e4aa6d9b6b6bf20f461b67b97235567b78a95d"}}, "hash": "a0ffc7ff936f4678ea3a48f76650aa47b2c1f5ee7fadf81d37a4636c166f4741", "text": "Convolutional Sequence to Sequence Learning\nJimeno-Yepes, Antonio, Koehn, Philipp, Logacheva,\nVarvara, Monz, Christof, Negri, Matteo, N \u00b4ev\u00b4eol,\nAur\u00b4elie, Neves, Mariana L., Popel, Martin, Post, Matt,\nRubino, Rapha \u00a8el, Scarton, Carolina, Specia, Lucia,\nTurchi, Marco, Verspoor, Karin M., and Zampieri, Mar-\ncos. Findings of the 2016 conference on machine trans-\nlation. In Proc. of WMT , 2016.\nBradbury, James, Merity, Stephen, Xiong, Caiming, and\nSocher, Richard. Quasi-Recurrent Neural Networks.\narXiv preprint arXiv:1611.01576 , 2016.\nCho, Kyunghyun, Van Merri \u00a8enboer, Bart, Gulcehre,\nCaglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk,\nHolger, and Bengio, Yoshua. Learning Phrase Represen-\ntations using RNN Encoder-Decoder for Statistical Ma-\nchine Translation. In Proc. of EMNLP , 2014.\nChorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy,\nCho, Kyunghyun, and Bengio, Yoshua. Attention-based\nmodels for speech recognition. In Advances in Neural\nInformation Processing Systems , pp. 577\u2013585, 2015.\nCollobert, Ronan, Kavukcuoglu, Koray, and Farabet,\nClement. Torch7: A Matlab-like Environment for Ma-\nchine Learning. In BigLearn, NIPS Workshop , 2011.\nURL http://torch.ch .\nDauphin, Yann N., Fan, Angela, Auli, Michael, and Grang-\nier, David. Language modeling with gated linear units.\narXiv preprint arXiv:1612.08083 , 2016.\nDyer, Chris, Chahuneau, Victor, and Smith, Noah A. A\nSimple, Fast, and Effective Reparameterization of IBM\nModel 2. In Proc. of ACL , 2013.\nElman, Jeffrey L. Finding Structure in Time. Cognitive\nScience , 14:179\u2013211, 1990.\nGehring, Jonas, Auli, Michael, Grangier, David, and\nDauphin, Yann N. A Convolutional Encoder Model\nfor Neural Machine Translation. arXiv preprint\narXiv:1611.02344 , 2016.\nGlorot, Xavier and Bengio, Yoshua. Understanding the\ndif\ufb01culty of training deep feedforward neural networks.\nThe handbook of brain theory and neural networks ,\n2010.\nGraff, David, Kong, Junbo, Chen, Ke, and Maeda,\nKazuaki. English gigaword. Linguistic Data Consor-\ntium, Philadelphia , 2003.\nHa, David, Dai, Andrew, and Le, Quoc V . Hypernetworks.\narXiv preprint arXiv:1609.09106 , 2016.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep Residual Learning for Image Recognition. In\nProc. of CVPR , 2015a.He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Delving deep into recti\ufb01ers: Surpassing human-\nlevel performance on imagenet classi\ufb01cation. In Pro-\nceedings of the IEEE International Conference on Com-\nputer Vision , pp. 1026\u20131034, 2015b.\nHochreiter, Sepp and Schmidhuber, J \u00a8urgen. Long short-\nterm memory. Neural computation , 9(8):1735\u20131780,\n1997.\nIoffe, Sergey and Szegedy, Christian. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. In Proceedings of The 32nd International\nConference on Machine Learning , pp.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5063748f-bc7b-4a58-ae66-8a06e22d00a9": {"__data__": {"id_": "5063748f-bc7b-4a58-ae66-8a06e22d00a9", "embedding": null, "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d2bce7f-3073-4a67-9e1b-15cfb44cc4a8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6c96ae8d1b654cd484ed277ec78c814e941c78781a19f0909063e666756c74ee"}, "2": {"node_id": "b78e3b9d-35dd-49b8-a721-e3b6d14f0300", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a0ffc7ff936f4678ea3a48f76650aa47b2c1f5ee7fadf81d37a4636c166f4741"}}, "hash": "8710fc93f4eea2f4c3887aa7b6e4aa6d9b6b6bf20f461b67b97235567b78a95d", "text": "In Proceedings of The 32nd International\nConference on Machine Learning , pp. 448\u2013456, 2015.\nJean, S \u00b4ebastien, Firat, Orhan, Cho, Kyunghyun, Memi-\nsevic, Roland, and Bengio, Yoshua. Montreal Neural\nMachine Translation systems for WMT15. In Proc. of\nWMT , pp. 134\u2013140, 2015.\nKalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen,\nvan den Oord, Aaron, Graves, Alex, and Kavukcuoglu,\nKoray. Neural Machine Translation in Linear Time.\narXiv , 2016.\nLeCun, Yann and Bengio, Yoshua. Convolutional networks\nfor images, speech, and time series. The handbook of\nbrain theory and neural networks , 3361(10):1995, 1995.\nL\u2019Hostis, Gurvan, Grangier, David, and Auli, Michael. V o-\ncabulary Selection Strategies for Neural Machine Trans-\nlation. arXiv preprint arXiv:1610.00072 , 2016.\nLin, Chin-Yew. Rouge: A package for automatic evalu-\nation of summaries. In Text Summarization Branches\nOut: Proceedings of the ACL-04 Workshop , pp. 74\u201381,\n2004.\nLuong, Minh-Thang, Pham, Hieu, and Manning, Christo-\npher D. Effective approaches to attention-based neural\nmachine translation. In Proc. of EMNLP , 2015.\nMeng, Fandong, Lu, Zhengdong, Wang, Mingxuan, Li,\nHang, Jiang, Wenbin, and Liu, Qun. Encoding Source\nLanguage with Convolutional Neural Network for Ma-\nchine Translation. In Proc. of ACL , 2015.\nMi, Haitao, Wang, Zhiguo, and Ittycheriah, Abe. V ocab-\nulary Manipulation for Neural Machine Translation. In\nProc. of ACL , 2016.\nMiller, Alexander H., Fisch, Adam, Dodge, Jesse, Karimi,\nAmir-Hossein, Bordes, Antoine, and Weston, Jason.\nKey-value memory networks for directly reading docu-\nments. In Proc. of EMNLP , 2016.\nNallapati, Ramesh, Zhou, Bowen, Gulcehre, Caglar, Xi-\nang, Bing, et al. Abstractive text summarization us-\ning sequence-to-sequence rnns and beyond. In Proc. of\nEMNLP , 2016.\n10", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f535e700-1b9a-4b90-a385-9a1ae62c3539": {"__data__": {"id_": "f535e700-1b9a-4b90-a385-9a1ae62c3539", "embedding": null, "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "318e8d10-9e10-4124-b963-da894ef2e3eb", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c34d0134221c51016d0422b4d0e7654c4b1c3d21d78c1b4671ba682271c69c9f"}, "3": {"node_id": "427fea22-4fef-4742-af36-2be94792a0ea", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "24fe423612546b081e2d88aac0bb2abf9a92f6be82e3d4799b105e47f10d903c"}}, "hash": "fad3ac9795352ea06785cf23da2a47b381a8ee70ee63ee360c1bfa4aff3bf9f5", "text": "Convolutional Sequence to Sequence Learning\nOord, Aaron van den, Kalchbrenner, Nal, and\nKavukcuoglu, Koray. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759 , 2016a.\nOord, Aaron van den, Kalchbrenner, Nal, Vinyals, Oriol,\nEspeholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray.\nConditional image generation with pixelcnn decoders.\narXiv preprint arXiv:1606.05328 , 2016b.\nOver, Paul, Dang, Hoa, and Harman, Donna. Duc in con-\ntext. Information Processing & Management , 43(6):\n1506\u20131520, 2007.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\nOn the dif\ufb01culty of training recurrent neural networks.\nInProceedings of The 30th International Conference on\nMachine Learning , pp. 1310\u20131318, 2013.\nRush, Alexander M, Chopra, Sumit, and Weston, Jason. A\nneural attention model for abstractive sentence summa-\nrization. In Proc. of EMNLP , 2015.\nSalimans, Tim and Kingma, Diederik P. Weight nor-\nmalization: A simple reparameterization to acceler-\nate training of deep neural networks. arXiv preprint\narXiv:1602.07868 , 2016.\nSchuster, Mike and Nakajima, Kaisuke. Japanese and ko-\nrean voice search. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2012 IEEE International Conference\non, pp. 5149\u20135152. IEEE, 2012.\nSennrich, Rico, Haddow, Barry, and Birch, Alexandra.\nNeural Machine Translation of Rare Words with Sub-\nword Units. In Proc. of ACL , 2016a.\nSennrich, Rico, Haddow, Barry, and Birch, Alexandra. Ed-\ninburgh Neural Machine Translation Systems for WMT\n16. In Proc. of WMT , 2016b.\nShazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof,\nDavis, Andy, Le, Quoc, Hinton, Geoffrey, and Dean,\nJeff. Outrageously large neural networks: The sparsely-\ngated mixture-of-experts layer. ArXiv e-prints , January\n2016.\nShen, Shiqi, Zhao, Yu, Liu, Zhiyuan, Sun, Maosong,\net al. Neural headline generation with sentence-wise op-\ntimization. arXiv preprint arXiv:1604.01904 , 2016.\nSrivastava, Nitish, Hinton, Geoffrey E., Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: a\nsimple way to prevent Neural Networks from over\ufb01tting.\nJMLR , 15:1929\u20131958, 2014.\nSukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, and\nSzlam, Arthur. End-to-end Memory Networks. In Proc.\nof NIPS , pp. 2440\u20132448, 2015.Sutskever, Ilya, Martens, James, Dahl, George E., and Hin-\nton, Geoffrey E. On the importance of initialization and\nmomentum in deep learning. In ICML , 2013.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Sequence\nto Sequence Learning with Neural Networks. In Proc. of\nNIPS , pp. 3104\u20133112, 2014.\nSuzuki, Jun and Nagata, Masaaki. Cutting-off redundant\nrepeating generations for neural abstractive summariza-\ntion. arXiv preprint arXiv:1701.00138 , 2017.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "427fea22-4fef-4742-af36-2be94792a0ea": {"__data__": {"id_": "427fea22-4fef-4742-af36-2be94792a0ea", "embedding": null, "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "318e8d10-9e10-4124-b963-da894ef2e3eb", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c34d0134221c51016d0422b4d0e7654c4b1c3d21d78c1b4671ba682271c69c9f"}, "2": {"node_id": "f535e700-1b9a-4b90-a385-9a1ae62c3539", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "fad3ac9795352ea06785cf23da2a47b381a8ee70ee63ee360c1bfa4aff3bf9f5"}}, "hash": "24fe423612546b081e2d88aac0bb2abf9a92f6be82e3d4799b105e47f10d903c", "text": "arXiv preprint arXiv:1701.00138 , 2017.\nWaibel, Alex, Hanazawa, Toshiyuki, Hinton, Geoffrey,\nShikano, Kiyohiro, and Lang, Kevin J. Phoneme Recog-\nnition using Time-delay Neural Networks. IEEE trans-\nactions on acoustics, speech, and signal processing , 37\n(3):328\u2013339, 1989.\nWu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V ,\nNorouzi, Mohammad, Macherey, Wolfgang, Krikun,\nMaxim, Cao, Yuan, Gao, Qin, Macherey, Klaus, et al.\nGoogle\u2019s Neural Machine Translation System: Bridging\nthe Gap between Human and Machine Translation. arXiv\npreprint arXiv:1609.08144 , 2016.\nYang, Zichao, Hu, Zhiting, Deng, Yuntian, Dyer, Chris,\nand Smola, Alex. Neural Machine Translation\nwith Recurrent Attention Modeling. arXiv preprint\narXiv:1607.05108 , 2016.\nZhou, Jie, Cao, Ying, Wang, Xuguang, Li, Peng, and Xu,\nWei. Deep Recurrent Models with Fast-Forward Con-\nnections for Neural Machine Translation. arXiv preprint\narXiv:1606.04199 , 2016.\n11", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e09d9d99-060d-41e7-adf9-27caf18d68ce": {"__data__": {"id_": "e09d9d99-060d-41e7-adf9-27caf18d68ce", "embedding": null, "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d14f1254-258a-4f0c-9d42-50293e276bb5", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ff60c568641fb71c2cf9323356649e57268f061a15a8b756c6d04aafc2eeb1c7"}, "3": {"node_id": "41291bbe-f253-424d-a1af-db318e580e5e", "node_type": "1", "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8bf1e87b85dbcb5c3bbcccb339b93405fb09c765b0ae443f80ac15767b97c151"}}, "hash": "ed377127b4f76f62d26faf1b817ca904ca1a51bbd6b3a2982b123b77d41abc98", "text": "Convolutional Sequence to Sequence Learning\nA. Weight Initialization\nWe derive a weight initialization scheme tailored to the\nGLU activation function similar to Glorot & Bengio\n(2010); He et al. (2015b) by focusing on the variance of\nactivations within the network for both forward and back-\nward passes. We also detail how we modify the weight\ninitialization for dropout.\nA.1. Forward Pass\nAssuming that the inputs xlof a convolutional layer land\nits weightsWlare independent and identically distributed\n(i.i.d.), the variance of its output, computed as yl=Wlxl+\nbl, is\nVar/bracketleftbig\nyl/bracketrightbig\n=nlVar/bracketleftbig\nwlxl/bracketrightbig\n(3)\nwherenlis the number inputs to the layer. For one-\ndimensional convolutional layers with kernel width kand\ninput dimension c, this iskc. We adopt the notation in (He\net al., 2015b), i.e. yl,wlandxlrepresent the random vari-\nables in yl,Wlandxl. Withwlandxlindependent from\neach other and normally distributed with zero mean, this\namounts to\nVar/bracketleftbig\nyl/bracketrightbig\n=nlVar/bracketleftbig\nwl/bracketrightbig\nVar/bracketleftbig\nxl/bracketrightbig\n. (4)\nxlis the result of the GLU activation function\nya\nl\u22121\u03c3(yb\nl\u22121)with yl\u22121= (ya\nl\u22121,yb\nl\u22121), and ya\nl\u22121,yb\nl\u22121\ni.i.d. Next, we formulate upper and lower bounds in or-\nder to approximate Var[xl]. Ifyl\u22121follows a symmetric\ndistribution with mean 0, then\nVar/bracketleftbig\nxl/bracketrightbig\n=Var/bracketleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/bracketrightbig\n(5)\n=E/bracketleftbig/parenleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/parenrightbig2/bracketrightbig\n\u2212E2/bracketleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/bracketrightbig\n(6)\n=Var[ya\nl\u22121]E/bracketleftbig\n\u03c3(yb\nl\u22121)2/bracketrightbig\n. (7)\nA lower bound is given by (1/4)Var[ya\nl\u22121]when expand-\ning (6) with E2[\u03c3(yb\nl\u22121)] = 1/4:\nVar/bracketleftbig\nxl/bracketrightbig\n=Var/bracketleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/bracketrightbig\n(8)\n=Var/bracketleftbig\nya\nl\u22121/bracketrightbig\nE2/bracketleftbig\n\u03c3(yb\nl\u22121)/bracketrightbig\n+\nVar/bracketleftbig\nya\nl\u22121/bracketrightbig\nVar/bracketleftbig\n\u03c3(yb\nl\u22121)/bracketrightbig (9)\n=1\n4Var/bracketleftbig\nya\nl\u22121/bracketrightbig\n+Var/bracketleftbig\nya\nl\u22121/bracketrightbig\nVar/bracketleftbig\n\u03c3(yb\nl\u22121)/bracketrightbig\n(10)\nandVar[ya\nl\u22121]Var[\u03c3(yb\nl\u22121)]>0.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "41291bbe-f253-424d-a1af-db318e580e5e": {"__data__": {"id_": "41291bbe-f253-424d-a1af-db318e580e5e", "embedding": null, "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d14f1254-258a-4f0c-9d42-50293e276bb5", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ff60c568641fb71c2cf9323356649e57268f061a15a8b756c6d04aafc2eeb1c7"}, "2": {"node_id": "e09d9d99-060d-41e7-adf9-27caf18d68ce", "node_type": "1", "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ed377127b4f76f62d26faf1b817ca904ca1a51bbd6b3a2982b123b77d41abc98"}}, "hash": "8bf1e87b85dbcb5c3bbcccb339b93405fb09c765b0ae443f80ac15767b97c151", "text": "We utilize the relation\n\u03c3(x)2\u2264(1/16)x2\u22121/4 +\u03c3(x)(Appendix B) to provide\nan upper bound on E[\u03c3(x)2]:\nE[\u03c3(x)2]\u2264E/bracketleftbig1\n16x2\u22121\n4+\u03c3(x)/bracketrightbig\n(11)\n=1\n16E[x2]\u22121\n4+E[\u03c3(x)] (12)Withx\u223cN(0,std(x)), this yields\nE/bracketleftbig\n\u03c3(x)2/bracketrightbig\n\u22641\n16E/bracketleftbig\nx2/bracketrightbig\n\u22121\n4+1\n2(13)\n=1\n16Var/bracketleftbig\nx/bracketrightbig\n+1\n4. (14)\nWith (7) and Var[ya\nl\u22121] =Var[yb\nl\u22121] =Var[yl\u22121], this\nresults in\nVar/bracketleftbig\nxl/bracketrightbig\n\u22641\n16Var/bracketleftbig\nyl\u22121/bracketrightbig2+1\n4Var/bracketleftbig\nyl\u22121/bracketrightbig\n. (15)\nWe initialize the embedding matrices in our network with\nsmall variances (around 0.01), which allows us to dismiss\nthe quadratic term and approximate the GLU output vari-\nance with\nVar[xl]\u22481\n4Var[yl\u22121]. (16)\nIfLnetwork layers of equal size and with GLU activations\nare combined, the variance of the \ufb01nal output yLis given\nby\nVar[yL]\u2248Var[y1]\uf8eb\n\uf8edL/productdisplay\nl=21\n4nlVar[wl]\uf8f6\n\uf8f8. (17)\nFollowing (He et al., 2015b), we aim to satisfy the condi-\ntion1\n4nlVar/bracketleftbig\nwl/bracketrightbig\n= 1,\u2200l (18)\nso that the activations in a network are neither exponen-\ntially magni\ufb01ed nor reduced. This is achieved by initializ-\ningWlfromN(0,/radicalbig\n4/nl).\nA.2. Backward Pass\nThe gradient of a convolutional layer is computed via back-\npropagation as \u2206xl=\u02c6Wlyl. Considering separate gradi-\nents\u2206ya\nland\u2206yb\nlfor GLU, the gradient of xis given by\n\u2206xl=\u02c6Wa\nl\u2206ya\nl+\u02c6Wb\nl\u2206yb\nl. (19)\n\u02c6Wcorresponds to Wwith re-arranged weights to enable\nback-propagation. Analogously to the forward pass, \u2206xl,\n\u02c6wland\u2206ylrepresent the random variables for the values\nin\u2206xl,\u02c6Wland\u2206yl, respectively. Note that Wand \u02c6W\ncontain the same values, i.e. \u02c6w=w. Similar to (3), the\nvariance of \u2206xlis\nVar[\u2206xl] = \u02c6nl/parenleftBig\nVar[wa\nl]Var[\u2206ya\nl] +Var[wb\nl]Var[\u2206yb\nl]/parenrightBig\n.\n(20)\nHere, \u02c6nlis the number of inputs to layer l+1. The gradients\nfor the GLU inputs are:\n\u2206ya\nl= \u2206xl+1\u03c3(yb\nl)and (21)\n\u2206yb\nl= \u2206xl+1ya\nl\u03c3/prime(yb\nl). (22)\n12", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bc0591f-1dd1-4c1e-8673-cf0f4e9b7895": {"__data__": {"id_": "7bc0591f-1dd1-4c1e-8673-cf0f4e9b7895", "embedding": null, "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bee79f4-ff09-42e7-a221-c1db43a284b2", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e3cd765478ac205e26922277607f0e46dd88da812c06d3598aa9cee61fbd7d9e"}, "3": {"node_id": "b61d7613-b518-4049-9a3f-9bac9b73eb74", "node_type": "1", "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "d9bd28401a70188f37ca21e5d3b18d361e50abd51d358d899992acac3f9f9ac0"}}, "hash": "a6d01c9a8ec8b329a2240ac55e728abd37ffe5b5157f7219ef109fa8d50e587b", "text": "Convolutional Sequence to Sequence Learning\nThe approximation for the forward pass can be used for\nVar[\u2206ya\nl], and for estimating Var[\u2206yb\nl]we assume an up-\nper bound on E[\u03c3/prime(yb\nl)2]of1/16since\u03c3/prime(yb\nl)\u2208[0,1\n4].\nHence,\nVar[\u2206ya\nl]\u22121\n4Var[\u2206xl+1]\u22641\n16Var[\u2206xl+1]Var[yb\nl)]\n(23)\nVar[\u2206yb\nl]\u22641\n16\u2206Var[\u2206xl+1]Var[ya\nl] (24)\nWe observe relatively small gradients in our network, typ-\nically around 0.001 at the start of training. Therefore, we\napproximate by discarding the quadratic terms above, i.e.\nVar[\u2206ya\nl]\u22481\n4Var[\u2206xl+1] (25)\nVar[\u2206yb\nl]\u22480 (26)\nVar[\u2206xl]\u22481\n4\u02c6nlVar[wa\nl]Var[\u2206xl+1] (27)\nAs for the forward pass, the above result can be general-\nized to backpropagation through many successive layers,\nresulting in\nVar[\u2206x2]\u2248Var[\u2206xL+1]\uf8eb\n\uf8edL/productdisplay\nl=21\n4\u02c6nlVar[wa\nl]\uf8f6\n\uf8f8 (28)\nand a similar condition, i.e. (1/4)\u02c6nlVar[wa\nl] = 1 . In the\nnetworks we consider, successions of convolutional layers\nusually operate on the same number of inputs so that most\ncasesnl= \u02c6nl. Note that Wb\nlis discarded in the approx-\nimation; however, for the sake of consistency we use the\nsame initialization for Wa\nlandWb\nl.\nFor arbitrarily large variances of network inputs and activa-\ntions, our approximations are invalid; in that case, the ini-\ntial values for Wa\nlandWb\nlwould have to be balanced for\nthe input distribution to be retained. Alternatively, meth-\nods that explicitly control the variance in the network, e.g.\nbatch normalization (Ioffe & Szegedy, 2015) or layer nor-\nmalization (Ba et al., 2016) could be employed.\nA.3. Dropout\nDropout retains activations in a neural network with a prob-\nabilitypand sets them to zero otherwise (Srivastava et al.,\n2014). It is common practice to scale the retained activa-\ntions by 1/pduring training so that the weights of the net-\nwork do not have to be modi\ufb01ed at test time when pis set to\n1. In this case, dropout amounts to multiplying activations\nxby a Bernoulli random variable rwhere Pr[r= 1/p] =p\nandPr[r= 0] = 1\u2212p(Srivastava et al., 2014). It holds\nthatE[r] = 1 andVar[r] = (1\u2212p)/p. Ifxis independentofrandE[x] = 0 , the variance after dropout is\nVar[xr] =E[r]2Var[x] +Var[r]Var[x] (29)\n=/parenleftbigg\n1 +1\u2212p\np/parenrightbigg\nVar[x] (30)\n=1\npVar[x] (31)\nAssuming that a the input of a convolutional layer has been\nsubject to dropout with a retain probability p, the varia-\ntions of the forward and backward activations from \u00a7A.1\nand\u00a7A.2 can now be approximated with\nVar[xl+1]\u22481\n4pnlVar[wl]Var[xl]and (32)\nVar[\u2206xl]\u22481\n4pnlVar[wa\nl]Var[\u2206xl+1].", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b61d7613-b518-4049-9a3f-9bac9b73eb74": {"__data__": {"id_": "b61d7613-b518-4049-9a3f-9bac9b73eb74", "embedding": null, "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bee79f4-ff09-42e7-a221-c1db43a284b2", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e3cd765478ac205e26922277607f0e46dd88da812c06d3598aa9cee61fbd7d9e"}, "2": {"node_id": "7bc0591f-1dd1-4c1e-8673-cf0f4e9b7895", "node_type": "1", "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "a6d01c9a8ec8b329a2240ac55e728abd37ffe5b5157f7219ef109fa8d50e587b"}}, "hash": "d9bd28401a70188f37ca21e5d3b18d361e50abd51d358d899992acac3f9f9ac0", "text": "(33)\nThis amounts to a modi\ufb01ed initialization of Wlfrom a nor-\nmal distribution with zero mean and a standard deviation of/radicalbig\n4p/n. For layers without a succeeding GLU activation\nfunction, we initialize weights from N(0,/radicalbig\np/n)to cali-\nbrate for any immediately preceding dropout application.\nB. Upper Bound on Squared Sigmoid\nThe sigmoid function \u03c3(x)can be expressed as a hyper-\nbolic tangent by using the identity tanh(x) = 2\u03c3(2x)\u22121.\nThe derivative of tanh istanh/prime(x) = 1\u2212tanh2(x), and\nwithtanh(x)\u2208[0,1],x\u22650it holds that\ntanh/prime(x)\u22641,x\u22650 (34)\n/integraldisplayx\n0tanh/prime(x) dx\u2264/integraldisplayx\n01 dx (35)\ntanh(x)\u2264x,x\u22650 (36)\nWe can express this relation with \u03c3(x)as follows:\n2\u03c3(x)\u22121\u22641\n2x,x\u22650 (37)\nBoth terms of this inequality have rotational symmetry w.r.t\n0, and thus\n/parenleftbig\n2\u03c3(x)\u22121/parenrightbig2\u2264/parenleftbigg1\n2x/parenrightbigg2\n\u2200x (38)\n\u21d4\u03c3(x)2\u22641\n16x2\u22121\n4+\u03c3(x). (39)\nC. Attention Visualization\nFigure 3 shows attention scores for a generated sentence\nfrom the WMT\u201914 English-German task. The model used\nfor this plot has 8 decoder layers and a 80K BPE vocabu-\nlary. The attention passes in different decoder layers cap-\nture different portions of the source sentence. Layer 1, 3\n13", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9b8337c3-efd1-4b76-932f-5ac249542895": {"__data__": {"id_": "9b8337c3-efd1-4b76-932f-5ac249542895", "embedding": null, "metadata": {"page_label": "14", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97ae1dac-86ff-4854-ad6d-4674257dfce4", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "37df03426700dc56f1bf26f81dc4ee421304853861f409d607eae775af6e85cd"}}, "hash": "37df03426700dc56f1bf26f81dc4ee421304853861f409d607eae775af6e85cd", "text": "Convolutional Sequence to Sequence Learning\nand 6 exhibit a linear alignment. The \ufb01rst layer shows the\nclearest alignment, although it is slightly off and frequently\nattends to the corresponding source word of the previously\ngenerated target word. Layer 2 and 8 lack a clear struc-\nture and are presumably collecting information about the\nwhole source sentence. The fourth layer shows high align-\nment scores on nouns such as \u201cfestival\u201d, \u201cway\u201d and \u201cwork\u201d\nfor both the generated target nouns as well as their preced-\ning words. Note that in German, those preceding words\ndepend on gender and object relationship of the respec-\ntive noun. Finally, the attention scores in layer 5 and 7\nfocus on \u201cbuilt\u201d, which is reordered in the German trans-\nlation and is moved from the beginning to the very end of\nthe sentence. One interpretation for this is that as genera-\ntion progresses, the model repeatedly tries to perform the\nre-ordering. \u201caufgebaut\u201d can be generated after a noun or\npronoun only, which is re\ufb02ected in the higher scores at po-\nsitions 2, 5, 8, 11 and 13.\n14", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b21f0fa-43f1-42b4-bba0-97d8142512ca": {"__data__": {"id_": "6b21f0fa-43f1-42b4-bba0-97d8142512ca", "embedding": null, "metadata": {"page_label": "15", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be32a1b6-df02-4633-b9a8-9f3a57d9a340", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "929512418db177f2b5adcf4f9b00090bc05e0ee5b131a5bd45d362f2ef24fe1e"}}, "hash": "929512418db177f2b5adcf4f9b00090bc05e0ee5b131a5bd45d362f2ef24fe1e", "text": "Convolutional Sequence to Sequence Learning\nLayer 1\n Layer 2\n Layer 3\nLayer 4\n Layer 5\n Layer 6\nLayer 7\n Layer 8\nFigure 3. Attention scores for different decoder layers for a sentence translated from English (y-axis) to German (x-axis). This model\nuses 8 decoder layers and a 80k BPE vocabulary.\n15", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e217ec36-ad39-4c40-99ed-54e10b954678": {"__data__": {"id_": "e217ec36-ad39-4c40-99ed-54e10b954678", "embedding": null, "metadata": {"page_label": "1", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eaae58fd-baa7-4d8f-a627-4531e2c1399b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "964b9a6c5f61660dcce57053c784e8eb879149c150241fa68e6086c2edec8083"}}, "hash": "964b9a6c5f61660dcce57053c784e8eb879149c150241fa68e6086c2edec8083", "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ab350db6-6467-4be9-a297-ea6068301555": {"__data__": {"id_": "ab350db6-6467-4be9-a297-ea6068301555", "embedding": null, "metadata": {"page_label": "2", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ea34853-1573-4000-88c8-873919d0f1fe", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "424365d43930189cf422ee1ade8a491ebcf4e442c0ec55c24b0b4b95bf4b3f4f"}}, "hash": "424365d43930189cf422ee1ade8a491ebcf4e442c0ec55c24b0b4b95bf4b3f4f", "text": "1 Introduction\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "59101f6d-aabb-4146-b573-f2e8ea767ba6": {"__data__": {"id_": "59101f6d-aabb-4146-b573-f2e8ea767ba6", "embedding": null, "metadata": {"page_label": "3", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f97977f-a8ef-4908-90fa-7bb879980d03", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "68100f25e7b0f82f7dd8da826dde561f4e936a3203a034bd7c63bb76eb6fe0ef"}}, "hash": "68100f25e7b0f82f7dd8da826dde561f4e936a3203a034bd7c63bb76eb6fe0ef", "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf876f47-0b35-4a07-abe9-f73b64a1c677": {"__data__": {"id_": "bf876f47-0b35-4a07-abe9-f73b64a1c677", "embedding": null, "metadata": {"page_label": "4", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e917ed70-7707-4875-b958-654c0af5fb63", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "5e4595539cda087ff29287abbbf33cc028d4a8676c58ec40c5c06340c9de6fcb"}}, "hash": "5e4595539cda087ff29287abbbf33cc028d4a8676c58ec40c5c06340c9de6fcb", "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\n4", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ff30797b-b5f4-4e6f-94f9-d328b37fd56a": {"__data__": {"id_": "ff30797b-b5f4-4e6f-94f9-d328b37fd56a", "embedding": null, "metadata": {"page_label": "5", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c943d83-50a6-4df9-946a-400040557fef", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6a5fe59d4fcfcf28c61b1cbf6e3fc6e147ba5d597ef1267ec717590bc6ca831b"}}, "hash": "6a5fe59d4fcfcf28c61b1cbf6e3fc6e147ba5d597ef1267ec717590bc6ca831b", "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\n5", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7b5f28c7-ecf9-409e-a0aa-230a0778d775": {"__data__": {"id_": "7b5f28c7-ecf9-409e-a0aa-230a0778d775", "embedding": null, "metadata": {"page_label": "6", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b0d908d-ca02-4ebf-bf71-35f841b2eb04", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "bba27ac8fbb1b07f867ca0e542eaceb123cd3e17d1df900862c915d401aa4252"}}, "hash": "bba27ac8fbb1b07f867ca0e542eaceb123cd3e17d1df900862c915d401aa4252", "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fa868cf2-6638-4e34-91f9-e2ed9301d35f": {"__data__": {"id_": "fa868cf2-6638-4e34-91f9-e2ed9301d35f", "embedding": null, "metadata": {"page_label": "7", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d06ef884-b1a9-4831-b274-d9bca4368733", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "5decd18e8c31784d036398ec728bd1da64c8c264abf11f770a1b88a46d6d1e32"}}, "hash": "5decd18e8c31784d036398ec728bd1da64c8c264abf11f770a1b88a46d6d1e32", "text": "length nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7da151ff-3a8b-4874-8b1d-6676594fc3bd": {"__data__": {"id_": "7da151ff-3a8b-4874-8b1d-6676594fc3bd", "embedding": null, "metadata": {"page_label": "8", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef3b3112-6b8d-4f3a-952a-bfa2d7916467", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "c84898c58d6c46c76a205dc11e993e922053f59710a2c0bc06c264932915d29c"}}, "hash": "c84898c58d6c46c76a205dc11e993e922053f59710a2c0bc06c264932915d29c", "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "08d26031-30a3-4c80-a4f4-6f3a57ae9f0a": {"__data__": {"id_": "08d26031-30a3-4c80-a4f4-6f3a57ae9f0a", "embedding": null, "metadata": {"page_label": "9", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6164f625-9d41-4bb1-9d65-fbba101f6963", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ac4ec04eef479021f479f7fab4cb86b6df2acb9d9fee899efbfa218fe7a2e818"}}, "hash": "ac4ec04eef479021f479f7fab4cb86b6df2acb9d9fee899efbfa218fe7a2e818", "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a684f2f9-1545-4572-b062-384505162233": {"__data__": {"id_": "a684f2f9-1545-4572-b062-384505162233", "embedding": null, "metadata": {"page_label": "10", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a596c78e-f592-4709-8c55-a2a56de14fc7", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e8136c24b7eede8d377b3875a274590f3bf2403d1a64b3d89e2be03e8f29dc98"}}, "hash": "e8136c24b7eede8d377b3875a274590f3bf2403d1a64b3d89e2be03e8f29dc98", "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3cb5a81a-17d9-4bdb-829e-6d44741be470": {"__data__": {"id_": "3cb5a81a-17d9-4bdb-829e-6d44741be470", "embedding": null, "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6a67f75-fb4d-4909-898b-bb5f0750403a", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ed3ef5d4f99484269e3e5d8027f40992090084a6b2f5fbf8c812ddaef4c55203"}, "3": {"node_id": "c7d715ea-b6ac-4aa5-877d-20861507801d", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "846291a34a81b38aaf7a2edb423482fa93a17106e69d8283b01536e03d00e01d"}}, "hash": "e38d4ad6b6b1d18b809142bf8a491e94ba9a33b977706a942ea34cc01f938fe9", "text": "[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7d715ea-b6ac-4aa5-877d-20861507801d": {"__data__": {"id_": "c7d715ea-b6ac-4aa5-877d-20861507801d", "embedding": null, "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6a67f75-fb4d-4909-898b-bb5f0750403a", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ed3ef5d4f99484269e3e5d8027f40992090084a6b2f5fbf8c812ddaef4c55203"}, "2": {"node_id": "3cb5a81a-17d9-4bdb-829e-6d44741be470", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "e38d4ad6b6b1d18b809142bf8a491e94ba9a33b977706a942ea34cc01f938fe9"}}, "hash": "846291a34a81b38aaf7a2edb423482fa93a17106e69d8283b01536e03d00e01d", "text": "Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n11", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bf7b80c-a412-4ebf-b46c-51dde442924a": {"__data__": {"id_": "7bf7b80c-a412-4ebf-b46c-51dde442924a", "embedding": null, "metadata": {"page_label": "12", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8314f153-2970-455c-b99c-e00aff13d161", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "ec643b4b199f6243e049209025376bcf09ddeaacddc01b1f80f9f845610860bc"}}, "hash": "ec643b4b199f6243e049209025376bcf09ddeaacddc01b1f80f9f845610860bc", "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "79a042d5-b02a-42f2-894b-41efa2d8451d": {"__data__": {"id_": "79a042d5-b02a-42f2-894b-41efa2d8451d", "embedding": null, "metadata": {"page_label": "13", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0fcdda1-7ef3-472e-8398-ed41d923cfaf", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "8e6fecc3745542327aec9ebe09b778347d1dde81bd29e9a6248766f39b3cf7b9"}}, "hash": "8e6fecc3745542327aec9ebe09b778347d1dde81bd29e9a6248766f39b3cf7b9", "text": "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bdd40762-53a2-4381-8d42-010e8df959e0": {"__data__": {"id_": "bdd40762-53a2-4381-8d42-010e8df959e0", "embedding": null, "metadata": {"page_label": "14", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1587cc8c-90a7-4be2-973a-c0af1d2e402d", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "0b07b120490797262997cf49cd707d588b3c237aacbddeea1b6064efe5e79975"}}, "hash": "0b07b120490797262997cf49cd707d588b3c237aacbddeea1b6064efe5e79975", "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9e4f1386-0807-4394-a6fd-8c531db7a2f8": {"__data__": {"id_": "9e4f1386-0807-4394-a6fd-8c531db7a2f8", "embedding": null, "metadata": {"page_label": "15", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f2e0772-14a7-493a-99ce-397a9b2acc56", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}, "hash": "6fb4fd214f60b758c4fdd56d21f9d8c2a705ef3157dc8ed10f62371666bf76eb"}}, "hash": "6fb4fd214f60b758c4fdd56d21f9d8c2a705ef3157dc8ed10f62371666bf76eb", "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"d4628515-1f2f-47ea-86f6-25d0d76c1035": {"node_ids": ["1d1e7d17-3d74-42d4-b1ad-de61da5659c6"], "metadata": {"page_label": "1", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "cb49f97a-3aa0-4212-8005-fb5c70cb8ff8": {"node_ids": ["924bd91d-ed72-4c4e-9100-6a2c3ccc96b8", "5a528f2d-0d66-479a-81c9-046f97289c43"], "metadata": {"page_label": "2", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "2cc76dff-87a2-46f4-9964-fe98b162f4a2": {"node_ids": ["3a5db9c7-ca84-4422-be6b-cf4ad6731a48", "19f72316-d940-4728-bfd3-6840b75b97d8"], "metadata": {"page_label": "3", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "28734f21-e5d5-418b-8a33-0318bc753a4e": {"node_ids": ["941c1ae7-ec72-49a0-aff4-0deaf59a32cf", "af992b63-bba3-4858-aadb-8e1194b8d90c"], "metadata": {"page_label": "4", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "23c8cb0f-647d-4596-9c23-e6fc2562944c": {"node_ids": ["4a9355c0-ee4d-40fa-92b0-4f0283c44351", "f527ad07-34d6-417e-ad5f-8429c0040d14"], "metadata": {"page_label": "5", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "4f54020d-2f43-4631-8cdd-9941aefd8ca3": {"node_ids": ["5f458e21-b384-4b55-8b1c-5cd833cda64d", "5aaf3b75-993b-40f0-8b2f-8fbca305f169"], "metadata": {"page_label": "6", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "cf3c4a92-db82-4877-9e2c-f2908ca6bd1e": {"node_ids": ["e274fee9-e0d4-4ae4-856e-bfc65684262f", "793bbc40-715f-4ee9-b11a-f8c20ea5a916"], "metadata": {"page_label": "7", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "73107650-0c1d-467a-8f9e-2a85dff78c1e": {"node_ids": ["055e1273-c70c-4622-9e78-93718cac370d", "b5b34ce3-0cb1-4672-97cf-11b544065441"], "metadata": {"page_label": "8", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "ba6488a6-f975-4588-a75f-d0b082dcf28b": {"node_ids": ["35298a8b-7f91-4f4a-95bf-92983732010c", "1040b50c-52a9-4920-beb4-dcd70eb2e18b"], "metadata": {"page_label": "9", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "57681db3-c712-4ebe-941e-901878af9b94": {"node_ids": ["3764afa8-ff16-4f59-8a98-d5e8e030056a", "4e5b2854-e70a-488d-a29c-2a9d112b08c9"], "metadata": {"page_label": "10", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0f4de1f6-d75d-481b-8233-4538aa6dd01a": {"node_ids": ["5c7b844d-4d02-44ef-9a17-8d40bc43a24d"], "metadata": {"page_label": "11", "file_name": "1508.04025.pdf", "file_path": "data/1508.04025.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "19139b82-d1b5-43d4-a7bd-e35297d8e82e": {"node_ids": ["aae345c1-f574-4e6d-906e-554557a3bea3"], "metadata": {"page_label": "1", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "257e9425-9f18-4f4f-89bd-9aace4633dbe": {"node_ids": ["4bd16d19-fff7-4bfa-ab29-de5e2d6eea51", "61591b94-fb42-4ab4-8bf6-4fc07098a3e3"], "metadata": {"page_label": "2", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "edff8844-dec3-413c-acb5-d95c49d37b7e": {"node_ids": ["7f6322eb-a831-4b34-a5d9-e7eec7b3f9c7", "64b0a2e2-8f35-42eb-8a38-ecd4aa48ef1d"], "metadata": {"page_label": "3", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "fd9fdfe0-2305-4b69-97ec-f5eda22bc01c": {"node_ids": ["d41f9130-62e5-4140-b7eb-3690aaa6f732", "ea998542-7a20-41be-a64b-ca912e79c9d5"], "metadata": {"page_label": "4", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0a402834-f7ed-4f26-8200-9b91cdf11b4c": {"node_ids": ["74ddba6b-99ca-44c9-8449-11cc73cda500", "d5428378-ba1f-4a18-a8a2-299298de8143"], "metadata": {"page_label": "5", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "a051f339-922c-4b89-b4fc-b6940a45dc5a": {"node_ids": ["8eb9e29d-7e82-4a3e-8059-2b979e80ac68", "857ade76-a6a8-4a28-bd11-6ed6c78e123f"], "metadata": {"page_label": "6", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "7bc9bee8-cb63-4c8a-8b58-15e0baf51f92": {"node_ids": ["be4f57e5-5637-473d-a7ae-ac79fe464385", "85cca884-cba4-43e1-afde-d2713a0c1180"], "metadata": {"page_label": "7", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "406a4296-6669-468a-bac1-42083f0c6181": {"node_ids": ["041f3096-e99f-4022-bdc7-2c70a15b014f", "63f19542-2331-4d54-b081-881dd6d50775"], "metadata": {"page_label": "8", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "65c138df-617c-4874-8abf-4d6feef1fe9e": {"node_ids": ["dd1a448b-490c-40cb-b9fc-f8fca2d6629c", "2c16c064-0a0f-48a1-a538-25289bd81aed"], "metadata": {"page_label": "9", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "a08a854c-ce44-4fe0-adbe-4c8c4fd402e0": {"node_ids": ["13fb6ce1-69ae-4efc-b403-c84b67c09020", "6e4a52a6-6ca3-45d7-8222-fa3a534edd16"], "metadata": {"page_label": "10", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0435b871-4048-4e5a-aa21-82cb34b77642": {"node_ids": ["925368db-e7ca-4466-af71-1e77a5b65668", "57595640-e086-42ce-a3e5-4f12fcb124a3"], "metadata": {"page_label": "11", "file_name": "1508.07909.pdf", "file_path": "data/1508.07909.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "a97d10ad-fd39-4d39-bec4-f1c888d80c37": {"node_ids": ["46fe14bb-fad4-4f6f-8c4d-2531488cc735", "04a952db-d803-4a90-8852-a299026eedfc"], "metadata": {"page_label": "1", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "05696886-7400-41a8-9087-a90319edb626": {"node_ids": ["3473b3cd-4dfe-4f70-a391-44577870b020", "39d7b088-bda1-4b5e-810d-133b42eeacff"], "metadata": {"page_label": "2", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "52a93de9-980f-425f-a7ed-1c2e53757d68": {"node_ids": ["59d3092a-68df-4820-83e0-80278ab6acb1"], "metadata": {"page_label": "3", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "3044d36b-22ab-4bf1-9baa-3218119a8e05": {"node_ids": ["a2af8075-0c76-4b88-967e-c7b58e914ea8"], "metadata": {"page_label": "4", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "10f495a8-e5a3-4331-b4c3-979249d8e5df": {"node_ids": ["43602b72-2b7a-40d4-aae1-2bb3109bdbfe", "958108fd-9415-41b9-bba1-cfead2a09248"], "metadata": {"page_label": "5", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "59f34a73-617d-4d45-90d0-2dfd09f9a620": {"node_ids": ["435d8253-c2ed-4503-8c24-b453b9b93b64", "cf26d3da-a8f2-4340-82da-b07e523bed31"], "metadata": {"page_label": "6", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "31969453-c8d0-4760-a7b4-a32666005be6": {"node_ids": ["c9e1e885-5520-47fe-8669-54c6025a7277", "7e7b6e0f-c4d1-4f65-99fd-2766d644e910"], "metadata": {"page_label": "7", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "b20e5506-61f6-4801-ad12-da7da6debc74": {"node_ids": ["dc5aeeb3-beff-4583-862b-a1501e6c7846", "64e50b3e-18cf-49d8-8f54-b7046d63fae2"], "metadata": {"page_label": "8", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "7c61d074-41bb-4a7c-9faf-f35d94f86ca3": {"node_ids": ["c3e8d662-0db6-4b5f-908e-d038e72c6ad7", "3af45a22-dfbd-44c2-b71b-bc58ea17228a"], "metadata": {"page_label": "9", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "27edb7b6-3c71-4d30-8b51-f392be1b0b1c": {"node_ids": ["2b6806b5-e7b8-4f53-bbb8-04ce601964d2", "ed6b8ae4-7744-4674-870f-6086bfa05304"], "metadata": {"page_label": "10", "file_name": "1511.06114.pdf", "file_path": "data/1511.06114.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "6518de0d-dda4-4d06-b56a-ad870a44f66b": {"node_ids": ["fc892e84-0da4-4245-840a-852069ad8181"], "metadata": {"page_label": "1", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "4df198f5-f606-4714-a3c2-cffd8c295899": {"node_ids": ["62529978-b833-457c-831b-8094e4a10a93", "32b02e64-4749-4af8-abde-96b4267d05f4"], "metadata": {"page_label": "2", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "6633cb98-b968-447b-969d-9ff214531f79": {"node_ids": ["ec2e768e-4b09-43d4-bc97-6419d2ebd26f", "8a7d85af-eafb-4b73-a496-754080e1c711"], "metadata": {"page_label": "3", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "f64ef16c-5d58-40d7-b16e-13a5550bd232": {"node_ids": ["c0526a46-e196-46d1-ae25-206a3a58006f", "102d1be8-4d2d-4804-aa05-aaaec189c3a8"], "metadata": {"page_label": "4", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "dedb899c-393a-4e00-88ee-3bd1e5d2dfdb": {"node_ids": ["7bd5681e-547b-4ffe-98b9-c5f840045017", "1a3fb2ef-0a42-4c8b-8212-5882c8e59ff3"], "metadata": {"page_label": "5", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "040f261a-e914-4818-9948-18bbb65aca82": {"node_ids": ["f260e5a4-ca53-4575-8201-9ae078411479", "9bf950e2-ccbd-403e-90fd-d858d97a4e2d"], "metadata": {"page_label": "6", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "ad590b0f-21fe-41a9-a41b-fedf22806134": {"node_ids": ["fb0df862-8a0c-4926-9d86-45e245183d49", "5420fbac-7c32-4b10-a748-aa149a857223"], "metadata": {"page_label": "7", "file_name": "1608.05859.pdf", "file_path": "data/1608.05859.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "1ec8c8b1-2c08-4d68-9769-aafcb27956e7": {"node_ids": ["e975a0d2-165f-4f97-8855-53d42cd0638f"], "metadata": {"page_label": "1", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "598dad45-b79a-4f0f-b791-626974833e17": {"node_ids": ["57953232-be82-44ce-97d3-910966bdd746"], "metadata": {"page_label": "2", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "e5f4a401-1e88-462f-8b8b-405dfd8ac7e4": {"node_ids": ["c0d270c5-dbca-42ea-9e99-c24e5b966712", "a8f3c25a-b9f4-4c20-aba3-1e29d643033c"], "metadata": {"page_label": "3", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "6742c4da-7e0a-4a21-9fe4-e182dd0e221c": {"node_ids": ["28744c52-322a-4899-a6e0-fedd5b134458", "2e0985ac-c3f2-42e1-a1fe-aade395804e8"], "metadata": {"page_label": "4", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "9e9f0a56-7e29-4909-88bd-73e467a5eaeb": {"node_ids": ["f8e35205-bd27-4b38-a90b-1c72ba35fe8f", "1c972d34-d3b8-4ec5-9acb-5d1fc239bf9b"], "metadata": {"page_label": "5", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "a7456b3b-d65d-4c0a-a434-f699011c3d80": {"node_ids": ["d1a2ad8d-d439-4461-af79-9b425b652b1c"], "metadata": {"page_label": "6", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "84e8d558-a595-4bf4-9587-f7ede9273991": {"node_ids": ["43b1053b-0a4a-46ae-a0ee-9fc41955ad9e", "42ec8213-57f8-4ef5-b272-27e16f7caaa7"], "metadata": {"page_label": "7", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "fd074a23-19f2-4efd-bd4a-ff87a76d76d0": {"node_ids": ["cdd72c20-924f-444d-8f21-a8fe217f38c5"], "metadata": {"page_label": "8", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "11f1db50-1d2a-41be-8e60-894c2a28ce8b": {"node_ids": ["6bf0c098-2f15-4fa6-8ba2-2765e2d4f404"], "metadata": {"page_label": "9", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "ba107626-2d72-4316-b3a1-fa3799a192fe": {"node_ids": ["3171923f-0c02-4444-a14b-c2814f035ea3"], "metadata": {"page_label": "10", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0f00fb90-1605-4f75-9509-e5dda2c7db19": {"node_ids": ["a65709ff-3b33-4f8e-9cdc-44bb8a0be449", "a541e458-9efa-4027-9f76-d4123e9752d6"], "metadata": {"page_label": "11", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "154ed6bc-79e8-4bc9-8426-2480a3c0238c": {"node_ids": ["bc492931-4fe0-4d5c-87d4-a9ab193f12be"], "metadata": {"page_label": "12", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0e9a8331-517d-4453-97f5-037b8d82d3d7": {"node_ids": ["1cd43094-43e0-4ba5-817f-d4145d585e6b"], "metadata": {"page_label": "13", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0e1eb5a1-f9db-4fb3-bb17-613d176c030b": {"node_ids": ["b3c9600a-369d-456c-b8b8-679f00fecd55"], "metadata": {"page_label": "14", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "44ee76b9-bf15-4cc8-95fd-aa2265dbf3ce": {"node_ids": ["3fbb6995-dc85-435f-8fe5-2f790713c197"], "metadata": {"page_label": "15", "file_name": "1703.03130.pdf", "file_path": "data/1703.03130.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0e371c83-9b2f-4a26-89e0-c6697ab820d8": {"node_ids": ["0cf5db57-0271-4cf3-b67d-7f6eff1d9cf2"], "metadata": {"page_label": "1", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "203af889-47e6-413d-b785-fcfee2a17832": {"node_ids": ["2d8bb20d-b87b-4ad1-9b35-af47cff055dd"], "metadata": {"page_label": "2", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "074d81c9-eb70-40c1-9099-2d5eaa51fc93": {"node_ids": ["e30d4ba2-7e62-4193-ab40-0ca19977cae5", "b8ead17b-639b-4db2-b46e-d74e26b71cb3"], "metadata": {"page_label": "3", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "963338af-f8c1-4291-a9b0-4686a6eb7f94": {"node_ids": ["273bc17e-2ea4-49da-af39-40cfc92f1752", "8081d5a9-f1a3-453a-bb77-1a7a6343f332"], "metadata": {"page_label": "4", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "ab286212-c22d-499d-8d31-60d0a2af8264": {"node_ids": ["4dac1fdc-d778-4658-b30c-4f48a7b3eb4c"], "metadata": {"page_label": "5", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "0ceb5ddd-d825-40cb-b26d-cdbe59a738f6": {"node_ids": ["09ad95d7-15a5-4ea4-b423-fbdecc0a8585"], "metadata": {"page_label": "6", "file_name": "1703.10722.pdf", "file_path": "data/1703.10722.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "d4e6f2f1-1603-4c5c-96fd-71b098f988db": {"node_ids": ["9497ee67-f2e1-406e-a923-8b6e55fc8775", "c628c85a-5f1c-495b-b97e-22608f93d7a1"], "metadata": {"page_label": "1", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "076a8af5-e36e-4070-9ae0-3d014e547de1": {"node_ids": ["e32034af-9926-49f8-8023-e4a452ec43c6", "297dbeac-874a-4aa1-a9de-1ed25528bea1"], "metadata": {"page_label": "2", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "e13b5f59-ed8a-42a8-8569-db7f6f108ff0": {"node_ids": ["8aaae5eb-f328-4746-b399-07994d2b0acb", "13be01f6-0b0c-4a82-bdad-1b2a0a99128c"], "metadata": {"page_label": "3", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "bf41e409-ceaa-4311-8982-adc573e48688": {"node_ids": ["6c1a6d24-8acc-4c3f-94c9-4e876126c15c", "0a363440-8cc2-458e-a73e-310ec1288106"], "metadata": {"page_label": "4", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "1b1dcd45-cc0b-40da-a634-3825d04c51b7": {"node_ids": ["9bbe9300-82ab-48f9-924e-25c28da0c22c", "a58397a2-e7ac-42c7-bd5a-06a0c0a7dc12"], "metadata": {"page_label": "5", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "1e067fee-1de3-4a11-bb31-b19b4c9e379f": {"node_ids": ["46074122-c9e3-4232-a7d4-2e98354962ee", "61a50473-f583-42dd-91be-5973c3121201"], "metadata": {"page_label": "6", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "1b371e23-4721-4cb3-a324-15e5c2cdcbdd": {"node_ids": ["e1448d1a-02c9-4f78-897b-a75f5e49518d", "27fc9f8d-9779-4761-aefc-081a9cf1604e"], "metadata": {"page_label": "7", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "44e46b2f-9826-4bf3-bc87-7dac192228b9": {"node_ids": ["fc5fa2b7-8871-41b4-ac91-b27d57ca55df", "bddd3a50-6b0f-465e-92a0-95c8fa716e8b"], "metadata": {"page_label": "8", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "03e1dd28-c2b5-4961-bff2-1de7c3789fd3": {"node_ids": ["802a0e3c-1c99-4f2f-bb01-1cd426dcc64d", "56bde1d1-72a9-42bd-b1cb-e76711c9c745"], "metadata": {"page_label": "9", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "2d2bce7f-3073-4a67-9e1b-15cfb44cc4a8": {"node_ids": ["b78e3b9d-35dd-49b8-a721-e3b6d14f0300", "5063748f-bc7b-4a58-ae66-8a06e22d00a9"], "metadata": {"page_label": "10", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "318e8d10-9e10-4124-b963-da894ef2e3eb": {"node_ids": ["f535e700-1b9a-4b90-a385-9a1ae62c3539", "427fea22-4fef-4742-af36-2be94792a0ea"], "metadata": {"page_label": "11", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "d14f1254-258a-4f0c-9d42-50293e276bb5": {"node_ids": ["e09d9d99-060d-41e7-adf9-27caf18d68ce", "41291bbe-f253-424d-a1af-db318e580e5e"], "metadata": {"page_label": "12", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "6bee79f4-ff09-42e7-a221-c1db43a284b2": {"node_ids": ["7bc0591f-1dd1-4c1e-8673-cf0f4e9b7895", "b61d7613-b518-4049-9a3f-9bac9b73eb74"], "metadata": {"page_label": "13", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "97ae1dac-86ff-4854-ad6d-4674257dfce4": {"node_ids": ["9b8337c3-efd1-4b76-932f-5ac249542895"], "metadata": {"page_label": "14", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "be32a1b6-df02-4633-b9a8-9f3a57d9a340": {"node_ids": ["6b21f0fa-43f1-42b4-bba0-97d8142512ca"], "metadata": {"page_label": "15", "file_name": "1705.03122.pdf", "file_path": "data/1705.03122.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "eaae58fd-baa7-4d8f-a627-4531e2c1399b": {"node_ids": ["e217ec36-ad39-4c40-99ed-54e10b954678"], "metadata": {"page_label": "1", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "9ea34853-1573-4000-88c8-873919d0f1fe": {"node_ids": ["ab350db6-6467-4be9-a297-ea6068301555"], "metadata": {"page_label": "2", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "5f97977f-a8ef-4908-90fa-7bb879980d03": {"node_ids": ["59101f6d-aabb-4146-b573-f2e8ea767ba6"], "metadata": {"page_label": "3", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "e917ed70-7707-4875-b958-654c0af5fb63": {"node_ids": ["bf876f47-0b35-4a07-abe9-f73b64a1c677"], "metadata": {"page_label": "4", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "9c943d83-50a6-4df9-946a-400040557fef": {"node_ids": ["ff30797b-b5f4-4e6f-94f9-d328b37fd56a"], "metadata": {"page_label": "5", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "5b0d908d-ca02-4ebf-bf71-35f841b2eb04": {"node_ids": ["7b5f28c7-ecf9-409e-a0aa-230a0778d775"], "metadata": {"page_label": "6", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "d06ef884-b1a9-4831-b274-d9bca4368733": {"node_ids": ["fa868cf2-6638-4e34-91f9-e2ed9301d35f"], "metadata": {"page_label": "7", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "ef3b3112-6b8d-4f3a-952a-bfa2d7916467": {"node_ids": ["7da151ff-3a8b-4874-8b1d-6676594fc3bd"], "metadata": {"page_label": "8", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "6164f625-9d41-4bb1-9d65-fbba101f6963": {"node_ids": ["08d26031-30a3-4c80-a4f4-6f3a57ae9f0a"], "metadata": {"page_label": "9", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "a596c78e-f592-4709-8c55-a2a56de14fc7": {"node_ids": ["a684f2f9-1545-4572-b062-384505162233"], "metadata": {"page_label": "10", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "e6a67f75-fb4d-4909-898b-bb5f0750403a": {"node_ids": ["3cb5a81a-17d9-4bdb-829e-6d44741be470", "c7d715ea-b6ac-4aa5-877d-20861507801d"], "metadata": {"page_label": "11", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "8314f153-2970-455c-b99c-e00aff13d161": {"node_ids": ["7bf7b80c-a412-4ebf-b46c-51dde442924a"], "metadata": {"page_label": "12", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "c0fcdda1-7ef3-472e-8398-ed41d923cfaf": {"node_ids": ["79a042d5-b02a-42f2-894b-41efa2d8451d"], "metadata": {"page_label": "13", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "1587cc8c-90a7-4be2-973a-c0af1d2e402d": {"node_ids": ["bdd40762-53a2-4381-8d42-010e8df959e0"], "metadata": {"page_label": "14", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}, "1f2e0772-14a7-493a-99ce-397a9b2acc56": {"node_ids": ["9e4f1386-0807-4394-a6fd-8c531db7a2f8"], "metadata": {"page_label": "15", "file_name": "1706.03762.pdf", "file_path": "data/1706.03762.pdf", "creation_date": "2023-11-06", "last_modified_date": "2023-11-06", "last_accessed_date": "2023-11-06"}}}}